## WEEK IV - Sequential Decision Making

>[🏠 MENU - 6CCS3AIN](year3/6ccs3ain.md)
>
>[⬅️ WEEK III - Bayesian Networks](year3/6ccs3ain/w3.md)
>
>[➡️ WEEK V - PCA](year3/6ccs3ain/w5.md)
>
>Outlines:
>
>1. Introduction to Sequential Decision Making
>2. Policies
>    - Alternative Criteria 
>      - MEU - maximum expected utility
>      - maxmax - Option with largest reward
>      - maxmin - Option avoid with smallest reward
>    - MDP
>3. Bellman Equation
>4. Value Iteration & Policy Iteration

### 4.1. Introduction to Sequential Decision Making

##### 4.1.1. Expected Value in Decision Making

- Sequential Decision Making Problem

  - One decision leads to another.
  - Each decision depends on the ones before, and affects the ones after.

- Stochastic

  - We need **expected value** to analysis. 
    > e.g.
    >
    > Consider being offered a bet in which you pay *£*2 if an odd number is rolled on a die, and win *£*3 if an even number appears. We want to know if it is a good bet. 
    >
    > We do this in terms of a random variable, which we will call $X$, where
    >
    > $X = \begin{cases}3 &\text{if the die rolls odd} \\ -2 &\text{if the die rolls even} \end{cases}$
    >
    > i.e., 
    >
    > $P(X=3)=0.5$ 
    >
    > $P(X=-2)=0.5$
    >
    > The expected value $E[X]=3 \cdot 0.5 + (-2) \cdot 0.5 = 0.5$

  - The expected value is then the weighted sum of the values, where the weights are the probabilities.

  - Formally the expected value of $X$ is defined by:
    $$
    E[X] = \sum\limits_k k \cdot P(X=k)
    $$
    where the summation is over all values of $k$ for which $P(X=k) \neq 0$. 

  - Expected value is not the value people well get actually. You can think of it as the long run average if you were offered the bet many times.

- Example

  > e.g.
  >
  > Pacman is at a T-junction, Based on their knowledge, estimates that if they go Left: 
  >
  > $P(X=10)=0.3$
  >
  > $P(X=1)=0.2$
  >
  > $P(X=-5)=1-0.3-0.2=0.5$
  >
  > Then we can get the expected value $E[X]=0.3 \cdot 10 + 0.2 \cdot 1 + 0.5 \cdot (-5) = 0.7$

##### 4.1.2. Agent Decision Making

- Decision Making Problem for Agent

  - Consider an agent with a set of possible actions $A$, each $a \in A$ has a set of possible outcomes $s_a$. 

  - The action $a^*$ which a <u>rational agent</u> should choose is that which maximises the agent's utility. In another word, 
    $$
    a^* = \arg \max\limits_{a \in A} u(s_a)
    $$
    where $s_a$ is the state obtained by choosing action $a$, and

    $u(s_a)$ is the utility of that state. 

  - The problem is that in any realistic situation, the resulting state is probabilistic.

  - Instead we have to calculate the **expected utility** of each action and make the choice on the basis of that.

    - Rational agent will make the decision by taking the best expected utility value. 

- Expected Utility

  - For each action a with a set of outcomes $s_a$, the agent should calculate the expected value that could be obtained from a state
    $$
    E[u(a)] = \sum\limits_{s' \in s_a} u(s')P(s_a = s')
    $$
    and pick the best. 

---

### 4.2. Policies

##### 4.2.1. Alternative Criteria

- One approach is to look at the option which has the least-bad worst outcome.

- This maximin criterion can be formalised in the same framework as **MEU (Maximum Expected Utility)**, making the rational (in this sense) action:
  $$
  a^* = \arg \max\limits_{a \in A} \{ \min\limits_{s' \in s_a} u(s') \}
  $$
  Its effect is to ignore the probability of outcomes and concentrate on optimising the worst case outcome.

  > e.g.
  >
  > ```mermaid
  > graph TD
  > 	s --> a1{a1}
  > 	s --> a2{a2}
  > 	
  > 	a1 --4--> s1
  > 	a1 --4--> s2
  > 	a1 --5--> s3
  > 	
  > 	a2 --3--> s4
  > 	a2 --5--> s5
  > 	a2 --7--> s6
  > ```
  >
  > Here we would pick $a_2$, since $4 + 4 + 5 < 3 + 5 + 7$

##### 4.2.2. Markov Decision Process (MDP)

- Motivation 

  - Agents aren't usually in the business of taking single decisions. Series of decisions is more common. 
  - Since the best overall result is not necessarily obtained by a greedy approach to a series of decisions.

- Transition Model

  - We can write a transition model to describe a series of actions. Since the actions are stochastic, the model looks like
    $$
    P(s'|s, a)
    $$
    where $a$ is the action that takes the agent from $s$ to $s'$. 

  - Transitions are assumed to be **first order Markovian**.

    - That is, they only depend on the current and next states.
    - So, we could write a large set of probability tables that would describe all the possible actions executed in all the possible states.

- MDP

  - The overall problem the agent faces here is a Markov decision process (MDP). 
  - Mathematically, we have
    - A set of states $s \in S$ with an initial state $s_0$.
    - A set of actions $A(s)$ in each state. 
    - A transition model $P(s'|s, a)$, and
    - A reward function $R(s)$. 
  - Captures any fully observable non-deterministic environment with a Markovian transition model and additive rewards.

##### 4.2.3. Policy

- Definition

  - A solution of a MDP is a **policy**, notated as $π$.

  - A policy is a choice of action for every state. In any state, $\pi(s)$ identifies what action to take. 

    > e.g.
    >
    > $\pi(s_0) = \text{left}, \pi(s_1) = \text{left}, ...$

- Optimum policy

  - Naturally we want to get the optimum policy, basing on the reward that the policies generate. 
  - Since actions are stochastic, policies won’t give the same reward every time. Thus, we compare the **expected value**.
  - The optimum policy $\pi^*$ is the policy with the highest expected value.
  - At every stage the agent should perform $\pi^*(s)$.

- Example

##### 4.2.4. Optimal Policy

- Calculating the Utilities

  - In general we need to compute $U_r([s_0, s_1, ..., s_n])$ for general $U_r(\cdot)$. That is, the utility of a run.
    - Before $U_r(\cdot)$ was just the sum of rewards in every state.
    - Can consider **finite** and **infinite** horizons.
    - Turns out that infinite horizons are mostly easier to deal with.
  - Stationary VS. Non-stationary
    - We assume utilities are **stationary**, i.e., the same state has the same value. 

- Rewards

  - With stationary utilities, there are two ways to establish $U_r([s_0, s_1, ..., s_n])$ from $R(s)$. 

    - **Additive** rewards
      $$
      U_r([s_0, s_1, ..., s_n]) = R(s_0) + R(s_1) + ... + R(s_n)
      $$
       as above. 

    - **Discounted** rewards
      $$
      U_r([s_0, s_1, ..., s_n]) = R(s_0) + \gamma R(s_1) + ... + \gamma ^n R(s_n)
      $$
      where the **discount factor** $γ$ is a number between 0 and 1.

    - The discount factor models the preference of the agent for current over future rewards.

- Utility of a policy

  - In order to compare policies, we have the following methods to prevent incomputable situations: 

  - Proper policy

    - Always end up in a terminal state eventually, thus they have a finite expected utility.

  - Average reward

    - Compute the average reward per time step.
    - Even for an infinite policy this will (usually) be finite.

  - Discounted reward

    - With discounted rewards we compare policies by computing their expected values.

    > e.g.
    >
    > The expected utility of executing $π$ starting in $s$ is given by:
    > $$
    > U^\pi(s) = E \left[ \sum\limits^\infin_{t=0} \gamma^t R(S_t) \right]
    > $$
    > where $S_t$ is the state the agent gets to at time $t$.
    >
    > $S_t$ is a random variable and we compute the probability of all its values by looking at all the runs which end up there after $t$ steps.

- The optimal policy is then 
  $$
  \pi^* = \arg \max\limits_\pi U^\pi (s)
  $$
  
  - It turns out that this is independent of the state the agent starts in.
  - If we have the correct utility values, the agent has a simple decision process. 
  
- Example

---

### 4.3. Bellman Equation

##### 4.3.1. Markov Decision and Markov Decision Process*

##### 4.3.2. Introduction

- Motivation

  - Find the optimal policy (for a given set of rewards). 

    - If we have the correct utility values, the agent has a simple decision process. It just picks the action a that maximises the expected utility of the next state:
      $$
      \pi^*(s) = \arg \max\limits_{a \in A(s)} \sum\limits_{s'} P(s'|s, a) U^{\pi^*} (s')
      $$

    - Only have to consider the next step.

    - We then want to compute $U^{\pi^*}(s)$. 

  - Turns out that there is a neat way to do this, by first computing the utility of each state.

- Computation
  $$
  U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s'|s, a) U(s')
  $$
  where $\gamma$ is a discount factor. 

---

### 4.4. Value Iteration and Policy Iteration

##### 4.4.1. Value Iteration

- Motivation

  - In an MDP with $n$ states, we will have $n$ Bellman equations.
  - Hard to solve these simultaneously because of the max operation. Thus we want to make them non-linear. 

- Mechanism

  - Start with arbitrary values for states.

  - Then apply the Bellman update:
    $$
    U_{i+1}(s) \leftarrow R(s) + \gamma \max\limits_{a \in A(s)} \sum\limits_{s'} P(s'|s, a) U_i(s')
    $$
    simultaneously to all the states.

  - Continue until the values of states do not change.

  - The values are guaranteed to converge on the optimal values (but might take some time).

- Implementation

  ```
  procedure VALUE ITERATION 
  	for s in S do
  		U(s) <- 0
  	end for
  	repeat 
  		Ucopy <- U
  		for s in S do
  			U(s) <- R(s) + γ max sum(P (s'|s, a)Ucopy(s'))
  		end for
  	until U == Ucopy 
  end procedure
  
  ```

  - States, $S$, reward, $R(s)$, set of actions, $A(s)$ and transition model, $P(s'|s, a)$, are exactly as defined earlier.
  - $\gamma$ is the discount factor.

- Rewards

  - The example so far has a negative reward $R(s)$ for each state. Encouragement for an agent not to stick around.

  - Can also think of $R(s)$ is being the cost of moving to the next state (where we obatin the utility):
    $$
    R(s) = -c(s,a)
    $$
    where $s$ is the action used.

  - Bellman becomes
    $$
    U_{i+1}(s) \leftarrow R(s) + \gamma \max\limits_{a \in A(s)} \sum\limits_{s'} (P(s'|s, a) U_i(s')) - c(s, a)
    $$

  > Note.
  >
  > The action can be dependent on the state.

##### 4.4.2. Policy Iteration

- Motivation
  - Rather than compute optimal utility values, **policy iteration** looks through the space of possible policies.
  
- Process
  
  - Starting from some initial policy $\pi_0$ we do:
  
    - Policy evaluation
  
      Given a policy $\pi_i$, calculate $U_i(s)$.
  
    - Policy improvement
  
      Given $U_i(s)$, calculate $\pi_{i+1}$
  
    - Repeat until convergence. 
  
  - Policy evaluation
  
    - Given a policy, the choice of action in a given state is fixed (that is what a policy tells us) so:
      $$
      U_i(s) = R(s) + \gamma \sum\limits_{s'} P(s'|s, \pi_i(s)) U_i(s')
      $$
  
    - There are lots of simultaneous equations, but now they are linear (no max) and so standard linear algebra solutions will work. 
  
  - Policy improvement
  
    - Calculate a new policy $\pi_{i+1}$ by applying:
      $$
      \pi_{i+1}(s) = \arg \max\limits_{a \in A(s)} \sum\limits_{s'} P(s'|s, a)U_i(s')
      $$
  
    - For each state we do a one-step MEU lookahead. This is a simple decision.
  
    - Use the values established by policy evaluation.
  
- Properties
  
  - The iteration will terminate when there is no improvement in utility from one iteration to the next.
  - At this point the utility $U_i$ is a fixed point of the Bellman update and so $\pi_i$ must be optimal.
  
- Approximate Solution

  - Issue with policy evaluation

    - There is a problem with the policy evaluation stage of the policy iteration approach.
    - If we have $n$ states, we have $n$ linear equations with $n$ unknowns in the evaluation stage. Thus solution is in $O(n^3)$.
    - So, we want an approximate solution.

  - Approximate policy evaluation

    - Run a simplified value iteration - Policy is fixed, so we know what action to do in each state.

    - Starting from some initial policy $\pi_0$ we do:

      - Approximate policy evaluation 

        Repeat
        $$
        U_{i+1}(s) \leftarrow R(s) + \gamma \sum\limits_{s'} P(s'|s, \pi_i(s))U_i(s')
        $$
        a fixed number of times. The difference to policy iteration is the arrow.

      - Policy improvement
        $$
        \pi_{i+1}(s) = \arg \max\limits_{a \in A(s)} \sum\limits_{s'} P(s'|s, a)U_i(s')
        $$
        for every state $s$.

      Until convergance

    - Often more efficient than policy iteration or value iteration.

- Solving MDPs

  - Have covered three methods for solving MDPs
    - Value iteration (Exact)
    - Policy iteration (Exact)
    - Modified policy iteration (Approximate)

  - Which to use is somewhat problem specific.

