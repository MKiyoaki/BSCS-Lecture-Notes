## WEEK IX - Ensemble Methods

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK VIII - Support Vector Machines](year3/7ccsmpnn/w8.md)
>
>[➡️ WEEK X - Clustering](year3/7ccsmpnn/w10.md)
>
>Outlines:
>
>1. 
>

### 9.1. Introduction to Ensemble Methods

##### 9.1.1. Ensemble Methods

- Motivation

  - Idea

    - Generate a collection of *weak* classifiers from a given dataset.
    - Each *weak* classifier is a simple classifier, e.g., a half-space classifier, (can be slightly better choosing randomly/blindly), given by a different dataset (generated by resampling).
    - Combine classifiers to be a final one, e.g., by averaging or voting. (Multi-classifier system)

  - Reduces variance: Higher precision of the match; results are less dependent on peculiarities of a single training set

  - Reduces bias: more accurate or better quality of the match

  - Improves the estimate if the learning algorithm is unstable

    - A learner is said to be unstable if a small change to training set causes large change in the output classifier. 

    > e.g.
    >
    > Decision trees and neural networks are unstable learners; k- nearest neighbor and naive Bayesian classifiers are stable learners

- Definition

  - Methods that combine multiple diverse classifiers to obtain better classification accuracy.
  - Two methods: bagging and boosting
    - Bagging (bootstrap aggregating)
      - Uses multiple versions of a training set.
      - Each created by drawing $n' < n$ samples from $\mathcal{D}$ with replacement.
      - Each bootstrap data set is used to train a different weak classifier.
      - Final classification decision is based on the vote of each weak classifier.
    - Boosting
      - First create a classifier with accuracy on the training set greater than average.
      - Then add new component classifiers to form an ensemble whose joint decision rule has arbitrarily high accuracy on the training set.

---

### 9.2. Bagging & Boosting

##### 9.2.1. Procedure

- Bagging

  1. Start with dataset $\mathcal{D}$.
  2. Generate $M$ dataset $\mathcal{D}_1, ..., \mathcal{D}_M$.
     - Each distribution is created by drawing $n' < n$ samples from $\mathcal{D}$ with *replacement*.
     - Some samples can appear more than once while others do not appear at all.
  3. Learn weak classifier for each dataset.
     - Weak classifiers $f_i(\textbf{x})$ for dataset $\mathcal{D}_i, i \in \{1, 2, ..., M\}$.
  4. Combine all weak classifiers using a majority voting or averaging scheme.
     - Averaging: $f_\text{final} ( \textbf{x}) = \text{sgn} \left( \sum^M_{i=1} \frac{1}{M} f_i(\textbf{x}) \right)$

- Boosting

  - Goal: improve the accuracy of any given learning algorithm.
  - The classification performance is *boosted* by combining a number of weak classifiers (having accuracy only slightly better than chance is sufficient).
  - Add weak classifier one by one to form the overall classifier such that higher classification accuracy can be achieved.
  - Each weak classifier is trained with *informative* dataset complementary to others.

  > e.g.
  >
  > A 2 dimensional 2-category classification task with 3 weak classifiers
  >
  > ...
  >
  > The final decision of classification is based on the votes of the weak classifiers.
  >
  > > by the first two weak classifiers if they agree, and by the third weak classifier if the first two disagree.

##### 9.2.2. Adaboost

- Definition

  - **AdaBoost** is a short form of *Adaptive Boosting*.
  - It allows adding weak classifiers to the final classifier until a desirable classification accuracy is achieved.
  - Each training pattern is assigned a weight. 
    - A lower weight is assigned to patterns being identified correctly; 
    - A higher weight is assigned to patterns not being identified correctly. 
    - So AdaBoost classifier takes more care about the *difficult* patterns.
  - It constructs a strong classifier by combining weak classifiers.

  ![Adaptive Boosting, Simply Explained through Python | by Vagif Aliyev |  Medium](https://miro.medium.com/v2/resize:fit:850/1*DwvwMlOcT1T9hZwIJvMfng.png)
  
- Procedure

  1. Given dataset and labels.
  2. Initialise weighted error weights and maximum number of weak classifiers.
  3. Select the weak classifier with minimum error. If error rate > 0.5, abandon this weak classifier and output strong classifier as weighted sum of selected weak classifiers.
  4. Otherwise, update weighted error weights and calculate the weight of the selected weak classifier.
  5. Repeat 2. 3. 4. If maximum number of weak classifiers is not reached. Then, output strong classifier as weighted sum of selected weak classifiers

- Evaluation

  - Pros
    - Simple for implementation
    - Good classification accuracy and generalisation
    - Can be applied to different classifiers
  - Cons
    - Solution is suboptimal
    - Sensitive to noisy data and outliers

- Structure

  ```mermaid
  graph LR
  	x --> 1[h_1]
  	x --> 2[h_2]
  	x --> k[...]
  	x --> max[h_kmax]
  	1 -- α1 --> Σ
  	2 -- α2 --> Σ
  	k --...--> Σ
  	max -- αkmax --> Σ
  	Σ --> sign
  	sign --> H
  ```

  - Final classifier: 
    $$
    H(\textbf{x}) = \text{sgn} \left( \sum_{k=1}^{k_\text{max}} \alpha_k \hat{h}_k (\textbf{x})\right)
    $$

##### 9.2.3. Stacked Generalization

- Definition
  - In stacked generalisation, the overall classification is done in two levels. 
  - The input will be processed by a group of classifiers in the first level. 
  - The output from the first layer will go though the classifier in the second level to make the final decision.

- Training procedure 
  - Training dataset $\mathcal{D}$ with $n$ samples, $K$ classifiers (first level) are used in the first level:
    1. Prepare $K$ sub-training dataset: $\mathcal{D}_1, \mathcal{D}_2, ..., \mathcal{D}_K$ of different size and samples.
    2. Train the $k$-th classifier using training dataset $\mathcal{D}_k, k \in \{1, 2, ..., K\}$.
    3. Create a new training dataset for the second-level classifier. The outputs of the first-level classifiers ($z_1, z_2, ..., z_k$) and the target output ($y$) of the classification problem (supervised) are taken to form the new dataset $\mathcal{D}_\text{new}$.
    4. Train the second-level classifier using the dataset $\mathcal{D}_\text{new}$.


##### 9.2.4. Structure of Ensemble Classifiers

- Parallel Structure
  - All classifiers will make decision independently. 
  - The result will be combined by the combiner.
  - Combination strategics includes averaging, voting, weighted voting, adaptive weights, etc.
  - The introduced ensemble approaches such as bagging, boosting and AdaBoost approaches use this structure.
- Serial Structure
  - A particular structure of hierarchical (tree) structure.
  - Information and decision result are propagated to the later level for making decision
  - Rough decision can be make in the upper levels for efficiency.
  - Accuracy can be fine tuned in the lower levels.
  - However, error will propagate.
- Hierarchical
  - A general structure of parallel/serial structure.
  - Combination of methods of parallel and serial structure can be used.

##### 9.2.5. Random Forests

- Decision Trees

  - A decision tree has a tree structure consisting of nodes linked by branches:
    - Starts at root node
    - Ends at leaf nodes
  - Training data consist of feature vectors and class labels:
    - Training dataset: $\mathcal{D} = \{(\textbf{x}_1, y_1), (\textbf{x}_2, y_2), ..., (\textbf{x}_N, y_N)\}$
    - Class label: $y_i \in \{\omega_1, \omega_2, ..., \omega_k\}$
      - $\textbf{x}_i = \begin{bmatrix} x_{1i} \\ x_{2i} \\ ... \\ x_{di} \end{bmatrix}, i \in \{1, 2, ..., N\}$
      - $N$: the number of training samples
      - $d$: the dimensionality of the feature space

- Classification with Decision Trees

  - Classification: Non-leaf nodes make decisions based on the value of an element of $\textbf{x}$ (input feature vector).

  - Typically decisions are binary (resulting in a **binary decision tree**)

    > e.g.  if $x_4 > 11$, go *right*, else go *left*. 

  - Leaf nodes perform classification

    - May represent one class label or $P(\omega|\textbf{x})$

- Training with Decision Tree

  - Determine tree structure

  - Determine decision rules. 

    - Typically, choose feature, $x_j$, and threshold, $t$, that best splits the data into distinct classes.

      > e.g. most salmon on left ($x_j \leq t$), most sea bass on right($x_j > t$).

    - Several metrices for best split, such as

      - Gini impurity
      - Information gain
      - Variance reduction

  - Calculate $P(\omega|\textbf{x})$ for each leaf node.

    - For each training exemplar, pass $\textbf{x}$ down the tree and count how many exemplars from each category arrive at each node.
    - Give sample class label associated with leaf node, or
    - Or given $P(\omega|\textbf{x})$ associated with leaf node, give sample class label of most probable class.

  > It is not restricted to dichotomies and can be used to classify dataset with more than two categories.

- Evaluation with Decision Tree

  - Cons
    - Decisions at nodes could be more complex: implemented by a more complex classifier.
    - Prediction performance can be poor (tends to over-fit training data)
    - Unstable (small changes to training set cause large changes in the classification accuracy)
      - Bagging improves unstable classifiers

- Random Forests

  - A random forest is <u>an ensemble of bagged decision trees</u> with randomised feature selection.
    - Bagging: randomly drawn subset of the training data
      - Typically, each tree trained on 63.2% of training data
    - Randomised feature selection: randomly drawn subset of features
      - Typically, number of features in subset $d' = \sqrt{d}$ or $\log_2(d)$ (round off to the nearest integer)
  - Procedure
    - If all the training samples are placed in a matrix, $\textbf{X}$ (columns are samples, rows are features).
    - Remark: Both resampling and randomised feature selection increase diversity among decision trees (component classifiers)