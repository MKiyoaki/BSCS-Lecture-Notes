## WEEK VI - Deep Generative Neural Networks

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK V - Deep Discriminative Neural Networks](year3/7ccsmpnn/w5.md)
>
>[➡️ WEEK VII - Feature Extraction](year3/7ccsmpnn/w7.md)
>
>Outlines:
>
>1. Introduction to Generative Neural Networks
>1. Generative Model Taxonomy
>1. GANs
>1. GAN Problems

### 6.1. Introduction to Generative Neural Networks

##### 6.1.1. Motivation of Generative Models

- GANs
  - Generative Models: produce net contents. 
  
- Application
  - Text to image generation
  - Image to image translation
  - Increasing image resolution
  - Predicting next video frame
  - ...

---

### 6.2. Generative Model Taxonomy

##### 6.2.1. Overview

> Maximum Likelihood
>
> > Explicit Density
> >
> > > Trackable Density
> > >
> > > Approximate Density
> > >
> > > > Variational Autoencoder
> > > >
> > > > Markov Chain
> >
> > Implicit Density
> >
> > > Direct (GAN)
> > >
> > > Markov Chain

- Maximum Likelihood

  - **Maximum Likelihood (MLE)** estimates the parameters of the model, which gives the highest probability to get the observed data samples. 
  - For a given model $p_\text{model}(x|\theta)$ with the parameter of $\theta$, the MLE is defined as follows:

  $$
  \theta^* = \arg\max_\theta \mathbb{E}_{x \sim p_\text{data}} \log p_\text{model} (x|\theta)
  $$

  - For generative models, the objective is to generate data samples similar to those in the training set. This is typically achieved by finding the model parameters that maximize the likelihood of the training data.

- Generative Modeling

  - Density Estimation
    - The objective is to learn the probability distribution $p(x)$ for a given dataset.
    - MLE can be used to estimate the probabiltiy distribution.
  - Sample Generation
    - Generate new data samples basing on the learnt distribution, sampling $x \sim p(x)$. 
  

##### 6.2.2. Trackable Density

- Definition

  - An explicit formula is needed. 

  - Explicit model based on a chain
    $$
    p_\text{model}(\textbf{x}) = p_\text{model}(x_1) \prod^n_{i=2} p_\text{model}(x_i | x_1, ..., x_{i-1})
    $$
    where $p_\text{model}(\textbf{x})$ is the likelihood of the input image $\textbf{x}$, $p_\text{model}(x_i | x_1, ..., x_{i-1})$ is the probability of $i$-th pixel value given all previous pixels. 

  > e.g. Examples:
  >
  > Fully visible belief nets: NADE, MADE, <u>PixelRNN</u>
  >
  > Neural Autoregress
  >
  > Change of variable models

- Evaluation

  - Cons:
    - Propagation is needed to generate a sample.
    - Generation is not controlled by a latent code. 

##### 6.2.3. Approximate Density

- Autoencoder

  - Autoencoder (AE) is a generative network, which contains encoder and decoder modules. 

    ![EveryThing about AutoEncoders | by Tejpal Kumawat | Medium](https://miro.medium.com/v2/resize:fit:600/0*LtrxkZrn87VTYML6.png)

    - Encoder is used to compress the input data $x$ into a low dimensional latent vector $z$. While decoder recover the latent vector $z$ into the estimate of original data sample $\hat{x}$. 
    - The network training process is to minimize the reconstruction error.

      > e.g.
      >
      > L2 loss function:
      > $$
      > J(\theta) = \sum^N_{i=1} \parallel x_i - \hat{x}_i \parallel^2
      > $$

  - Variational Autoencoder performs better than standard AE. 

- Variational Autoencoder (VAE)

    - Definition

      - VAE introduces probability distribution to model the latent space, in order to rebuild and generate the data. 
        $$
        p_\theta (x) = \int p_\theta (z) p_\theta (x | z) dz
        $$
        
        where
        
        - $p_\theta(x)$ is the density function (model),
        - $z$ is the latent code,
        - $\theta$ is the model parameter vector.
        
      - This is an <u>intractable density</u> function and thus cannot be optimised directly but have to derive and optimise the lower bound of the likelihood instead.
      
    - Difference with AE

      - AE gives the latent output as a fixed vector $z$, while VAE gives a distribution $q(z|x)$.
      - For VAE, latent space samples a latent code from the distribution space $z \sim q(z|x)$.
      - Decoder recovers the input according to $p(x|z)$.
      


---

### 6.3. Generative Adversarial Networks (GANs)

##### 6.3.1. Introduction to GANs

- Motivation

  - We want to sample from complex and high-dimensional training sample distribution.
  - Thus, we can sample from a simple distribution, e.g., random noise, and transform it to training distribution by a generator network.
  - Simple idea for generating samples $x$ using latent code $z$. 
    - However, there is no guidance to train the generator.
    - Randomly generated latent code $z$ generates output noise.

- Definition

  ![Generative Adversarial Networks (GANs) for Audio-Visual Speech Recognition  in Artificial Intelligence IoT](https://pub.mdpi-res.com/information/information-14-00575/article_deploy/html/images/information-14-00575-g001.png?1697781602)

  - GAN introduces a two-player structure (actor-critic approach).
  - Consists of two main elements:
    - **Generator**
      - Maps random noise $z$ to produce a meaningful sample.
      - If not well trained, the produced sample is a noise.
    - **Discriminator**
      - To decide if the sample $x$ is real or produced from the Generator (fake). 
      - The objective is to train the generator until the generated samples cannot be identified by the discriminator. 
  - Notions
    - Data
      - Real sample: $\textbf{x} \sim p_\text{data}(\textbf{x}), \textbf{x} \in \mathcal{X}$
      - Latent code: $\textbf{z} \sim p_\textbf{z}(\textbf{z}), \textbf{z} \in \mathcal{Z}$
        - $p_\textbf{z}(\textbf{z})$ can be a uniform or Gaussian distribution.
    - Generator $G(\textbf{z}, \theta_g)$
      -  $\theta_g$ denotes the weights for the neural network of Generator, or mathematical expression with parameters. 
         -  Define implicitly a distribution $p_g(\textbf{x}, \theta_g)$
         -  Generator maps the $\mathcal{Z}$ space to $\mathcal{X}$ space, i.e., $G: \mathcal{Z} \to \mathcal{X}$.
      -  The output of Generator is denoted as $G(\textbf{z})$.
         -  When feeding $\textbf{z}$  into the Generator, its output is a sample, $\textbf{x} = G(\textbf{z})$. 
         -  Note that $\textbf{x}$ is a dummy variable, both real and generated samples can use the same variable $\textbf{x}$ to denote a sample.
    - Discriminator $D(\textbf{x}, \theta_d)$
      -  $\theta_d$ denotes the weights for the neural network of Discriminator, or mathematical expression with parameters. 
         -  Decide if the sample is real or fake, i.e., $D: \mathcal{X} \to R$.
      -  $D(\textbf{x}) \in [0, 1]$
        - It denotes the output of Discriminator. 
        - When feeding $\textbf{x}$ into the Discriminator, it returns a value between 0 and 1, where 0 denotes a fake sample while 1 denotes a real sample.
    - Ideal case: $p_\text{data}(\textbf{x}) = p_g(\textbf{x}, \theta_g)$

##### 6.3.2. Training GANs

- Cross Entropy

  - The loss function for GANs, defined as follows:
    $$
    V(D, G) = \mathbb{E}_{\textbf{x} \sim p_\text{data} (\textbf{x})} \underbrace{[ \log D(\textbf{x})]}_\text{for real samples} + \mathbb{E}_{\textbf{z} \sim p_\textbf{z}(\textbf{z})} \underbrace{[\log (1- D(G(\textbf{z}))]}_\text{ for fake samples }
    $$
    where $\mathbb{E}(\cdot)$ denotes the expectation operator.

  - Unless otherwise stated, $\log(\cdot)$ refers to natural logarithm $\log_e(\cdot)$. *Only applies to this GAN topic.*

  - Minibatch can be used in training, say, calculating the gradient of $V(D, G)$.

- Training Process

  - Formulated as a **minimax** game
    - Maximise $V(D, G)$, i.e., $\max_D V(D, G)$ by adjusting the parameter of $D$, $\theta_d$.
      - For discriminator, we aims to predict real samples close to 1, predict generated samples close to 0. 
      - Thus, we expect
        - $D(\textbf{x})$: ⬆️ the more the better
        - $G(\textbf{z})$: ⬇️
        - $\log(D(\textbf{x}))$: ⬆️
        - $\log( 1 - G(\textbf{z}))$: ⬆️
    - Minimize $V(D, G)$, i.e., $\min_G V(D, G)$ by adjusting the parameter of $G$, $\theta_g$. 
      - For generator, we aims to let discriminator gives a prediction close to 1. 
      - Thus, we expect
        - $G(\textbf{z})$: ⬆️
        - $\log(1 - G(\textbf{z}))$: ⬇️
  - Idea of training procedures: Repeat
    - Maximise $V(D, G)$ by adjusting $\theta_d$ (for a few times)
    - Minimize $V(D, G)$ by adjusting $\theta_g$. 
  - Backpropagation

---

### 6.4. GAN Problems

##### 6.4.1. Problems

- Overview

  - Non-convergence
    - Oscillation of model parameters
  - Mode collapse
    - Generator collapses causing the generation of limited varieties of samples
  - Diminished gradient (gradient saturation)
    - The generator gradient vanishes and does not learn
  - Unbalance between the generator and discriminator
    - Causing overfitting (say, the discriminator works too well)
  - Highly sensitive to hyperparameters
    - Learning rate, network parameters (number of layers, hidden node, activation functions, filter size, ...)
    - Will cause the above

- Non-convergence

  - Chasing game of two players, i.e., Discriminator and Generator 
  - Zero-sum game: one player wins and another loses

- Diminished gradient:

  - Regarding to the gradient descent calculation:
    $$
    \Delta \theta_g \frac{1}{m} \sum^m_{i=1} \log (1 - D(G(\textbf{z}^{(i)})))
    $$

  - The gradient is flat when the generator does NOT work well.

  - The gradient is getting steep when the generator works well, since $\log (1 - D(G(\textbf{z})))$ is getting small.

  - Solutions

    - Network Design

      - DCGAN (Deep convolutional generative adversarial networks)

        > Architecture guidelines for stable Deep Convolutional GANs 
        >
        > - Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). **Transposed convolution** is used for upsampling.
        > - Use batchnorm (Batch Normalisation) in both the generator and the discriminator. 
        > - Remove fully connected hidden layers for deeper architectures. 
        > - Use ReLU activation in generator for all layers except for the output, which uses Tanh. 
        > - Use LReLU activation in the discriminator for all layers.

      - Remark: In some experiments, upsampling followed by a conv layer can work better than transposed convlution. 

    - Cost Functions

      - One-sided label smoothing
        - The Discriminator is penalised when its output is over 0.9 for real samples to avoid overconfidence. 
      - Feature Matching
        - Minimise the difference between the features of the real and generated samples when training the Discriminator.
        - Cost function: $\parallel \mathbb{E}_{\textbf{x} \sim p_\text{data}} f(\textbf{x}) - \mathbb{E}_{\textbf{z} \sim p_\textbf{z} f (G(\textbf{z}))} \parallel^2 $

    - Optimization

      - Experience replay
        - This technique is used extensively in Reinforcement Learning. A buffer is used to store the previously generated samples. 
        - The stored samples (experiences) and the current samples generated by the Generator will be used for training.
      - Training with Labels
        - Conditional GAN (CGAN)
        - Provide extra information, e.g., label  of the (real/generated) samples to facilitate training.
        - When the label $y$ is fed into the Generator, the generated samples are of class $y$.
      - Adding noise (when training the Discriminator)
        - After adding noise, the gradient of the Discriminator’s cost function is proportional to the gradient of Jensen Shannon Divergence (JSD). 
        - The JSD is not zero when the distribution of real and generated samples are very different.
        - Noise with zero-mean normal distribution with variance $\sigma^2$.
      - Unrolling GAN
        - Stabilises training. Alleviates the problem of mode collapse. 
        - Aim: It is to train Generator better to achieve the above advantages
        - Idea:
          - Generator **looks ahead a few steps** (say, $N$ steps) to evaluate how the Discriminator reacts, i.e., unrolling.
          - Then, Generator is updated according to the evaluated cost (predicted cost), i.e., the cost obtained in the $N$-th step
          - The Discriminator is updated the same way as the original GAN.



