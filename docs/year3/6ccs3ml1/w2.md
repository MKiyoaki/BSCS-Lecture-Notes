## WEEK II - Inductive Learning

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK I - Introduction to Machine Learning](year3/6ccs3ml1/w1.md)
>
>[âž¡ï¸ WEEK III - Probabilisitc Models 1](year3/6ccs3ml1/w3.md)
>
>Outlines:
>
>1. Decision Trees
>    - Inductive Learning
>    - Decision Trees
>    - Information and Entropy
>    - Reduced Error Pruning
>    - More Decision Trees Approaches
>2. Regression
>    - Linear Regression
>    - Gradient Descent
>    - Multivariate Linear Regression
>3. Classification
>    - Linear Classifiers
>    - Logistic Regression
>    - Ensemble Learning
>    - Boosting
>    - Bagging
>

> Ref: [7CCSMPNN: Week2](year3/7ccsmpnn/w2.md)

### 2.1. Decision Trees

##### 2.1.1. Inductive Learning

- Defintion
  - Learning from examples.
  - Inductive learning hypothesis
    - Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.
  - Including <u>simple classifiers and regression models</u>.
  - Optimisation aims to identify the *best* model parameters.

##### 2.1.2. Decision tree for Iris example

- Motivation
  - Approximating discrete-valued target functions.
  - Learned function represented by a decision tree.
- Components
  - *Node*: a test of the value of a feature for a data point.
  - *Leaf node*: specifies the value to be returned.
- Features
  - Disjunction of conjunctions of constraints on the feature values of instances or data points.
  - Learned trees can be re-represented as sets of ifâ€“then rules.

##### 2.1.3. Decision trees

![Decision-Tree-2](https://media.geeksforgeeks.org/wp-content/uploads/20250107141748035578/Decision-Tree-2.webp)

- Process

  - After we pick a feature, one of five things can be the case:
    1. If all remaining examples are *positive*, then the answer is "yes".
    2. If all remaining examples are *negative*, then the answer is "no".
    3. If there are some *positive* and some *negative* examples, then choose the best feature to split them.
    4. If there are no examples left, then there are no examples that fit this case.
       - Ending up here means we have no data on the specific case we are asking about. 
       - Solution is **plurality classification**, i.e., return a default value. It could be the best guess based on the parent node.
         - Could be majority, random pick, or a probability. 
    5. If there are no features left, but there are still positive and negative examples, these examples have the same description but different classifications.
       - Can be due to noise, or unobservability of features.
       - Solution is **plurality classification**. 

  ```
  function DTL(examples, attributes, parent examples) returns a decision tree
  	if examples is empty then return Plurality-Value(parent eamples)
  	else if all examples have the same classification then return the classification
  	else if attributes is empty then return Plurality-Value(examples)
  	else
  		best <- Most-Important-Attribute(attributes, examples) 
  		tree <- a new decision tree with root test best
  		for each value vi of best do
  			examplesi <- {elements of examples with best = vi} 
  			remain <- attributes - best
  			subtree <- DTL(examplesi , remain, examples)
  			add a branch to tree with label vi and subtree subtree
  	return tree
  ```

- Choosing the Best Feature

  - Motivation
    - A good feature splits the examples into subsets that are (ideally) *all positive* or *all negative*.

##### 2.1.4. Information and Entropy

- Entropy

  - We will measure the information in a feature by looking at how it reduces **entropy**, which measures uncertainty.

  - Entropy in a probability distribution $\langle P_1, ..., P_n \rangle$ is 
    $$
    H(\langle P_1, ..., P_n \rangle) = \sum^n_{i=1} - P_i \log_2 P_i
    $$

  - Maximum for a uniform distribution, minimum for a single point.

  - Example

    > e.g.
    >
    > Suppose we have a set $S$ of $p$ positive and $n$ negative examples at the root. If we had to pick the class at this point, we could compute the probability based on the positive and negative examples.
    >
    > > Chance of $\text{Class}=1$ would be $p(\text{Class}=1) = \frac{p}{p + n}$. 
    > >
    > > So we have a probability distribution over $c$ classes, and we cancompute the entropy of $S$.
    > > $$
    > > H(S) = H(\langle \frac{p}{p+n}, \frac{n}{p+n} \rangle) = \sum^c_{i=1} - p_i \log_2 p_i
    > > $$
    > > where $p_i$ is the proportion of the examples in $c_i$.

- Information Gain

  - A feature $A$ splits the examples $S$ into subsets $S_i$. 

    - Each of these subsets is a new branch in the decision tree. 
    - Each of these subsets will have its own entropy.

  - We can measure the goodness of $A$ by the reduction (if any) in the entropy of $S$ and the entropy of all the $S_i$.

  - We call this difference **information gain**.
    $$
    Gain(S, A) = H(S) - \sum_i \frac{S_i}{|S|} H(S_i)
    $$
    So the entropy of the $S_i$ collectively is the weighted sum of their entropies. 

  - The weight is the <u>proportion of examples in that set</u>.

  - Example

    > e.g.
    >
    > If $S_i$ has $p_i$ positive and $n_i$ negative examples, i.e.,
    > $$
    > H(S_i) = H(\langle \frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i}\rangle)
    > $$
    > is the entropy of $S_i$. 
    >
    > > Thus, 
    > > $$
    > > Gain(S, A) = H(S) - \sum_i \frac{p_i + n_i}{p + n} H(\langle \frac{p_i}{p_i + n_i}, \frac{n_i}{p_i + n_i}\rangle)
    > > $$
    > > The feature $A$ with the biggest $Gain(S,A)$ is the one to pick.

##### 2.1.5. Other Metrics for Picking Attributes

- Gain ratio

  - Motivation

    - With enough precision, Time would uniquely identify each case. This would look very good, but would be useless.

  - Definition

    - We can add in a metric that penalises features with lots of values. We then combine this with Gain to get a new metric:
      $$
      SplitInfo(S, A) = - \sum_i \frac{|S_i|}{|S|} \log_2 \frac{|S_i|}{|S|}\\
      GainRatio(S, A) = \frac{Gain(S, A)}{SplitInfo(S, A)}
      $$
    - We can use *GainRatio* as the way of picking the best feature. The feature with the **highest Gain Ratio** is the best choice.

- Gini Impurity
  - Definition
    - Alternative to entropy to choose features.
    - Suppose we have a set $S$ of p positive and $n$ negative examples at the root. We assign a probability based on the number of examples.
    - Gini impurity is <u>the probability of this classification mislabelling a randomly selected example</u>.
    
  - Example
    
    > e.g.
    >
    > Chance of Class $\text{yes}$ would be
    >
    > $p(\text{Class = yes}) = \frac{p}{p + n}$
    
  - Computation
    
    - The general formula for Gini impurity when there are $c$ classes is
      $$
      G(S) = \sum^c_{i=1} p_i \sum_{k \neq 1} p_k
      $$
      Often easier to apply as
      $$
      G(S) = 1 - \sum^c_{i=1} p_i^2
      $$
    
      > e.g.
      >
      > If $c=2$, then $G(S) = 1 - ((\frac{p}{p+n})^2 + (\frac{n}{p+n})^2)$
    
    - To pick a feature we calculate $G(S,A)$ for each feature. The feature with the **lowest Gini impurity** is the best choice.
      $$
      G(S, A) = \sum_i \frac{|S_i|}{|S|} G(S_i)
      $$
      where $|S_i|$ indicates the size of $S_i$, the number of things it contains.

##### 2.1.6. Reduced Error Pruning

- Motivation
  - DTL grows the tree just enough to perfectly classify all the examples. If the set is too small, or the set contains noise, then this can easily lead to **overfitting**.
  - Given two decision trees $d$ and $d_1$, $d$ is said to overfit the training data if
    1. $d$ has a smaller error than $d_1$ on the training data.
    2. $d_1$ has a smaller error that $d$ on all the other instances.
- Solution
  - Two main approaches to preventing overfitting.
    1. Stop growing the tree earlier.
    2. Allow to (possibly) overfit, and then **prune** the tree. This is commonly used. 
- Reduced error pruning
  - Process	
    - Consider every branch in the tree as a candidate for pruning. 
    - Remove the branch, and make the root of the branch into a leaf.
    - Use the plurality classification for examples at that leaf. Test the new tree:
      - Keep a branch pruned if the pruned tree performs better on some validation set than the original tree.
      - Repeat while it is possible to prune a branch and improve performance.
  - Rule post-pruning
    - Pruning with limited data problem
      - When data is limited, the lackness of data is a problem.
    - Solutions
      1. Build the decision tree as before.
      2. Create a rule set: One rule for each path from root to leaf.
      3. Remove preconditions (feature tests) from rules if that improves their accuracy.
      4. Sort rules by accuracy and apply them in order/sequence when classifying examples.

##### 2.1.7 Broadening Decision Tree Approach

- Multvalued Features

  - When features have many values, information gain gives an inappropriate estimation of the usefulness of the feature. 
  - Tend to split examples into small classes.
  - Convert to Boolean tests.

- Continuous/integer input features

  - Infinite sets of possible values.
  - Modify approach to identify **split points** which give highestinformation gain. Weight > 160

- Continuous output values

  - When trying to predict continuous output values need to create a **regression tree**, which ends with a linear function.

- DLT Variants

  > The basic algorithm given above is the core of the ID3 algorithm.
  >
  > ID3 includes using entropy to make the choice about which feature to split on.
  >
  > C4.5 extended ID3, adding post-processing to simplify the tree.
  >
  > CART is a similar method, with a complex relationship with ID3/C4.5. 

---

### 2.2. Regression

##### 2.2.1. Linear Regression

![Linear Regression in Machine learning](https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png)

- Definition

  - Objective is to learn a linear function of continuous inputs.

  - Equation is of the form (univariate):
    $$
    h_w(x) = w_1x + w_0
    $$
    where the $w$ subscript indicates the vector $[w_0, w_1]$.

  - Idea is that we want to estimate the values of $w_0, w_1$ from data.

- Process

  - Finding the $h_w$ that best fits the data is linear regression (choose $w$ so that $h_w$ is close to $y$ for our training examples).
  - To fit the line, we find the $[w_0, w_1]$ that minimize the loss/error.

- Cost Function

  - Definition
    - Traditionally we use the squared loss function $L_2$, where
      $$
      \begin{array}{lcl}
      J(h_w) = Loss(h_w) &= & \sum\limits_{j=1}^n L_2(y_j, h_w(x_j)) \\
      ~ &= & \sum\limits_{j=1}^n (y_j - h_w(x_j))^2 \\
      ~ &~ & \sum\limits_{j=1}^n (y_j - (w_1x_j + w_0))^2
      \end{array}
      $$
      summed over all the training examples, and where the data we have are pairs $(x_i, y_i)$ (aka. cost function)
    - i.e., We would like to find
      $$
      w^\star = \arg\min_\textbf{w} Loss(h_\textbf{w})
      $$
    
  - We use the squared loss function because Gauss showed that for normally distributed noise, this gives us the most likely values of the weights.
  
  - Plotting the lost vs. cost function (for simplicity, $h_w(x) = w_1 x$).


##### 2.2.2. Gradient Descent

- Process

  <img src="https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png" alt="Gradient Descent: A comprehensive guide to Implementing in Python" style="zoom:67%;" />

  - Start with some initial values for $\theta_0, \theta_1$. 

  - Keep changing these values so that the cost/loss function $J(h_w)$ is reduced. For each $w_i$ we update with
    $$
    w_i \leftarrow w_i - \alpha \frac{\partial}{\partial w_i} Loss(w)
    $$
    where $\alpha$ is the **learning rate** and controls how big a step we take downhill. 

    - Note that we MUST **simultaneously** update $w$. 

  - Continue this until you reach a **minimum**, i.e., convergence. 

  - Gradient descent algorithm can be used for other functions/problems too. 

  - If the slope at the minimum is 0 it means $\frac{\partial}{\partial w_i} Loss(w) = 0$ and the algorithm is end. 

- Learning Rate

  - $\alpha$ is the learning rate. 
    - If it is too small, then tiny steps downhill and the speed is low.
    - If it is too large, we might miss the minimum and may not converge. 

- Batch gradient descent vs. Stochastic gradient descent

  - Batch gradient descent
  
    - Consider all the training examples simultaneously.
  
    - At each step we consider all the training examples, and update the weights using:
      $$
      \begin{array}{lcl}
      w_0 &\leftarrow &w_0 + \alpha \sum_j (y_j - h_w(x_j)) \\
      w_1 &\leftarrow &w_1 + \alpha \sum_j (y_j - h_w(x_j))x_j 
      \end{array}
      $$
  
    - This is guaranteed to converge. But it can be slow since we need to compute for all $N$ examples at each step.
  
  - Stochastic gradient descent
  
    - At each step we consider all the training examples, and update the weights using:
      $$
      \begin{array}{lcl}
      w_0 &\leftarrow &w_0 + \alpha \sum_j (y_j - h_w(x_j)) \\
      w_1 &\leftarrow &w_1 + \alpha \sum_j (y_j - h_w(x_j))x
      \end{array}
      $$
      For $N$ examples at each step.
  
    - This is often quicker. But if learning rate is constant, may not converge.
  
    - Can often be made to converge by <u>decreasing</u> the learning rate over time.

##### 2.2.3. Multivariate Linear Regression

- Senarios

  - Assume we have variables $x_{j, 1}, ..., x_{j, i}, ..., x_{j, n}$ and are interested in a vector of weights $w_i$. 

  - Simplify the handling of the weights by creating a dummy attribute to pair with $w_0$, then $x_{j, 0} = 1$.

  - Then $h_w(x_j)$ is just the weighted sum of the variable values:
    $$
    h_w(x_j) = \sum^{i = n}_{i = 0}w_j x_{j, i}
    $$

- Evaluation

  - We learn by doing gradient descent, as before. We just adjust more weights each time.
  - We have to worry about **overfitting**.
  - We have to take the complexity of the model into account.

- Solution

  - We add a regularisation term to the loss function:
    $$
    Loss'(h) = Loss(h) + \lambda \text{Complexity}(h) \\
    \text{Complexity}(h_w) = \sum_i w_i^2
    $$
    Intuition: smaller $w$ will give simpler functions $h_w$. 

  - $\lambda$ is the regularisation parameter: trade-off between fitting the data and having a simple function. Then use $Loss'(h)$ when computing the weight updates.

---

### 2.3. Classification

##### 2.3.1. Linear Classifiers

- Definition
  - We can turn a linear function into a classifier.
  - Function defines the **decision boundary** that separates the two classes. A linear decision boundary will separate two **linearly separable** classes.
  - Classify based on where a point lies in relation to the line.
- Process
  - Starting with arbitrary weights:
    1. Use the decision rule (as in previous slide) on an example.
    2. If it classifies correctly, do not update weights.
    3. Otherwise update weights as before.
  - It is common to use stochastic gradient descent (one example per update) here.
- Learning Curve
  - Typically, the variation across runs is very large.
  - The curve is not smooth because the boundary is hard, so can misclassify a lot of examples even a long way into learning.
  - Worse if the data is noisy.

##### 2.3.2. Logistic Regression

- Motivation
  - The linear classifier always predicts 0 or 1, even for examples that are very close to the boundary. 
  - However, In many tasks, we need more gradated predictions. 
  - Soften the threshold function: approximate the hard threshold with a continuous function.
- Definition
  - Use the **logistic function** as a threshold: $h_w(\textbf{x}) = \frac{1}{1 + e^{-w \cdot \textbf{x}}}$
  - Convergence is slower, but behaves much more predictably.
  - Commonly uses **cross-entropy loss** function.

##### 2.3.3. Ensemble Learning

- Motivation

  - Linear separability cannot handle complex tasks. As a result, there is lots of work on methods for non-linear boundaries.

    > e.g. Neural networks/deep learning, Support vector machines. 

  - Note that decision trees can handle XOR functions.

- Definition

  - Every classifier has an error rate. It will always misclassify some examples
  - Using an **ensemble** is an easy way to improve on this.
    - Take $N$ classifiers, use them all on the same example.
    - Have them vote on the classification.

##### 2.2.4. Boosting

- Definition

  - **Boosting** extends the idea of a simple ensemble.
  - Builds on the idea of a **weighted training set**.
  - Higher weighted examples are counted as more important during training.
- Process

  - Starts with all examples of equal weight, and learns a classifier $h_1$. Test it.
  - Increase the weights of the misclassified examples and learn a new classifier $h_2$. Repeat.
  - Final ensemble is the combination of all the classifers, weighted by how well they perform on the training set.
- Variants
  - Many variants of the basic boosting idea.
  - The **AdaBoost** algorithm is a commonly used approach to
    boosting.
    - Given an initial classifier that is slightly better than random (weak model), AdaBoost can generate an ensemble that will perfectly classify the training set.

##### 2.2.5. Bagging

- Definition

  - From a training set $\mathcal{D}$, we create multiple training sets: $\mathcal{D}_1, ..., \mathcal{D}_n$
  - The $\mathcal{D}_i$ are sampled from $\mathcal{D}$ Uniform distribution, with replacement.
  - Results in multiple sets with the same properties.

- Results
  - Combining results
    - Typical combination for classifiers is by voting.
    - Can also combine regression ensembles, by averaging.
    - Both can be weighted (for boosting).

- Random forests
  - Bagging, applied to decision trees.
    - So, a set of decision trees learnt from bagged training sets.
  - Plus random selection of features for each tree. Features are selected randomly, with replacement (between trees).
    - So, bagging at the feature level.
  
- Random subspace method

  - Random selection of features for each tree. Features are selected randomly, with replacement (between trees). This is called the **random subspace method**. 

  - Can apply it to other kinds of ensemble.

