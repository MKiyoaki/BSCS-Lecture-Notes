## WEEK VII - Memory Virtulisation II

>[🏠 MENU - 5CCS2OSC](year2/5ccs2osc.md)
>
>[⬅️ WEEK VI - Memory Virtulisation I](year2/5ccs2osc/w6.md)
>
>[➡️ WEEK VIII - User-level Memory Allocations](year2/5ccs2osc/w8.md)
>
>Outlines:
>
>17. Lazy Loading
>      - Motivation
>        - Optimization to virtual memory space
>        - Method - trigger Page Fault
>      - Lazy Allocation
>      - Lazy Zeroing and Lazy Copying
>18. Demanding Paging
>      - Swap Data
>      - Implementation
>      - Demand-paged Virtual Memory
>19. Page Replacement and Frame Allocation
>    - Page Replacement Algorithm
>    - Frame Allocation Algorithm
>
>```mermaid
>graph LR
>	goal[Optimization]
>	method[PageFault]
>	lazy1[LazyAllocation]
>	lazy2[LazyZeros]
>	pr[PageReplacement]
>	fa[FrameAllocation]
>	dp[DemandPaging]
>	swap[SwapData]
>	
>	lazy1 --trigger--> method
>	lazy2 --trigger--> method
>	swap --trigger--> method
>	method --goal--> goal
>	dp --when the request addr is outside of RAM--> swap
>	pr --when RAM is full--> swap
>	fa --monitor the RAM division--> swap
>	
>	
>```

### 17. Lazy Allocation and Lazy Loading

> Ref:
>
> [页错误 Page Fault /缺页异常 详解 - Zhihu](https://zhuanlan.zhihu.com/p/498507998) 

##### 17.1. Motivation

- Virtual Memory Optimization

  - The OS can create **useful illusions** by handling page faults in creative ways. (Core idea)

    - Useful illusions - that the machine has more memory than the physical memory in the machine
    - Such idea can improve the effiency by triggering page faults errors. 

    > e.g.,
    >
    > - Lazy allocation, lazy loading...
    > - virtual memory via *page swapping* (a.k.a. just *paging* or *swapping*)
    > - memory-mapped files: the illusion that files on disk are resident in the address space

- Page Fault

    - <u>Access to an unmapped address</u> causes a **page fault**. 
      - Unmapped address
        - Each process has a virtual address space.
        - Usually, much of it is unused, *i.e., unmapped*. 
        - MMU hardware performs address translation. 
    - Process
      - A hardware interrupt causing the OS to regain control
      - Page faults create a *point of interposition* in the OS, between user programs and hardware.
      - OS may choose to *reserve* some virtual address space *without* allocating physical memory there.


- The OS might do the following processes then resume the process. 

    - Allocate physical memory (lazy allocation)
    - Allocate memory and populate with file data (memory-mapped file; lazy loading)
    - Keep emulating the access (rarer...)

##### 17.2. Lazy Allocation

- Process - make all allocation in the process *lazy*, all occurring in page-sized chunks.
  - for stack: allocate/grow as used
  - for heap: allocate/grow as used?
  - for program segments: copy as needed from disk
- Exceptions - the heap mapping doesn't work like this
  - It is made/grown explicitly by system call. 
  - Not grown lazily by page fault. 

##### 17.3. Lazy Zeroing

- Process
  - Illusion of a large *zero-initialized* region by mapping *read-only* the same *zero frame*, repeat for times. 
  - On first write, fault handler maps fresh frame.
  
- Evaluation
  - ✅ allocation appears fast
  - ✅ allows sparse use of memory
  
  > i.e. $O(n)$ zeroes fit in $O(1)$ memory, and zeros are readable at all times.
  
  >e.g.
  >
  >Consdering that, a process tried to *write* to the read-only zero mapping
  >
  >`mov $0x00120348,(%rax)  ; %rax points to our zero-init’d memory area`
  >
  >In the page fault handler, the operating system do the following
  >
  >1. Considered the *faulting address* and *which address space* (process)
  >2. Determined (how?) that this area was allocated *copy on write*
  >3. Allocated a fresh frame
  >4. Map it at the faulting page
  >
  >    *i.e., replacing the previous mapping of shared zero frame*
  >
  >5. Copied the shared contents (zeroes) into the new frame
  >6. Resumed the process at the faulting instruction. 
  >


- Lazy Coping

    - Lazy zeroing is really a special case of lazy copying, known as **Copy on Write (CoW)**. 
    - Process
      - Any shared data, not just zeroes, can be shared by many-to-one read-only mappings, then copy *at the time of write* (if it ever happens. )
      - Often used for text segments in executable files. Programs rarely write to their own text. 
      - Usually all *processes* that are instances of the same *program* can share it, saving memory.

---

### 18. Demand Paging

##### 18.1. Swap Data

- Definition

    - Another approach like lazy loading, in order to <u>giving the illusion to run more processes or bigger processes</u> (for optimisation on virtual memory).

      > The illusion is of the computer having more memory available to processes than it actually has physical memory.

- Motivation

  - By swapping data <u>from and to disk with the physical memory</u> (via the virtual memory space), with using page faults to trigger re-loading.
      - Since not all memory is being used at the same time, we could swap the data in the memory from and to the disk. 
      - Not-recently-used pages are *replaced* or *evicted*. (Suspiciously like in a cache)
      - We move data to a swap device. *i.e. a disk or other slow device further down the storage hierarchy*.

- Storage hierarchy
  - A systems using a mix of technologies to get the best performance of storage.

    >![Memory Hierarchy Design](https://media.geeksforgeeks.org/wp-content/uploads/20230609020524/Memory-Hierarchy-Design-768.png)
    >
    >Higher level - small, fast, expensive
    >Lower level - big, slow, cheap

    - Virtual memory as an extension - to free up **physical memory**
      - Combine main memory with reserved space on disk (swap area). 
        - pages not (currently) needed can be *swapped out* to disk, not taking up DRAM.
        - And brought back on a faulting access. 
        - Unlike cache memory, swap space <u>is managed by software (OS), not hardware</u>. 
  - Disk
    - Definition
      - Disks are *block* devices, meaning they read persistent and write data in large-ish units at a time (typically between 512 bytes and 4kB).
      - Feature
        - Unlike memory, they are *persistent* but are not byte- or word-addressable (only *block*-addressable). 
        - As exposed by the *disk controller*, via which the operating system communicates with the disk, a disk is a linear sequence of blocks.
        - Operating systems commonly provide a layer of abstraction above this
        - i.e., the filesystem - a tree of files (leaf nodes) and directories (branch nodes). 
        - The OS stores this tree as a block-level data structure on the disk.
      - As alternatives to holding a filesystem, block devices can be used as *swap areas* and also *databases* (a ‘relational’ alternative to filesystems)

##### 18.3. Swap Data Implementation

- Page Table ❌

  - Page tables as defined by the hardware are no longer sufficient to keep track of what’s in our virtual address space. 
  - As far as the hardware knows, page tables exist only to map each page number to either a framework number or to invalid (i.e., nothing mapped). 

- Two alternatives

  1. Stash extra information in *invalid* page table entries.
     - Process
       - If hardware sees an PTE whose valid bit is *not* set, it will just ignore it.
       - So as the OS, we're free to use the other bits however we like.
     - Implementation
       - Keep a separate data structure in the OS.
         > e.g. Linux has a red-black tree of VMA. 
       - Allows lazy creation of page table entries themselves.
     - Feature
       - If we opt to stash data in the PTE, it is entirely up to the OS what that data is and how it is encoded.
       - But it must be able to encode whatever we will need to know when a page fault comes in, so we can handle the fault. 
         > e.g., what disk block the data is at, but also things like what permissions apply, whether copy-on-write applies, etc.
  
       - It doesn't have to involve a *block* number at all. Something more abstract might sometimes be appropriate.
  2. <u>Demand-paged virtual memory</u> - Most of the ingredients of a modern virtual memory system.


##### 18.4. Demand-paged Virtual Memory

- Definition

    - **Demand-paged**
      - They transfer page-sized chunks of memory on demend. *(i.e., when a fault comes in)* 
      - Following with the lazy loading idea.

- Prefetching

    - Definition
      - Proactively transferring data *before* it is needed is called **prefetching**.
    - Reason
      - Being lazy is often a good choice for performance, but it’s not straightforward.
      - Fault handling makes processes wait on I/O, so frequent swapping will hurt performance. 
      - Being more eager (less lazy) could save time if <u>we can accurately predict which other pages will be needed</u>. 

- Application

    - Modern systems combine **demand paging** with **judicious prefetching**, often based on 
      - heuristics (simple), or
      - predictive models (fancier).

- Further ingredients

    - **Block cache**
      - For programs *accessing files on disk*
        - It’s good to keep an in-memory cache of those blocks (since disk is *much* slower than memory).
        - But DON'T also cache the swap file <u>if we are using a file for our demand-paging swap area</u>. 
    - **Memory-mapped files**
      - It’s a convenient programming abstraction to have files on disk appear as *data in memory*. 
      - We can demand-page the file data from/to disk.
        - Like a kind of inverse of when we swap out memory to disk.
        - But DON'T additionally cache the file in the block cache.
    - **Resolving interactions between these**
      - a *unified virtual memory system* treats cacheable disk I/O and swappable memory as (roughly) the same thing.
        - But DON'T additionally cache the file in the block cache.

---

### 19. Page Replacement & Frame Allocation

##### 19.1. Introduction to page replacement

- Introduction

    - When physical memory is full, in order to free up memory, we need to determine <u>the best choice for swapping the page out to disk.</u> This method is called **page replacement**. 
    - Page replacement is analogous to replacement in cache memory, but it is implemented in software inside the OS.

- Feature

  > Imagine that each resident page is conceptually stored in a linked list (kept by the OS)...

    - **FIFO** - Treat list as a queue, pick from the head
    - **Random** - Pick randomly
    - **Least recently used (LRU)**
      - Like FIFO, but somehow keep the list sorted in order of *use timestamp*. 
      - Follows locality of reference, which is good.


- LRU Implementation

    - Hardware limitation
      - Hardware doesn't give us timestamps but with the *Used bit (Referenced bit)*. 
      - Including bits stand for *is valid, perms, is dirty, is used, etc*. It additionally sets the Dirty bit whenever the page is written to.
      - The OS periodically clears all Used bits. So if the bit is set, we know the page was used *recently*. 

        > Notice: But we don’t know *how recently*, relative to others that also have the bit set.


  - Best approximation of LRU

    - As we need to pick a page, preferring those with a clear *Used bit*. To break ties, we can at best degenerate to Random or FIFO. Second-chance FIFO is a common approach for this. 
    - Second-chance FIFO
      - Process
        1. Walk the list from oldest to newest, like FIFO. 
        2. If we see a non-Used page, choose it. 
        3. If we see a Used page, give it a second chance, i.e., clear its U bit and keep walking, *sending it to the back ('new end') of the list*.
    - Optimisation
      - Instead use a circular list with a 'current position' pointer, from where we resume our search each time
         we need to pick. 
      - This is called **Clock** and avoids an expensive send-to-back list manipulation.

##### 19.2. Frame allocation

- Introduction to frame allocation

    - When physical memory is full, in order to free up memory, we need to determine <u>how many frames do we give to each process.</u> This method is called **frame allocation**. 

- Feature

    - Since taking a page fault and swapping in a page from disk is slow. If done frequently, performance hits the floor. 
    - Conceptually, each process has a set of pages that it is using now, i.e. that it needs in order to make uninterrupted progress over a small time interval (100ms). Size of its working set is called the **working set size (WSS)**. 

      >WSS: count *Used bits* after clearing them (say) 100ms ago (Imaging a cutting a vertical 100ms slice through the ‘locality of reference’ plot on p164.)

    - When physical memory is scarce, how many frames do we give to each process? In short, ‘equal shares’ is not a good policy!
      - when all processes get less than their WSS, system throughput will drop to nearly zero!
      - This is called *thrashing*: #runnable plummets and the system is spending ∼all its time waiting on I/O
    - Better to give *some* processes at least WSS frames. Others will have to wait. 
      - Many other complementary strategies exist for managing out-of-memory conditions; e.g. Linux has ‘OOM killer'. 
