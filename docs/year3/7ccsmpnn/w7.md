## WEEK VII - Feature Extraction

>[ðŸ  MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[â¬…ï¸ WEEK VI - Deep Generative Neural Networks](year3/7ccsmpnn/w6.md)
>
>[âž¡ï¸ WEEK VIII - Support Vector Machines](year3/7ccsmpnn/w8.md)
>
>Outlines:
>
>1. Feature Engineering, Selection & Extraction
>1. Feature Extraction Methods

### 7.1. Feature Engineering, Selection & Extraction

##### 7.1.1. Feature Engineering

- Definition

  - Modify some measurements to make them more useful for classification.

    > e.g.
    >
    > translate date to day-of-week 09/08/2019 to Friday
    >
    > > useful to make predictions about future sales based on past sales
    > >
    > > not useful to make predictions about chances of death based on date-of-birth

  - Feature engineering can result in dimensionality **increase**: length($x$) â‰¥ length($m$)

##### 7.1.2. Feature Selection

- Motivation

  - Some measurements may be more useful for classification than others.

  - Some measurements may not be selected because they are unobtainable or too costly to obtain:

    > e.g.
    >
    > the DNA of every fish

  - Some measurements may not be selected because they are uninformative:

    > e.g.
    >
    > gender for classifying fish species

  - Some measurements may not be selected because they are redundant:

    > e.g.
    >
    > size and weight might be highly correlated
    >
    > > including both may provide no extra information, but complicate the classification process unnecessarily

- Definition

  ```mermaid
  graph LR
  	m(Measurements m) --> filter[xi = mi]
  	filter --> x(Features x)
  ```

  > Feature selection results in dimensionality **reduction**: length($m$) â‰¥ length($x$). 

  - Feature selection is the process of choosing the most discriminative subset of data on which to perform the classification process.

  - Benifits

    - Reduces risk of over-fitting

      > e.g.
      >
      > if training set contained more male than female salmon, this might be exploited by classifier

    - Reduces training time (by reducing complexity of classifier).

- Feature selection may not be obvious.

  - Since we do not know if a measurement is useful.
  - If domain knowledge cannot help it may be necessary to search for the best features:
    - perform classification with different possible subsets of features to find those that produce the least error.

##### 7.1.3. Feature Extraction

- Definition

  > Feature extraction *usually* results in dimensionality **reduction**: length($m$) â‰¥ length($x$)

  - Find features that are functions of the raw data.

  - Projects original data points into new feature space. Ideally, new feature space is one in which it is easier to classify data.

    > e.g. Project 2D data to a 1D feature space.

---

### 7.2. Feature Extraction Methods

##### 7.2.1. Principal Component Analysis (PCA)

- Definition

  <img src="https://numxl.com/wp-content/uploads/principal-component-analysis-pca-featured.png" alt="Principal Component Analysis (PCA) 101 - NumXL" style="zoom: 50%;" />

  - A method for dimensional reduction. 
  - Takes $N$-dimensional data and finds the $M (Mâ‰¤N)$ orthogonal directions in which the data has the most variance.
    - These $M$ principal directions form a subspace.
    - We can represent an N-dimensional datapoint by its projection onto the $M$ principal directions.
    - Typically $M<N$, so the subspace has smaller dimensionality than the original space.

- Process

  - We choose the first $M$ principal components, those with the largest variance (loosing all information about where the datapoint is located in the remaining orthogonal directions).

  - Karhunen-LoeÌ€ve Transform (KLT)

    - One method of applying PCA.

    - Calculate mean of all data vectors
      $$
      \mu = \frac{1}{N} \sum^N_{i=1} x_i
      $$

    - Calculate covariance matrix of zero-mean data
      $$
      C = \frac{1}{N} \sum^N_{i=1} (x_i - \mu) (x_i - \mu)^T
      $$

    - Find eigenvalues $E$ & eigenvectors $V$ of $C$.
      $$
      C = VEV^T
      $$

    - Order eigenvalues from large to small, and discard small eigenvalues and their respective vectors.

    - Form matrix $\hat{V}$ of remaining eigenvectors and apply to data (with mean subtracted)
      $$
      y_i = \hat{V}^T (x_i - \mu)
      $$

- Neural Networks for PCA

  - Using zero-mean data as $\textbf{x}$. A linear neuron (i.e. a linear threshold unit without the heaviside function)

  - Output, $y$, is projected data.

  - Hebbian Learning
    $$
    \Delta w = \alpha y \textbf{x}^t
    $$

    - for one neuron
      - $\textbf{w}$ is aligned with 1st principal component
      - $\textbf{w}$ grows without bounds

  - Oja's Rule
    $$
    \Delta w = \alpha y (\textbf{x}^t - y \textbf{w})
    $$

    - for one neuron
      - $\textbf{w}$ is aligned with 1st principal component.
      - Weight decay term causes $\textbf{w}$ to approach unit length. 

- Example

  > e.g.
  >
  > ...

- 



##### 7.2.2. 