## WEEK VIII - Probabilistic Algorithms

>[üè† MENU - 5CCS2FC2](year2/5ccs2fc2.md)
>
>[‚¨ÖÔ∏è WEEK VII - Linear Programming](year2/5ccs2fc2/w7.md)
>
>[‚û°Ô∏è WEEK IX - The Limits of Computation I](year2/5ccs2fc2/w9.md)
>
>Outlines
>
>- Probability
>- Random Variables
>- Probabilistic Turing Machine PTM
>- Monte Carlo Algorithms & Las Vegas Algorithms
>

#### 27. Probability & Random Variables

###### ***Theorem 27.1.*** Recap of Probability, Events, and Random Variables

- Sample Spaces

  A **sample space** is a set of all possible outcomes from some random process $S = \{\text{all possible outcomes}\} \implies \{s_1, s_2, ..., s_n\}$

- Probability Measures

  A **probability measure** on $S$ is a function that $\text{Prob: }S \rightarrow [0,1]$

- Events

  An **event** is any subset that

   $E \subseteq S \\ E \subseteq P(S)$

  We extend the probability measure to events, notated as 

  $\text{Prob: } P(S) \rightarrow [0,1] \\ \text{Prob}(E)=\Sigma_{s \in E}\text{Prob}(s)$

- Random Variables

  A **random variable** on $S$ is a measurement of a random outcome, noted as $s \in S$

  The **probability** that a random variable $X$ takes the value $k \in \mathbb{R}$ is given by 

  $\text{Prob}(x=k) = \text{Prob}(\{ s \in S : X(s)=k\})$

- Probability Mass Function (p.m.f.)

  The **probability mass function** of a random variable $X$ is a function which says <u>how likely a given value is to appear</u> as the measurement of a random event, notated as $p_X(k)=\text{Prob}(X=k)$

###### ***Theorem 27.2.*** Expectation of a Random Variable

The **expectation** of a random variable $X$ is the weighted average of the possible values of $X$, notated as $\text{E}[X]=\Sigma_kk \cdot p_x(ks)$

> e.g.
>
> | $k$              | 1    | 2    | 3    | 4    | 5    |
> | ---------------- | ---- | ---- | ---- | ---- | ---- |
> | $p_X(k)$         | 0.15 | 0.3  | 0.15 | 0.1  | 0.05 |
> | $k \cdot p_X(k)$ | 0.15 | 0.6  | 0.45 | 0.4  | 0.25 |
>
> $\text{E}(X)=0.15+0.6+0.45+0.4+0.25=1.85$

###### ***Theorem 27.3.*** Let $X$ and $Y$ be any two random variable and let $a, b \in \mathbb{R}$ be real numbers. Then: 

1. $\text{E}[aX+b] = a\text{E}[X]+b$
2. $\text{E}[X+Y] = \text{E}[X] + \text{E}[Y]$

###### ***Theorem 27.3.*** (Markov‚Äôs Inequality) Let $X$ be a random variable with expectation $\text{E} [X]$. Then:

$\text{Prob}(X \geq a \cdot \text{E}[X]) \leq \frac{1}{a}$ for any $a>0$. 

###### ***Theorem 27.4.*** Variance of a Random Variable

- The **variance** of a random variable $X$ is a measure of the expected deviation from the average $Œº = \text{E} [X]$

> e.g. 
>
> Attempt 1:
>
> $\text{Var}[X] = \text{E}[\text{differences between } X \text{and }Œº] \\ = \text{E}[X-Œº] \\ = \text{always Zero}$  ‚ùå
>
> Attempt 2:
>
> $\text{Var}[X] = \text{E}[\text{suqared differences between } X \text{and }Œº] \\ = \text{E}[(X-Œº)^2] \\ = \text{a better measure of deivation}$  ‚úÖ

- The **standard deviation** of a random variable $X$ is the <u>square-root of the variance</u> of $X$, notated as $\sigma_X = \sqrt{\text{Var}[X]}$

###### ***Theorem 27.5.*** Let $X$ be a random variable. Then, we have that $\text{Var}[X] = \text{E}[X^2] - \text{E}[X]^2$

---

#### 28. Probabilistic Turing Machine

***Theorem 28.1.*** Considering a Probabilistic Turing Machine (PTM) $M = <\Sigma, Q, q_{init}, q_{accept}, q_{reject}, \delta>$

- Terminology
  - `Alphabet`
  
    $\Sigma = \{a_1, ... , a_k\}$
  - `Finite set of control states`
  
    $Q = \{q_1, ... , q_n\}$
  - `Initial state`
  
    $q_{init} \in Q$
  - `Accept and Reject states`
  
    $q_{accept}, q_{reject} \in Q$
  
  - `Probabilistic transition function` ‚ùóÔ∏è
  
    $\delta(\text{current state}, \text{read sybmol}) \to \text{probability distribution on possible instructions}$ 
    
  - `Sample space`
  
    $S = P(Q \times \Sigma \cup\{‚äî\} \times \{\leftarrow, \rightarrow\})$
  
- Configurations and Computations

  - The **configurations** of a PTM $M$ are defined as for regular Turing Machines as a complete description of the machine at any given time

    > $(\text{current state, left substring, right substring})$

  - The **computations** of a PTM $M$ are all possible sequences of configurations obtained by running $M$ on a given input

    > A tree contains all the possible computation results of $M$

- Acceptance Criteria

  - However, **each (branching) transition** now has as associated **probability**, stating the likelihood that the machine will apply the associated instruction. ‚ùóÔ∏è

  - We can compute the probability that $M$ follows a particular branch by <u>multiplying the probabilities</u> along 

    - the branch, 
    - and then the path in the branching computation tree,
    - and finally all paths that lead to the accept state and adding them together.

    > $\text{Prob}(M(w)=1) = \text{Prob(branch 1 \textbf{OR} branch 2 \textbf{OR} ... )} \\ = \Sigma_{\text{all accepting branches}}\text{Prob(accepting branches)}$

  

###### ***Theorem 28.2.*** Measurements of PTMs

Any **numberial property** that we can associate with a computation can be considered a **random variable**

- The **Termination time** $T(n)$
- The amount of **Space / Tape** used $S(n)$
- Number of times the machine is in a **given state**, etc.

---

#### 29. Monte Carlo & Las Vegas Algorithms

***Theorem 29.1.*** Probabilistic Complexity Class

- Bounded-error Probabilistic Polynomial Time (BPP)

  A language $L$ is said to be solvable in **bounded-error probabilistic polynomial time** if there is some machine $M$ that is: 

  - Probabilistically ‚ÄúSound‚Äù $\text{ if } w \notin L \text{ then Prob} (M(w)=1) \leq \frac{1}{3}$
  - Probabilistically ‚ÄúComplete‚Äù $\text{ if } w \in L \text{ then Prob} (M(w)=1) \geq \frac{2}{3}$
  - Polynomial-time $T(n) \in O(n^k) \text{ for some } k > 0$

- Zero-error Probabilistic Polynomial Time (ZPP)

  A language $L$ is said to be solvable in **zero-error probabilistic polynomial time** if there is some machine $M$ that is:

  - Sound $\text{ if } w \notin L \text{ then Prob}(M(w)=1)=0$
  - Complete $\text{ if }w \in L \text{ then Prob}(M(w)=1)=1$
  - Expected Polynomial-time $\text{E}[T(n)] \in O(n^k) \text{ for some } k > 0$

###### ***Theorem 29.2.*** Monte Carlo Algorithms & Las Vegas Algorithms

- Monte Carlo Algorithms

  A randomized algorithm with **deterministic runtime** but <u>may return incorrect answers</u> due to their probabilistic nature.

  $\textbf{BPP} = \{ \text{problems solvable in polynomial time with a Monte Carlo Algorithm} \}$

- Las Vegas Algorithms

  A randomized algorithm that **always sound and complete**, but whose <u>runtime may vary du</u>e to their probabilistic nature.

  $\textbf{ZPP} = \{\text{problems solvable in polynomial time  with a Las Vegas Algorithm}\}$

###### ***Theorem 29.3.*** **ZPP** $\sube$ BPP

***Proof.***

>1. Suppose that $L \in \textbf{ZPP}$ then there is some PTM $M$ that is **sound and complete** with repect to $L$ with an expected polynomial runtime
>
>   $\text{E}[T(n)] < c \cdot n^k \text{ for some c,k>0}$
>
>2. Construct a new PTM $M'$ as follows, where $K = 3 \cdot \text{E}[T(n)]$
>
>3. Clearly $M'$ always terminates in $O(n^k)$ steps, by design.
>
>   3.1.  Suppose that $w \notin L$
>
>   3.2.  Since, $M$ is sound $M‚Ä≤$ will only incorrectly accept $w$ if it had to guess 
>
>   ‚Äã	$\text{Prob}(M'(w)=1) \leq \text{Prob}(T(n) > 3 \cdot \text{E}[T(n)]) \leq \frac{1}{3}$
>
>4. We claim that this algorithm is **probabilistically sound**
>
>   $w \in L \implies \text{Prob}(M'(w)=1) \leq \frac{1}{3}$
>
>   4.1.  Suppose that $w \in L$
>
>   4.2.  Since, $M$ is complete $M'$ will only incorrectly reject $w$ if it had to guess
>
>   ‚Äã	$\text{Prob}(M'(w) = 1) \geq 1 ‚àí \text{Prob}(T(n) > 3 \cdot E[T(n)]) \geq \frac{2}{3}$
>
>5. We claim that
>
>   $w \in L \implies \text{Prob}(M'(w)=1) \geq \frac{2}{3}$
>
>6. Hence our machine $M‚Ä≤$ is a **polynomial-time Monte Carlo algorithm**. Therefore $L \in \textbf{BPP}$. 
>

