## WEEK VIII - Support Vector Machines

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK VII - Feature Extraction](year3/7ccsmpnn/w7.md)
>
>[➡️ WEEK IX - Deep Generative Neural Networks](year3/7ccsmpnn/w9.md)
>
>Outlines:
>
>1. Introduction to Support Vector Machines
>1. 

### 8.1. Introduction to Support Vector Machines

##### 8.1.1. Motivation

- Definition
  - Support Vector Machines (SVMs) based on *Linear Discriminant Functions*
    - Include margins to optimise solution
    - Can allow errors to occur in a controlled way
  - Comparable to Neural Networks
    - Allow non-linear mappings in higher dimensional feature space through use of Kernel functions
    - Advantages over NNs as simpler to select models and less susceptible to over-fitting
- Two-Class Classification Problem:
  - Given labelled training samples: $S = \{(\textbf{x}_1, y_1), (\textbf{x}_2, y_2), ..., (\textbf{x}_N, y_N)\}$, where
    - $\textbf{x}_i = \begin{bmatrix}x_{i1} \\ x_{i2} \\ ... \\ x_{id}\end{bmatrix}$
    - $y \in \{-1, 1\}, i \in \{1, 2, ..., N\}$
    - $N$ denotes the number of training samples. 

  - Goal: Design a hyperplane $f(x) = 0$ which can classify correctly (all) the training samples.
  - Process
    - To achieve the optimal hyperplane, we expect:
      - No errors, i.e., no mis-classification
      - Distance or margin between nearest *support vectors* and separating plane is maximal.


##### 8.1.2. Linear SVMs

- Definition

  - Deals with linearly separable 2-class classification problem.

  - Hyperplane is defined as follows
    $$
    f(\textbf{x}) = \textbf{w}^T \textbf{x} + w_0 = 0
    $$
    where $\textbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_d \end{bmatrix}$

  - Find $\textbf{w}$ and $w_0$ such that

    - The margin is optimal.
    - The classifier can correctly classify the samples, i.e., 

      $$
      \begin{cases}
      \textbf{w}^T \textbf{x} + w_0 \geq 1, &\forall \textbf{x} \in \text{class +1}\\
      \textbf{w}^T \textbf{x} + w_0 \leq -1, &\forall \textbf{x} \in \text{class -1}\\
      \end{cases}
      $$

- Optimal Margin

  - Distance of a point from a hyperplane
    $$
    z = \frac{|f(\textbf{x})|}{\parallel \textbf{w} \parallel}
    $$

    > e.g.
    >
    > $f(\textbf{x}) = w_1x_1 + w_2x_2 + w_0 = 2x_1 + 4x_2 - 6 = 0$
    >
    > > Distance from a point $(x_1, x_2) = (1, 3): z = \frac{2 \times 1 + 4 \times 3 - 6}{\sqrt{2^2+4^2}} = 1.789$ 

  - $\parallel · \parallel$ is the L2 norm operator (also known as Euclidean norm).

  - Achieve a maximum margin (distance) means

    - Find the largest margin $z$ <u>between the hyperplane and support vectors</u>.

    - The margin is given by:
      $$
      \begin{array}{l}
      \min_{x_i; y_i=-1} \frac{|f(x_i)|}{\parallel \textbf{w} \parallel} + \min_{x_i; y_i=+1} \frac{|f(x_i)|}{\parallel \textbf{w} \parallel} \\
      = \frac{1}{\parallel \textbf{w} \parallel} \left( \min_{x_i; y_i=-1} |f(\textbf{x}_i)| + \min_{x_i; y_i=+1} |f(\textbf{x}_i)| \right) \\
      = \frac{2}{\parallel \textbf{w} \parallel}
      \end{array}
      $$


##### 8.1.3. Constrained Optimisation Problem

- Definition
  $$
  \min_\textbf{w} J(\textbf{w}) = \frac{1}{2} \parallel \textbf{w} \parallel^2
  $$
  subject to $y_i(\textbf{w}^T \textbf{x}_i + w_0) \geq 1, i \in \{1, 2, ..., N\}$

  - Minimising $\frac{1}{2} \parallel \textbf{w} \parallel^2$ is equivalent to maximise the margin. The constraint $y_i(\textbf{w}^T \textbf{x}_i + w_0) \geq 1$ is to make sure all samples $\textbf{x}_i$ are correctly classified.

  - Method of Larange Multipliers
    $$
    \mathcal{L}(\textbf{w}, w_0, \lambda) = \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1)
    $$
    where $\lambda = \begin{bmatrix} \lambda_1 &\lambda_2 &... &\lambda_N \end{bmatrix}$, $\lambda_i \geq0, \forall i \in \{1, 2, ..., N\}$

    - The above primal problem can be transformed to the following dual problem, which is defined as follows:
      $$
      \max_{\lambda \geq 0} \min_{\textbf{w}, w_0} \left( \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1) \right)
      $$

    - Thus,
      $$
      \begin{array}{rcl}
      \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda) = 0}{\partial \textbf{w}} = \textbf{0} &\implies &\textbf{w} = \sum_{i=1}^N \lambda_i y_i \textbf{x}_i\\
      \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda) = 0}{\partial w_0} = 0 &\implies &\sum^N_{i=1} \lambda_iy_i=0
      \end{array}
      $$

    - By putting them together, we have
      $$
      \mathcal{L}(\lambda) = \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1) = ... = \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j \textbf{x}^T_i \textbf{x}_j
      $$
      The dual problem is reduced to:
      $$
      \max_{\lambda \geq 0} \left( \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j \textbf{x}^T_i \textbf{x}_j \right)
      $$
      where $\sum^N_{i=1} \lambda_i y_i = 0, \lambda_i \geq, \forall i \in \{1, 2, ..., N\}$

    - Solution $λ_i$ to can be found by using *quadratic programming solver*.

      - It is a scalar function which does not depend explicitly on the dimension of the input space.
      - The solution of the Lagrange multipliers $\lambda_i$ may not be unique but the hyperplane characterised by $\textbf{w}$ and $w_0$ is unique.
      - For those $\textbf{x}_i$ with $\lambda \neq 0$, they are known as **support vectors**. As a result, $\textbf{w} = \sum^{N_s}_{i=1} \lambda_i y_i \textbf{x}_i, N_s \leq N$ denotes  the number of support vectors.
      - The support vectors lie on the two hyperplanes satisfying $\textbf{w}^T \textbf{x} + w_0 = ±1$. 
    
  - Rearranging the terms in $\mathcal{L}(\lambda)$.

    - To maximise $\mathcal{L}(\lambda)$ in $\lambda_i$,since $λ_i \geq 0$ and $y_i(\textbf{w}^T \textbf{x}_i + w_0)−1 \geq 0$, one possibility is to have $λ_i(y_i(\textbf{w}^T\textbf{x}_i +w_0)−1) = 0$ for all $y_i(\textbf{w}^T\textbf{x}_i + w_0)−1 \geq 0$, where $i \in \{0, 1, ..., N\}$.
    - This is supported by *Karush-Kuhn-Tucker (KKT)* conditions.
    - As $y_i(\textbf{w}^T \textbf{x}_i + w_0) −1 \geq 0$, it suggests that some $λ_i = 0$ for those $x_i$ not being a support vector.
      - Support vector $\textbf{x}_i: y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1 = 0 \implies \lambda_i \neq 0$
      - Non-support vector $\textbf{x}_iy_i (\textbf{w}^T\textbf{x}_i + w_0) - 1 > 0 \implies \lambda_i =0$

##### 8.1.4. Summary for Linear SVM Classifier

- The linear SVM classifier (linearly separable case) can be found by solving the solution $(\textbf{w}, w_0, λ_i)$ to the following conditions:
  $$
  ...
  $$
  

- 



---



