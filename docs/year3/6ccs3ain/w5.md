## WEEK V - PCA

>[ðŸ  MENU - 6CCS3AIN](year3/6ccs3ain.md)
>
>[â¬…ï¸ WEEK IV - Sequential Decision Making](year3/6ccs3ain/w4.md)
>
>[âž¡ï¸ WEEK VI - Consensus Mechanisms](year3/6ccs3ain/w6.md)
>
>Outlines:
>
>1. Introduction to PCA
>     - Motivation
>     - PCA/SVD
>     - Mechanism of PCA
>2. Basic Linear Algreba
>     - Matrix and Matrix Calculation
>     - Matrix Transformation
>3. PCA Algorithm
>     - Data
>     - Process
>     - Covariance Matrix

### 5.1. Introduction to Principal Component Analysis

> [Ref: PCA, t-SNE, UMAP - bilibili](https://www.bilibili.com/video/BV1oH4y1c7ZR/)

##### 5.1.1. Motivation

- Recognise Digits

  <img src="https://miro.medium.com/v2/resize:fit:800/0*9jCey4wywZ4Os7hF.png" alt="Tutorial 1: MNIST, the Hello World of Deep Learning | by David Yang |  Fenwicks | Medium" style="zoom:67%;" />

  - MNIST: hand-writing digits dataset
  - Problem
    - Each digit has $128 \cdot 128 = 16384$ features/dimensions
    - Is there a nice way to reduce the number of features/dimensions?
    - We try to find the two components (each is a combination of the features)

##### 5.1.2. PCA/SVD

<img src="https://numxl.com/wp-content/uploads/principal-component-analysis-pca-featured.png" alt="Principal Component Analysis (PCA) 101 - NumXL" style="zoom:50%;" />

- Demention Reduction methods
- Adventages
  - State-of-the-art for many applications (supervised and unsupervised) 
  - Incredibly efficient (often, almost linear time)
  - Strong theoretical background
  - Can also be used to store data in more efficient way (Image compression). 
  - Visual evaluation possible for a small number of components (say 2)
- Small disclaimer: PCA and SVD (Singular value decomposition) are slightly different, but very very similar, weâ€™ll look at PCA (which often uses SVD)

##### 5.1.3. Mechanism of PCA

- Example

  > e.g.
  >
  > Say you have a bunch of house listings and you would like to group them into *student housing*, *regular* and *luxury*
  >
  > Letâ€™s say we have the following features
  >
  > - Floor size in $m^2$
  > - Number of rooms
  > - Distance supermarket
  > - Distance Kingâ€™s
  > - Hipster vibe

- Reduce to two or three features

  > e.g.
  >
  > Size
  >
  > - Floor size in $m^2$
  > - Number of rooms
  >
  > Location
  >
  > - Distance supermarket
  > - Distance Kingâ€™s
  > - Hipster vibe

- Process

  > e.g.
  >
  > Take the floor size and the number of rooms, since they are often correlated. 
  >
  > - Take the line that minimises the Least Squares Distance, we get the projection of the points. (points to a line)
  > - Then maximise the variance of the data. 
  >   - Reason: Retain more information. 
  > - Intuitively, the more variance we capture, the better we can approximate the higher-dimensional space (here $d=2$). 
  
  > e.g.
  >
  > 3D -> 2D
  >
  > ![Principal Component Analysis????](https://media.licdn.com/dms/image/v2/C4D12AQEtW-q4NcDAWg/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1648787394930?e=2147483647&v=beta&t=rTr9aqhvmc-K-Ix1-d1PiOjvji6sOxAX92ehRKjOEsQ)
  >
  > Let's say our three dimensions $(x_1, x_2, x_3)$ are as on a 3D space. 
  >
  > - Distance supermarket
  > - Distance Kingâ€™s
  > - Hipster vibe
  >
  > Then after reducing it to 2D it looks like the r.h.s.
  >
  > We may also wish to reduce it to just a line p1Dq, but we can see that this would be very lossy. 
  
  > e.g.
  >
  > 5D -> 3D


---

### 5.2. Basic Lingear Algreba

##### 5.2.1. Matrix

- Matrix Mulitplication

  - Matrix

    - Assume  we have a $m \times n$ ($m \text{ by } n$) matrix.

  - Process

    <img src="https://i.ytimg.com/vi/u6MVJiOw2d4/maxresdefault.jpg" alt="Matrix Multiplication" style="zoom: 50%;" />

  - Example

    > e.g.
    > $$
    > \begin{bmatrix}
    > 1 \\
    > 1 \\
    > 1 \\
    > 1 \\
    > 1 \\
    > \end{bmatrix}
    > \cdot
    > \begin{bmatrix}
    > 1 & 2 & 3 & 4
    > \end{bmatrix}
    > =
    > \begin{bmatrix}
    > 1 & 2 & 3 & 4 \\
    > 1 & 2 & 3 & 4 \\
    > 1 & 2 & 3 & 4 \\
    > 1 & 2 & 3 & 4 \\
    > 1 & 2 & 3 & 4 \\
    > \end{bmatrix}
    > $$

- Transpose

  - The **transpose** of a matrix $A$ is an operation which flips the matrix along its diagonal (switches the row and column indices of the matrix)

    > e.g.
    > $$
    > A = 
    > \begin{bmatrix}
    > a &b &c &d \\
    > e &f &g &h
    > \end{bmatrix}
    > $$
    > then
    > $$
    > A^T = 
    > \begin{bmatrix}
    > a &e \\
    > b &f \\
    > c &g \\
    > d &h 
    > \end{bmatrix}
    > $$

  - Note that $(A^T)^T = A$


##### 5.2.2. Matrix Transformation

- 2-D Image Transformation

  - Assume a 2D image which is represented by the following vectors
    $$
    \begin{bmatrix}
    1 &0 \\
    0 &1 
    \end{bmatrix}
    $$
    
  - **Stretch** along the y-axis and **squish** along the x-axis with the matrix
    $$
    \begin{bmatrix}
    0.5 &0 \\
    0 &2
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    x \\
    y
    \end{bmatrix}
    =
    \begin{bmatrix}
    0.5x \\
    2y
    \end{bmatrix}
    $$

  - **Rotate** counterclockwise
    $$
    \begin{bmatrix}
    0 &-1 \\
    1 &0
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    x \\
    y
    \end{bmatrix}
    =
    \begin{bmatrix}
    -y \\
    x
    \end{bmatrix}
    $$

  - So-called **shear mapping**
    $$
    \begin{bmatrix}
    1 &m \\
    0 &1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
    x \\
    y
    \end{bmatrix}
    =
    \begin{bmatrix}
    x + my \\
    y
    \end{bmatrix}
    (m \approx 1)
    $$

- Eigenvector

  - When a vector <u>doesn't change its direction</u> after multiplying with a matrix, then it is an **eigenvector**.

  - A vector $v$ is an eigenvector of the matrix $M$ if
    $$
    M \cdot v = \lambda v
    $$
    where $\lambda$ is the corresponding eigenvalue.

  - Example

    > e.g.
    >
    > Assume $M = \begin{bmatrix}2 & 2 \\ 5 & -1\end{bmatrix}$. 
    >
    > We can verify that $v_1 = \begin{bmatrix}1 \\ 1\end{bmatrix}$ and $\lambda_1 = 4$: 
    > $$
    > M \cdot v_1 =
    > \begin{bmatrix}2 & 2 \\ 5 & -1\end{bmatrix}
    > \cdot
    > \begin{bmatrix}1 \\ 1\end{bmatrix}
    > =
    > \begin{bmatrix}2+2 \\ 5-1\end{bmatrix}
    > =
    > 4 \cdot \begin{bmatrix}1 \\ 1\end{bmatrix}
    > =
    > \lambda_1 v_1
    > $$
    > We can verify that $v_2 = \begin{bmatrix}-2 \\ 5\end{bmatrix}$ and $\lambda_2 = -4$
    > $$
    > M \cdot v_2 =
    > \begin{bmatrix}2 & 2 \\ 5 & -1\end{bmatrix}
    > \cdot
    > \begin{bmatrix}-2 \\ 5\end{bmatrix}
    > =
    > \begin{bmatrix}-8 \\ 20\end{bmatrix}
    > =
    > -4 \cdot \begin{bmatrix}-2 \\ 5\end{bmatrix}
    > =
    > \lambda_2 v_2
    > $$


---

### 5.3. PCA Algorithm

##### 5.3.1. Data

- Assume a matrix to store wtih data, where each data point is an $d$-dimensional row vector.
  $$
  X = 
  \begin{bmatrix}
  ... &x_1^T &... \\
  ... &x_2^T &... \\
  ~ &... &~ \\
  ... &x_n^T &... \\
  \end{bmatrix}
  $$
  
- The dimensions are $n \times d$. 

##### 5.3.2. Process

1. Compute the mean row vector $\overline{x} = \frac{1}{n}\sum\limits_{i=1}^n x_i$

2. Compute the mean row matrix: 
   $$
   \overline{X} =
   \begin{bmatrix}
   1 \\
   1 \\
   ... \\
   1
   \end{bmatrix}
   \cdot
   \overline{x}^T =
   \begin{bmatrix}
   ... &x^T &... \\
   ... &x^T &... \\
   ~ &... &~ \\
   ... &x^T &...
   \end{bmatrix}
   $$
   The dimensions are $n \times d$. 

3. Subtract mean (obtain mean centred data)
   $$
   B = X - \overline{X}
   $$
   The dimensions are $n \times d$. 

4. Compute the covariance matrix of rows of $B$: 
   $$
   C = \frac{1}{n} B^T B
   $$
   The dimensions are $(n \times d)^T \times (n \times d) = (d \times n) \times (n \times d) = d \times d$. 

5. Compute the $k$ largest eigenvectors $v_1, v_2, ..., v_k$ of $C$. Each eigenvector has dimensions $1 \times d$. 

6. Compute matrix $W$ of $k$-largest eigenvectors: 
   $$
   W = 
   \begin{bmatrix}
   ... &... &~ &...\\
   v_1 &v_2 &... &v_k \\
   ... &... &~ &...\\
   \end{bmatrix}
   $$
   Dimensions of $W$ are $(d \times k)$.

7. Multiply each datapoint $x_i$ for $i \in \{1,2,...,n\}$ with $W^T$
   $$
   y_i = W^T \cdot x_i
   $$
   Dimensions of $y_i$ are $(k \times d) \times (d \times 1) = k \times 1$.

##### 5.3.3. Covariance matrix

- The covariance matrix measures the correlation between pairs of features.

- Finding the largest eigenvectors allows us to explain most of the variance in data. 

- The more variance is explained by the eigenvectors, the more important they are. 

- We can measure the explained variance by considering the quantity:
  $$
  \frac{\sum\limits^r_{i=1}\lambda_i}{\sum\limits^d_{i=1}\lambda_i}
  $$
