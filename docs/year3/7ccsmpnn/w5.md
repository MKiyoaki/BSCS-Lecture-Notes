## WEEK V - Deep Discriminative Neural Networks

>[ðŸ  MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[â¬…ï¸ WEEK IV - Multilayer Perceptrons & Backpropagation](year3/7ccsmpnn/w4.md)
>
>[âž¡ï¸ WEEK VI - Deep Generative Neural Networks](year3/7ccsmpnn/w6.md)
>
>Outlines:
>
>1. Deep Neural Networks
>   - Introduction to DNNs
>   - Difficulties in Pratices
>   - Solutions
>2. CNNs
>   - Introduction to CNNs
>   - Variants of Cross-correlation (Conv Layer)
>   - Pooling Layer
>   - FC Layer
>3. Issues with Deep Networks
>   - Volume of Training Data
>   - Overfitting
>   - Fail to Generalize

### 5.1. Deep Neural Networks

##### 5.1.1. Introduction to Deep Neural Networks

- Definition

  - A *deep* network is a neural network with more than 3 layers. Typically far more than 3 layers.
  - A *shallow* network is a neural network with less or qual than 3 layers.
  - The term **deep learning** refers to the process of training a deep neural network. 
- Motivation
  - Three-layer nerual networks are able to approximate any continuous non-linear function arbitrarily well.
    - Universal function approximators.
    - Can solve any pattern recognition task, in theory
  - However, to solve complex tasks (i.e., perform complex mapping), we need larger networks with more parameters. 
  - The success of VGGNet shows that <u>a deep network ususally has a better performance than a wide network</u>. 
- Deep Networks
  - Deep network aims to provide a hierarchy of representations with increasing level of abstraction.
    - It is the natural of dealing with many tasks.
  - Deep network allows **recurrent structures**. i.e., Recurrent nerual networks (RNNs). 
    - Recurrent networks process temporal information, where the value of $\textbf{x}$ changes over time. 
    - Training a recurrent network is achieved by unfolding the network and using backpropagation on the unfold network. 
    - There is one layer in the unfolded network for each time-step, so recurrent networks for processing long sequences are equivalent to very deep networks. 

##### 5.1.2. Difficulties in Pratices

- Vanishing Gradient

  ```mermaid
  graph LR
  	input --> y1
  	y1((y1)) --w21--> y2((y2))
  	y2 --w32--> y3((y3))
  	y3 --w43--> y4((y4))
  	y4 --w54--> y5((y5))
  	y5 --> output
  ```

  - Definition
    - Consider a deep network with one neuron per layer. While applying backpropagation: each time the error is propagated further backwards it is multiplied by a factor of the form $\phi'(w_{ji}x_i)w_{ji}$.
      - When using standard approach of activation function, the derivative of activation function results $\phi'(w_{ji}x_i)$ is small for most values of $\textbf{wx}$.
      - When using standard approach of weight initialization, the term $w_{ji}$ may also be small.
    - Thus this factor may getting more and more **smaller** when backpropagation. 
      - Neurons in the earlier layers learn much more slowly than neurons in later layers.
      - <u>Early layers contribute nothing to solving the task</u> and hence a deeper network cannot improve with the performance. 

- Exploding Gradient Problem

  - Definition
    - Consider a deep network with one neuron per layer. Each time the error is propagated further backwards it is multiplied by a factor of the form $\phi'(w_{ji}x_i)w_{ji}$. 
      - If the weights $w_{ji}$ are initialized to, or learn, large values, then this factor could be far more than 1. 
    - If not careful, the gradient tends to get **larger** as we move backward through hidden layers. This means:
      - Neurons in the earlier layers make, large, often random changes in their weights.
      - Later layers can NOT learn due to constantly changing output of earlier layers and, hence making network deeper makes the performance worse. 

- Solutions

  - Since the issues of gradients, backpropagation is inherently unstable.
  - To train deep networks it is necessary to mitigate this issue, by using:
    - activation functions with non-vanishing derivatives
    - better ways to initialize weights
    - adaptive variations on standard backpropagation
    - batch normalization
    - skip connections

  - Besides, in order to adapt large amount of parameters in the network, we need:
    - very large labelled datasets
    - large computational resources


##### 5.1.3. Solutions

- Activation Functions with Non-vanishing Derivatives

  - Rectified Linear Unit (ReLU)
    $$
    \phi'(net_j) = 
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    0 &\text{if } net_j < 0
    \end{cases}
    $$

  - Leaky Rectified Linear Unit (LReLU)
    $$
    \phi'(net_j) =
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    a &\text{if } net_j < 0
    \end{cases}
    $$

  - Parametric Rectified Linear Unit (PReLU)
    $$
    \phi'(net_j) =
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    a_j &\text{if } net_j < 0
    \end{cases}
    $$

- Better Ways to Initialize Weights

  - For weights connecting $m$ inputs to $n$ outputs:

    | Methods       | Activation Function | Choose Weight from Uniform distribution with range of       | Choose Weight from Normal distribution with mean=0, deviation of |
    | :------------ | ------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
    | Xavier Glorot | sigmoid/tanh        | $\left( -\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right)$ | $\sqrt{\frac{2}{m+n}}$                                       |
    | Kaiming He    | ReLU/LReLU/PReLU    | $\left( \sqrt{-\frac{6}{m}}, \sqrt{\frac{6}{m}}\right)$     | $\sqrt{\frac{2}{m}}$                                         |

- Adaptive Versions of Backpropagation

  - Motivation
  
    - Backpropagation struggles to deal with gradients in the cost function $J(\textbf{w})$ that are too small or too large.
  
      > Recall that backpropagation performs gradient descent to find parameters that minimize a cost function. 
  
    - Variation in the magnitude of the gradient may occur between:
  
      - Different layers (due to vanishing and exploding gradient)
      - Different parts of the cost function for a single neuron
      - Different directions for a multi-dimensional function
  
  - Momentum
  
    ![img](https://miro.medium.com/v2/1*ZASXdJV80xN7ntdINFEgIQ.jpeg)
  
    - Adds moving average of previous gradient to current gradient.
    - Increases step size when weight changes are consistently in same direction. 
  - Adaptive learning rate
    - Vary the learning rate for individual parameters during training.
      - Increase the learning rate if the cost is decreasing.
      - Decrease the learning rate if the cost is increasing.
    - Backpropagation algorithms with adaptive learning:
      - AdaGrad
      - RMSprop
    - Backpropagation algorithms with adaptive learning and momentum:
      - ADAM
      - Nadam
  
- Batch Normalization

  - Motivation
    - Learning in one layer of a network will change the distribution of inputs received by the subsequent layer of the network.
      - Consequently, learning is slow as later layers are always having to compensate for changes made to earlier layers.
      - Different inputs to one neuron can have very different scales. Inputs with smaller scales will tend to have less influence than ones with larger scales, even if the smaller ones are more discriminatory.
  - Definition
    - Batch normalization attempts to solve both these issues by scaling the output of each individual neuron so that it has a mean close to 0, and a standard deviation close to 1.

      $$
      BN(\textbf{x}) = \beta+\gamma \frac{\textbf{x} - E(\textbf{x})}{\sqrt{\text{Var}(\textbf{x}) + \epsilon}}
      $$
      where
      - $\beta, \gamma$ are parameters learnt by backpropagation,
      - $\epsilon$ is a constant used to prevent division-by-zero errors,
      - $E(\textbf{x})$ is the mean of $\textbf{x}$,
      - $\text{Var}(\textbf{x})$ is the variance (the squared standard deviation) of $\textbf{x}$. The mean and variance can be calculated using the value of $\textbf{x}$ in the current batch or using all the training data presented so far.
    - Batch normalization can be applied before or after the activation.

  - Evaluation

    - It enables saturating activation functions to be used as it limits activations to the range where the gradients are non-zero.
    - Make weight initialization less critical.
    - Generally stabilises learning, since gradient are less likely to vanish or explode.

- Skip Connections

  <img src="https://tikz.net/janosh/skip-connection.png" alt="skip-connection" style="zoom:33%;" />

  - Definition
    - Connections that skip one or more layers of the network (Residual module).
    - Skip connections let gradients by-pass parts of the network where the gradient has vanished. 
    - Network effectively becomes shallower, but this may be temporary. 


---

### 5.2. Convolutional Neural Networks (CNNs)

##### 5.2.1. Introduction to CNNs

- Definition

  - CNN is the most popular type of deep neural network.

  - A CNN is:

    - Any neural network in which at least one layer has an transfer function implemented using convolution/cross-correlation.
    - As opposed to vector multiplication.

  - Motivated by desire to recognize patterns with tolerance to location.

    > e.g.
    >
    > Spatial location from image, audio, etc.

    - Tolerance to location could be achieved by learning to recognize the pattern at each location independently.
    - Computationally more effecient to *share the weights* between the sub-networks (i.e. to have multiple copies of the same sub-network processing different locations).

- Kernel

  - Weight sharing is achieved by using cross-correlation as the transfer function.

  - A neuron's weights are defined as an array, called a **mask**, **filter**, or **kernel**. 

    > e.g. 
    >
    > $\begin{bmatrix} -1 &1 \\0 &-2 \end{bmatrix}$
  
  - The values in the array, the weights, will be learnt using backpropagation.
  
  - The input is also an array. 
  
    > e.g.
    >
    > An image or the output of the preceding layer of the network.
  
- Convolutional Layer

  ![CNN iamge](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwHpdxADHgZ2NMU6j2BlD2DYdy3K5Y6ZTPgg&s)

  - Cross-correlation is implemented as follows. For each location in the input array in turn:

    1. Multiply each weight by corresponding value in the input,
    2. Sum these products and write answer in the corresponding location in the output array.
  
    > Note.
    >
    > Output is also an array of numbers, Sometimes called a *feature map.* 
    >
    > 
  
  - The output is strongest where the location in the input matches the weights in the kernel. 
  
    - The output is the same at the two locations in the input with the same values.
  
    - Provides tolerance to location means, the same pattern in different locations produces the same output.
  
  - Cross-correlation is equivalent to vector multiplication.
  
    > e.g.
    >
    > We can flatten a 4 * 4 array into a 1 * 16 array, whereas a 3 * 3 kernel can be flattened as 9 * 1. 
  
    - Calculating the output of transfer function in the normal way would give as:
      $$
      \sum_i w_{ji}x_i = \textbf{w}\textbf{x}
      $$
  
    - However, it is easier to use one kernel to produce all the outputs using cross-correlation. 
  
  - Convolutional Operation
  
    - The operation performed by the convolutional layers is cross-correlation (or linear filtering), instead of convolution. 
  
      > Cross-correlation:
      > $$
      > Y(i, j) = X \star H = \sum_{k, l} X(i+k, j+l) H(k, l)
      > $$
      > Convolution:
      > $$
      > Y(i,j) = X * H = \sum_{k, l} X(i+k, j+l) H(-k, -l)
      > $$

##### 5.2.2. Variants of Cross-correlation

- Receptive Field (RF)

  - The mask defines the weights of a neuron. A neuron only receives input from a small region of the input array, called **receptive field (RF)**. 

  - Different masks can have different RFs size.

    > e.g.
    >
    > $2 \times 2$ mask: $\begin{bmatrix} -1 & 1 \\ 0 & -2\end{bmatrix}$
    >
    > $3 \times 3$ mask: $\begin{bmatrix} -1 & 1 & 0 \\ 0 & -2 & -2 \\ -1 &1 &0.1\end{bmatrix}$

  - Cross-correlation generates copies of the same neuron at each location.

    - The output is the response of multiple, identical, neurons with the same weights but different RF locations.

- Variants

  - **Padding** $P$
    - Defines number of 0 valued pixels appended to each edge of the input. Increase the padding increases size of the output.
  - **Stride** $S$
    - Defines number of pixels moved between calculations of the output. Increasing stride reduces size of the input.
  - **Dilation** $D$
    
    - Defines the spacing between the pixels to which the mask is applied. Increasing dilation increases RF size. 
    
  - The output size for an input with the size $W \times W$ is as follows:
    $$
    W_{out} = \frac{W + 2P - D(K-1) -1}{S} + 1
    $$
  
  - Multiple masks can be used to detect multiple patterns. This will result in multiple output arrays.
    - Such a multi-channel output is often represented as a stack of squares or as a cuboid.
    - A neuron can have **multi-channel** input (e.g., from different colour channels of an image). In this case the mask has multiple channels. 
    - Typically, the input, output and masks are all multi-channel.
    - It may be necessary to use many masks to learn to recongize many different sub-patterns in the data. However, this will result in many channels in the input to the next layer.
  - $1\times 1$ convolution can be used to reduce the number of channels, and reduce the computational burden of the next layer.
    - Contains a single weight in each channel.
    - Always applied with $\text{stride}=1$, and no padding.
    - Provides a weighted sum of the input channels at each location.
    - Multiple $1\times1$ filters can be used to produce differently weighted sums of the input channels.
    - Can also be used to increase the number of channels if many $1\times 1$ filters used.
  - The output dimension of $D$ masks is $D$.

##### 5.2.3. Pooling Layer

- Pooling

  ![Pooling layers in Neural nets and their variants | AIGuys](https://miro.medium.com/v2/resize:fit:1200/0*lIcR0gOMK0xzTr6_.png)

  - CNNs contain pooling layers to increase tolerance to:
    - the location of patterns,
    - the configuration of sub-patterns.
  - Pooling layers replace values within a region of the input array with a single value calculated, typically, as the mean or maximum. i.e., *average pooling* or *max pooling*. 
  - Pooling typically has a stried >1 to decrease the spatial size of the layer.
  - Different sized pooling regions (RFs) can be used.
    - If same size as the input array, the output is a scalar value. This is called *global pooling*.
  - Pooling also called **sub-sampling** or **down-sampling**. 

- Application of Pooling

  - Typically, pooling is applied separately to each input array, so the number of output channels is equal to the number of input channels. 
  - However, it is possible to perform cross-channel pooling to reduce the resulting number of channels.


##### 5.2.4. Fully-Connected Layer

- Definition

  - Typically, particularly when applied to classification tasks, the final layers of a CNN are **fully connected (FC) layers**.
    - They work exactly the same as a traditional, feedforward, neural network. 

      > e.g. a MLP

  - Classification is then performed using the final layer of the fully connected layer.
  - To create the input to 1st fully connected layer, it is necessary to **flatten** the output of the last convolutional or pooling layer.
    - This is the same as vectorization.
    - The vector from each feature map are concatenated into one vector.

      > Note. 
      >
      > The size of the flattened output will depend on size of the last feature map which will depend on size of the original input. e.g., an image. 
      >
      > Because the input to a fully connected layer needs to be a fixed size, all inputs processed by the network need to be the same size.

      > e.g.
      >
      > For a given input with the size of $W \times W \times C$, the flattened size is $W \cdot W \cdot C$. For instance, for a input $30 \times 30 \times 3$, the flattened size is $2700$. 

- Combination to the Network

  - The final network may contain multiple convolutional, pooling and fully connected layers connected sequentially.
  - It is also possible for operations to be performed in parallel (as well as sequentially).
  - Training
    - Input presented and activation propagates through the network to the final layer.
    - Output at final layer determines predicted class of input.
    - Error backpropagated to adjust weights (in conv and fc layers), thresholds (in activation functions), parameters of normalization layers.

---

### 5.3. Issues with CNNs/Deep Networks

##### 5.3.1. Volume of Training Data

- Motivation

  - Deep NNs have lots of tunable parameters: 
    - Typically 4 million to > 140 million for classifying colour images.
    - There is a trend to increasing depth and complexity of deep NNs.
  - Consequentlyï¼š
    - Need more training data,
    - Training is computationally intensive,
    - There is a danger of over-fitting the training data. 

- Data Augmentation

  - Use data augmentation to generate more data.

    >  e.g.
    >
    > For images apply class preserving transformations, shifts, rotations, changes to scales, flips, shears, crops, etc.

- Transfer Learning

  - Train the network (with intially random weights) on related tasks with lots of available data.
  - Then train the network (with pre-trained weights) on the tasks with little data.

##### 5.3.2. Overfitting

- Regularization

  - Use to mitigate overfitting.

  - A modification to the learning algorithm intended to improve generalization. 

    > e.g. L1 and L2 norm. Add a minor value to the algorithm.

  - One of the most effective methods of regularization is *dropout*. 

- Dropout

  - During training: at each iteration, randomly select a fraction of neurons and set their activity to be zero.
  - During useage: all neurons behave normally. 
  - Dropout forces random sub-networks to correctly classify each sample, preventing recognition of individual sampels be reliant on individual neurons.

- Limitations

  - Even if all precautions are taken against over-fitting, the network may still learn to exploit uninteded characteristics of data.

    > e.g.
    >
    > Certain lighting conditions.
    >
    > With smudge on lens.
    >
    > In a certain context.
    >
    > With a camera with a dead pixel.

  - Then the neural network may learn to discriminate these classes using these irrelevant characteristics, and learn nothing about what the objects look like. 

##### 5.3.3. Fail to Generalize

- Motivation

  - Deep NNs can produce excellent results on testing dataset, but still fail to generalize.

    > e.g. rotation leads to very different categorisations. 

    - This is a particular problem as it means it is very easy for someone to manipulate an image (into an **adversary example**) so that it is wrongly classified.

      > e.g. by making imperceivable changes to the pixel values.

    - This is a particular problem as it means it is very easy for someone to manipulate the real world so that it is wrongly classified.