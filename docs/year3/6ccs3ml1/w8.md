## WEEK VIII - Reinforcement Learning 2

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK VII - Reinforcement Learning 1](year3/6ccs3ml1/w7.md)
>
>[âž¡ï¸ WEEK IX - Evolutionary Algorithms](year3/6ccs3ml1/w9.md)
>
>Outlines:
>
>1. 

### 8.1. Adaptive Dynamic Programming

##### 8.1.1. Introduction to ADP

- Motivation

  - Problems with direct utility estimation
    - Treats utilities of states as independent. But we know that they are connected.
    - Ignoring the connection means that learning may converge slowly.
    - Another approach to utility estimation: **adaptive dynamic programming (ADP)**.
      - Still doing passive reinforcement learning. But doing it smarter.

- Definition

  - We can improve on direct utility estimation by applying a version of the Bellman equation.

  - The utility of a state is the reward for being in that state plus the expected discounted reward of being in the next state, assuming that the agent chooses the optimal action:
    $$
    U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} \Pr(s'|s, a) U(s')
    $$

    - For $n$ states, there are $n$ Bellman equations (one per state).
    - We want to solve these simultaneous equations to find the utilities. But the equations are nonlinear because of the *max* operator.

  - In passive learning, we have $Ï€$ so we know what action we will carry out.

  - Because of this, the *max* operator is removed so the equations become linear:
    $$
    U^\pi(s) = R(s) + \gamma \sum_s' P(s'|s, \pi(s)) U^\pi (s')
    $$

    - Bellman states a constraint on utilities. In pratice, there are two approaches
      - Directly solve the Bellman equations
      - Apply value iteration

  - 



---

### 8.2. Temporal Difference Learning



---

### 8.3. Active Reinforcement Learning



---

### 8.4. Q-learning



---

### 8.5. SARSA



---

### 8.6. Function Approximation