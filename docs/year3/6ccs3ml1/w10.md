## WEEK X - Learning from Demonstration

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[⬅️ WEEK IX - Evolutionary Algorithms](year3/6ccs3ml1/w8.md)
>
>[➡️ REVISION](year3/6ccs3ml1/re.md)
>
>Outlines:
>
>1. Learning from Demonstration
>1. Confidence-Based Autonomy
>1. Ethics of ML

### 10.1. Learning from Demonstration

##### 10.1.1. Introduction to LfD

- Motivation

  - Problem: learn a mapping from (world) state to actions. This mapping is called a **policy**. Same as in reinforcement learning (RL).
  - In RL, learning is from experience. In Learning from Demonstration (LfD), learning is from examples. LfD is a *supervised* learning of policies, which aims to <u>learn the policy from the provided samples</u>.
  - An example in LfD is a sequence of state-action pairs.
    - Model-free learning.
  - The policy is derived only from those states encountered during learning.
  - So the policy is *partial*.
  
- Definition

  - $S$ is set of states.

  - $A$ is set of actions.

  - Transition function:
    $$
    T(s'|s, a): S \times A \times S \to [0, 1]
    $$

  - $Z$ is observed state, accessed through mapping:
    $$
    M: S \to Z
    $$

    - If state is fully observable, then $M = I$, where $I$ means identity. 

    - Policy selects actions:
      $$
      \pi: Z \to A
      $$

  - Rather like an MDP, but with the extra indirection of:
    $$
    π:Z \to A
    $$
    rather than:
    $$
    \pi : S \to A
    $$

  - **Partially observable.** 

- Varieties

  - In LfD, there is an explicit **teacher** who provides **demonstrations** that the learner trains on.

  - Demonstration $d_j \in D$, such that
    $$
    d_j = \{(z^i_j, a^i_j)\}
    $$
    where $z_j^i \in Z$ and $a_j^i \in A$ and:
    $$
    i = 0, ..., k_j
    $$

  - This is the sequence of state/action pairs discussed above.

  - The set of demonstrations, $D$, is what differentiates LfD from other learning methods.

  - Demonstration-based learning methods include:

    - Learning from Demonstration (LfD)
    - Learning by Demonstration (LbD)
    - Programming by Demonstration (PbD)
    - Learning from Observations
    - ...

- Process

  - Teacher demonstrates execution of some behaviour.
  - Learner receives these demonstrations and derives a policy able to reproduce the demonstrated behaviour.
    - Make design choices
      1. Demonstration approach
      2. Problem space representation
      3. Policy derivation
    - Execution
      1. *Gathering* examples
      2. *Deriving* policy

##### 10.1.2. Demonstration Approach

- Choice of Demonstrator
  - Human performs the task themselves (Robot observes)
  - Human teleoperates the robot to perform the task (Robot observes)
  
- Demonstration technique

  - Batch learning: Policy learnt once, after all data is gathered.
  - Interactive learning: Policy learnt incrementally, as data is gathered.

- Problem space representation

  - Continuity.
  - States: continuous or discrete.
  - Actions: continuous or discrete.
  - Often defined by domain, but some scope for choice.


##### 10.1.3. Policy Derivation

- Definition

  - Mapping function

    - $D$ is a dataset of state/action pairs.

    - Directly approximate mapping from observed state to actions.
      $$
      f(): Z \to A
      $$
      which can be seen as a classification problem. 

    - From state, measured by robot, pick action.

  - System model

    - $D$ is used to determine model of the world $T(s'|s, a)$ and a reward function $R(s)$.

    - These are then used to derive policy $π$.

      > Reinforcement learning, but a bit different from what we have seen already.

  - Plans

    - Apply AI planning methods.

    - $D$, and optionally model of users' intentions, used to learn rules.

    - Rules associate pre-conditions and post-conditions with each action:
      $$
      L(\{preC, postC\}, a)
      $$
      and optionally world model $T(s'|s, a)$.

    - Planner then creates policy $π$.

##### 10.1.4. Gathering Examples

- Demonstration

  - Identity/no embodiment mapping
    $$
    g_E (Z, a) \equiv I(Z, a)
    $$

  - Demonstration is performed on actual robot.

  - Teleoperation: human teacher operates the robot and robot's sensors record data
    $$
    g_R(Z, a) \equiv I(Z, a)
    $$

    - Joystick operation.
    - Kinesthetic teaching. 

  - Shadowing: robot operates itself but attempts to mimic human
    $$
    g_R(Z, a) \not\equiv I(Z,a)
    $$

    - Mimic motions, but record own motions.
    - Follow a teacher in a navigation task.

- Imitation

  - Non-identity embodiment mapping
    $$
    g_E(Z, a) \not\equiv I(Z, a)
    $$
  
  - Demonstration is performed by human and observed on actual robot.
  
  - Sensors on teacher
    $$
    g_R(Z, a) \equiv I(Z, a)
    $$
    so get direct data from demonstrator, but not the robot.
  
  - External observation
    $$
    g_R(Z, a) \not\equiv I(Z, a)
    $$
    so sensor data is external to demonstrator and robot.

##### 10.1.5. Methods for Policy Derivation

- Motivation

  - Picking the right action directly from the data.
    - Classification: for discrete actions
    - Regression: for continuous actions

- Classification

  - Simplest idea is to pick an individual low-level action.
    - Do this in many different ways:
      - Gaussian Mixture Models (GMMs)
      - k-Nearest Neighbours (kNN)
      - Decision trees
      - Bayesian networks
    - Low-level actions are then put together in a sequence.
      - Hidden Markov Model (HMM)
      - Dynamic Bayesian Network (DBN)
  - More complex idea starts with higher level actions.
    - High level actions are sequences.
    - Classify between sequences.

- Regression

  - Use any model that maps to continuous values.
  - Could just use kNN.
  - Locally Weighted Regression.
  - Neural Networks.

- Planning

  - Represent desired behaviour (i.e., policy choice) as a **plan**.
  - Sequence of actions that lead from initial state to goal state.
  - Plan has:
    - pre-conditions
    - post-conditions
    - world state $T(s'|s, a)$
    - action sequence
  - However, it does NOT handle non-determinism very well.

- System model

  - Derives policy
    $$
    \pi: Z \to A
    $$
    from model of the world $T(s'|s, a)$ and reward $R(s)$.

    - Reinforcement learning. Aim is to maximise cumulative reward over time.
    - Reward function could be engineered or learn.

  - Start with more policy than in active learning, less policy than in passive learning.

  - Start with the partial policy and exploration.

  - Rewards

    - Engineered reward
      - Corresponds to the usual case. Sparse rewards for particular states.
      - LfD has concentrated on helping to locate rewards, reducing blind exploration.
      - Demonstration can highlight rewards.
      - *Ask for Help* requests advice when all actions are approximately equally valued.
    - Learned reward.
      - Try to learn a reward function that generalises from individual rewards.
      - Function approximation in RL.

---

### 10.2. Confidence-Based Autonomy

##### 10.2.1. Introduction to Confidence-Based Autonomy

- Motivation
  - Interactive algorithm for LfD policy learning.
  - Initially, learner has no knowledge.
  - Learner acquires policy through observing demonstrations.
  - Learner practices the task.
  - If learner encounters unfamiliar state, then learner can ask for more demonstrations.
- Definition
  - Learner's actions: $a \in \mathcal{A}$
  - Underlying Markov Decision Process (MDP).
  - Demonstrations: $(s_i, a_i)$, where $a_i \in A$, selected by teacher.
  - Two components:
    - Confident execution: demonstrations triggered by learner's confidence.
    - Corrective demonstration: demonstrations triggered by teacher correcting errors.
  - Goal is for learner to generalise from demonstrations and learn a policy.

##### 10.2.2. Confident Execution & Corrective demonstration

- Confident Execution

  - Robot's behaviour policy is represented by a classifier
    $$
    C: s \to (a, c, db)
    $$
    where

    - $s$ is the state,
    - $a \in \mathcal{A}$ is the action,
    - $c$ is the action selection confidence,
    - $db$ indicates decision boundary. 

  - Policy is trained on state vectors $s_i$ (inputs) and actions $a_i$ (labels).

    - if $c > \tau$, learner executes $a_p$

    - otherwise, learner receives a demonstration (Demonstrated action: $a_d$)

  - State space divided into two regions:

    - High confidence: autonomous execution
    - Low confidence: demonstration

  - Two situations where confidence may be low:

    - Unfamiliar (outlying points)

    - Ambiguous (overlapping regions)

- Corrective demonstration

  - Teacher may decide to provide a corrective demonstration Corrective action: $a_c$

- Evaluation

  - Two evaluation criteria are used to decide between autonomous execution and demonstration:
    if $(c > \tau_\text{conf}) \wedge (d < \tau_\text{dist})$ then *autonomous* else *demonstration*

  - Nearest neighbour distance
    - Euclidean distance from current state to most similar training data point.
    - Threshold $\tau_\text{dist} = 3 \times$ average nearest-neighbour distance across data set of demonstrations.
  - Classification confidence
    - Threshold $\tau_\text{conf}$ more complicated to compute.
    - Different threshold computed for each decision boundary.
  - Initially $\tau_\text{dist} = 0, \tau_\text{conf} = \infin$

- Classification

  - Classified point = $(o, a, a_m, c)$ where

    - $o$ is the original observation,
    - $a$ is the demonstrated action, 
    - $a_m$ is the model-selected action, and
    - $c$ is model action confidence (returned by classifier)

  - Set of all points mistakenly classified by decision boundary $i$.
    $M = \{(o, a_i, a_m, c) | a_m \neq a_i \}$

  - Confidence threshold is average confidence of misclassified points:
    $$
    \tau_{\text{conf}_i} = \frac{\sum^{M_i}_{j=1} c}{|M|}
    $$

  - If $M_i = 0$, then there were no classification mistakes.

---

### 10.3. Ethics of Machine Learning

##### 10.3.1. Introduction to Ethics for ML

- Definition
  - Need to think about more than just the algorithm.
  - ML algorithms are used to make decisions.
  - First, do no harm.

##### 10.3.2. Weapons of Math Destruction

- Definition

  - Term coined by Cathy O'Neil.
  - WMD.
  - Concept that ML models have the capacity to create great damage to society.

- Features

  - Opacity
    - Back box models that cannot easily be interrogated. Especially by the subjects.
  - Scale
    - The effects of the model can extend beyond the decision that it helps to make.
  - Damage
    - The model causes damage.
    - The model causes harm to people, especially members of vulnerable groups.

  > e.g.
  >
  > Often start out as ideas with the best of intentions. 
  >
  > - Value-added teacher model.
  > - Medical school admission.
  > - Employee profiling.
  > - Predictive policing.
  > - Recidivism risk model.

- Solution

  - Need a good system
    - System = algorithm + data
  - So need to think about what the data brings.
  - Need to audit against the WMD characteristics.
