## WEEK VIII - Reinforcement Learning 2

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK VII - Reinforcement Learning 1](year3/6ccs3ml1/w7.md)
>
>[âž¡ï¸ WEEK IX - Evolutionary Algorithms](year3/6ccs3ml1/w9.md)
>
>Outlines:
>
>1. 

### 8.1. Adaptive Dynamic Programming

##### 8.1.1. Introduction to ADP

- Motivation

  - Problems with direct utility estimation
    - Treats utilities of states as independent. But we know that they are connected.
    - Ignoring the connection means that learning may converge slowly.
    - Another approach to utility estimation: **adaptive dynamic programming (ADP)**.
      - Still doing passive reinforcement learning. But doing it smarter.

- Definition

  - We can improve on direct utility estimation by applying a version of the Bellman equation.

  - The utility of a state is the reward for being in that state plus the expected discounted reward of being in the next state, assuming that the agent chooses the optimal action:
    $$
    U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} \Pr(s'|s, a) U(s')
    $$

    - For $n$ states, there are $n$ Bellman equations (one per state).
    - We want to solve these simultaneous equations to find the utilities. But the equations are nonlinear because of the *max* operator.

  - In passive learning, we have $Ï€$ so we know what action we will carry out.

  - Because of this, the *max* operator is removed so the equations become linear:
    $$
    U^\pi(s) = R(s) + \gamma \sum_s' P(s'|s, \pi(s)) U^\pi (s')
    $$

    - Bellman states a constraint on utilities. In pratice, there are two approaches
      - Directly solve the Bellman equations
      - Apply value iteration

- Directly Solving the Equations

  - This is just a set of ismultaneous equations that can be solved with a Linear Programming solver.
  - Updates all the utilities of all the states where we have experienced the transitions.
  - Note that updated values are *estimates*.
    - They are no better than the estimated values of utility and probability we had before.
    - We just get quicker convergence because the utilities are consistent.

- Value Iteration

  - We can also use value iteration to update the utilities we have for each state.

  - Update until convergence using:
    $$
    U_{i+1} (s) \leftarrow R(s) + \gamma \sum_{s'} P(s'|s, \pi(s)) U_i(s')
    $$

  - Again, the results are still estimates, and no better than the estimates we got from direct estimation or solving the Bellman equations.

- Evaluation

  - The quality of the utility estimates will depend on how well we have *explored the space*.
  - Roughly, this is how many times we have encountered each state.
  - Regarding to results, typically ADP is quicker than direct utility estimation.

- 



---

### 8.2. Temporal Difference Learning



---

### 8.3. Active Reinforcement Learning



---

### 8.4. Q-learning



---

### 8.5. SARSA



---

### 8.6. Function Approximation