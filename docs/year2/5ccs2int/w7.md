## [WEEK VII - Supervised Learning](https://keats.kcl.ac.uk/pluginfile.php/10920626/mod_resource/content/5/Week_7_Lecture_Notes_Supervised_Learning.pdf)

>[ðŸ  MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[â¬…ï¸ WEEK VI - Unsupervised Learning](year2/5ccs2int/w6.md)
>
>[âž¡ï¸ WEEK VIII - ](year2/5ccs2int/w8.md)
>
>Outlines:

### 15. Introduction to Supervised Learning

##### 15.1. Definition

- Supervised Learning
  - Capture patterns of a set of training data with labels, to then predict new examples. 
  - The challenge is to create an algorithm that learns how these features should be combined. 
- Regression Problems
  - In regression problems, weaim to predict new values based on previously seen data. Instead of predicting values, what we can do is predict the underlying function that generated the sample data we observe. 
- Decision Tree

---

### 16. Linear Regression

##### 16.1. Introduction and Motivation

- Motivation

  - Given a plenty of blue points as samples, how to estimate a line as the functions that satisfying the relationship within the samples? 

    > e.g.
    >
    > Defining a line is nothing more than defining two parameters. In the case of the formula below, defining $\beta_0, \beta_1$ such that $\hat{y}=\beta_0+\beta_1x$

- Modelling with data

  - We will need a formula that stands for the model of the function

    > e.g.
    >
    > $y=\frac{1}{2}x + \epsilon $â€‹
    >
    > where $\epsilon$ stands for a normal distribution. 

  - Normal Distribution

    > e.g.
    >
    > The mean is $\mu = 2.5$, and the standard deviation is $\sigma = 0.3$ (therefore, by definition, the variance is $\sigma^2 = 0.09$). 
    >
    > Note that we write normal distributions as $\mathcal{N}(Î¼,Ïƒ2)$, therefore $f(5) âˆ¼ \mathcal{N}(2.5,0.09)$ - here we are using $f(5)$ to denote the distribution of $y$ when $x = 5$.


##### 16.2. Example

- Goal

  - We want to determine the line that best approximates the points, find the best fitting line (finding the best $\beta_0, \beta_1$ from the formula $\hat{y}=\beta_0+\beta_1x$), and in particular determine the best value. 

- Method

  - To find the best line, i.e., $\hat{y}$, we want to **minimise the residuals**. In particular, we want to **minimise the sum of the squares of them**. 

    > e.g.
    >
    > - Assuming the data points
    >
    >   $\begin{matrix}x &y \\-1 &2 \\0 &0 \\2 &0\\5 &? \end{matrix}$â€‹
    >
    >   We want to find the best-fit function according to the previous three data points so to predict the position of the last point, i.e., find best $y$ when $x=5$.
    >
    > - Take values for $x=1$ for example, we have $2=\beta_0+\beta_1 \cdot (-1) + \epsilon_1$â€‹
    >
    >   $\epsilon_1^2=(2-\beta_0+\beta_1)^2$â€‹
    >
    >   $\epsilon_2^2=(0-\beta_0)^2$
    >
    >   $\epsilon_3^2=(2-\beta_0-2\beta_1)^2$â€‹â€‹
    >
    >   ($\epsilon$ could be treated as the distance or cost from the data points to the estimation funciton line)
    >
    > - Putting it all together, we get that the **sum of squared errors (SSE)** is
    >
    >   $\text{SSE} = \epsilon_1^2 + \epsilon_2^2 + \epsilon_3^2 = 4âˆ’4\beta_0 +4\beta_1 +3\beta_0^2 +5\beta_1^2 +2\beta_0\beta_1$

  - In order to minimise the error, we want to find the best values of $\beta$s so to find the minimum possible value of SSE. With using partial derivatives, take the sum of $\frac{d(\text{SSE})}{d(\beta_0\beta_1)}$â€‹â€‹

    > e.g.
    >
    > Assume $\beta_0$ is constant, we can get a function $h(\beta_1) = 5\beta_1^2 +(4+2\beta_0)\beta_1 +(4âˆ’4\beta_0 +2\beta_0^2)$ 
    >
    > Assume $\beta_1$ is constant, we can get a function $g(\beta_0) = 3\beta_0^2 +(âˆ’4+2\beta_1)\beta_0 +(4+4\beta_1 +4\beta_1^2)$â€‹
    >
    > Therefore we can get the minimum value that:
    >
    > $\begin{cases}\hat{\beta_0}=\frac{4-2\beta_1}{6} \\ \hat{\beta_1}=\frac{-4-2\beta_0}{10}\end{cases} \implies \begin{cases}\hat{\beta_0}=\frac{6}{7} \\ \hat{\beta_1}=-\frac{4}{7}\end{cases}$
    >
    > So that the best fit function will be $\hat{y}=-\frac{4}{7}x+\frac{6}{7}$

  ##### 16.3. Formal Definition

  - **Linear Regression**

    - Let $x_j, ... , x_n \in \mathbb{R}^D$ be a set of points in $D$ dimensions and $y_1, ... , y_n$ be, respectively, the values associated with them. 

    - In a linear regression, we are looking for values $\beta_0, ... , \beta_D$â€‹â€‹ such that

      $\hat{y_j} =h(x_1)=\beta_0+\beta_1x_{j,1}+\beta_2x_{j,2}+Â·Â·Â·+\beta_Dx_{j,D}$â€‹

      and that best approximate the set of points. 

    - In other words, we are looking to minimise **SSE** (or MSE) between $y_1, . . . , y_n$ and $\hat{y_1}, ... , \hat{y_n}$â€‹. This is also known as **multiple linear regression**.

    > Note that linear relates to the set of parameters $\beta_i$, and not to the variables $x_i$â€‹ themselves. 
    >
    > e.g., polynomial regression, which is given by a formula that looks something like $\hat{y_j} = h(x_j) = \beta_0 + \beta_1 x_j + \beta_2 x^2_j + ... + \beta_D x^D_j$, could be a special case of linear regression. 

  - **Simple Linear Regression**

    - Same as linear regression but with $D = 1$â€‹â€‹.

  ##### 16.4. Solution for Simple Linear Regression

  - Solution equation

    - For a simple linear regression $y_i=\beta_1x_i+\beta_0+\epsilon_i$, the least squares estimates for $\beta_1$ and $\beta_0$ are given by

      $\begin{cases}\hat{\beta_1}=\frac{n\sum_ix_iy_i-(\sum_ix_i)(\sum_iy_i)}{n\sum_ix_i^2-(\sum_ix_i)^2} \\ \hat{\beta_0}=\frac{(\sum_ix_i^2)(\sum_iy_i)-(\sum_ix_i)(\sum_ix_iy_i)}{n\sum_ix_i^2-(\sum_ix_i)^2}\end{cases}$

    >Proof. 

  ---

  ### 17. Decision Trees

  ##### 17.1. Introduction and Motivation

  - Definition
    - For binary decision trees (two children in non-leaf nodes), we adopt the convention that the left branch represent when the statement is *True*. If we had a non-binary branch, we can simply indicate each arrow with a different attribute value.
    - Could be help for making decision. Could also be used for **Decision Tree Learning**. 
      - Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. 
      - In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.

  ##### 17.2. Gini Index (Gini Impurity)

  > e.g.
  >
  > ```mermaid
  > graph TD;
  > 	A[HasJob <= 0.5]--True-->H[class=High]
  > 	A--False-->B[OwnHouse <= 0.5]
  > 	B--True-->M[class=Medium]
  > 	B--False-->L[class=Low]
  > ```
  >
  > A very simple example of a decision tree classifier. A data point (representing a person) has attributes HasJob and OwnsHome, which can be 0 (False) or 1 (True). The classification problem assigns it to one of the classes C = {High, Low, Medium} to indicate the credit risk associated to the person.

  - Introduction

    - Gini index measures how 'impure' the set of points in the given node in the three. The **impurity** is done with respect, in this case, to the different classes. 

      > e.g.
      >
      > assuming for the root node `HasJob <= 0.5`, there are 15 samples with 6 High, 5 Medium and 4 Low, the Gini index is $0.658 = \frac{6}{15} \cdot \frac{9}{15} + \frac{5}{15} \cdot \frac{10}{15} + \frac{4}{15} \cdot \frac{11}{15}$
      >
      > The first part, $\frac{6}{15} \cdot (1-\frac{6}{15})=\frac{6}{15} \cdot \frac{9}{15}$ stands for the class High. 

  - Formal Definition

    - Let $v = (v_1, ... , v_k) \in \mathbb{N}^k$ be a k-dimensional vector of non-negative integers. Let $n = \sum\limits_{i=1}^k v_i$ be the sum of all entries of $v$. Then, the Gini Index of $v$â€‹ is given by

      $\text{Gini}(v) = \sum\limits_{i=1}^k p_i (1-p_i) \text{ where } p_i = \frac{v_i}{n}$â€‹â€‹

##### 17.3. Construct a Decision Tree

- Gini-Split Index

  - The **Gini-Split Index** is done to decide <u>which attribute (feature) to split a decision tree by</u>. We could calculate the index and pick the lowest point as the proper node to be splited. 

  - Here we assume we have only categorical features *(i.e., not numbers)*. Let $S$ be a set of $n$ data points with features $A_1 , ... , A_k$. Let $A_i$â€‹ be an attribute *(i.e., a dimension of feature vectors, such as â€˜OwnsHomeâ€™)*, with a possible values, then

    $\text{Gini-Split}(A_i)=\sum\limits^a_{j=1}\frac{n_j}{n}\text{Gini}(v_j)$â€‹â€‹

    where $v_j$ is the vector that counts the number of data points in each class in the child node where $A_i = j$, and $n_j$ is the sum of the elements of $v_j$.

  - To decide which attribute to split with next in our decision tree, we simply take

    $\arg\min\limits_{A_1,..., A_k} \{\text{Gini-Split}(A_i)\}$â€‹

    > e.g.
    >
    > $\text{Gini-Split(â€˜HasJobâ€™)} = \underbrace{\frac{6}{15}}_{\text{HasJob = False}} \cdot \overbrace{0}^{\text{Gini}(6,0,0)} + \underbrace{\frac{9}{15}}_{\text{HasJob = True}} \cdot \overbrace{0.494}^{\text{Gini}(0,5,4)} \approx 0.296$â€‹â€‹

  - It makes more sense to allow different splits in different branches. 

    - That is because when we are given a new point, it is no secret to us what are the values of each feature, and thus we can simply traverse the tree accoring to the current best option. 	
    - Indeed, allowing different branches split differently is what, for example, `scikit-learn` package for Python does.


##### 17.4. Notes on Neural Networks (NNs)







