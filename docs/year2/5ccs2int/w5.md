## WEEK V - Introduction to Machine Learning

>[ðŸ  MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[â¬…ï¸ WEEK IV - Handling Uncertainty](year2/5ccs2int/w4.md)
>
>[âž¡ï¸ WEEK VI - Unsupervised Learning](year2/5ccs2int/w6.md)
>
>Outlines:
>
>11. Machine Learning
>       - Definition
>       - Classical Tasks
>       - Terminology
>       - Evaluation on Algorithm

### 11. Machine Learning

##### 11.1. Definition

- Several ways of defining Machine Learning

- Types

  - Supervised Learning
    - The program is trained <u>on a given set of examples (with labels)</u>. 
    - It learns how to reach an accurate conclusion when given new data.
  - Unsupervised Learning
    - The program is given <u>a bunch of unlabelled data</u> and must discover patterns and relationships in them.
  - Reinforcement Learning
    - The program learns from the consequences of its actions (reward or punishment), 
    - rather than from being explicitly taught and selects its actions on basis of its past experiences (exploitation) and also by new choices (exploration).


##### 11.2. Classical Tasks

- Classification (Supervised) - Organises data in classes and determines the class of new data-points.
- Regression (Supervised - Fit functions to data and determine values of new data-points. 
- Clustering (Unsupervised) - Separate data in groups and determines the group of new data-points.
- Dimensionality Reduction (Unsupervised) - Transforms high-dimensional data into lower-dimensional data while preserving desired properties.

##### 11.3. Terminology

- **Training Set**

  - Is the portion of the data used for the learning. 
  - In other words, we give our program both data and labels for a portion of the data.
- **Test Set**

  - Is the untouched portion of data used to evaluate our model after trained. 
  - Of course, we 'hide' the test labels from the algorithm in that phase.

##### 11.4. Evaluation on Algorithm - in the context of *supervised learning*

- Metrics for Classification Problems

  - Accuracy
    - **Precision** - proportion of true positives over true positives and false positives. 
    - **Sensitivity** (or recall) - proportion of true positives over true positives and false negatives.
    - **F1 Score** - harmonic mean between sensitivity and precision. 
      
      > Note.
      >
      > True or False depends on whether predictions and actual outputs are equivalent, while Positive or Negative depends on the original predictions values. 
      
      $$
      \begin{matrix}
      \text{Precision} &= &\frac{TP}{TP + FP} \\
      \text{Sensitivity} &= &\frac{TP}{TP + FN} \\
      
      F_1 &= &\frac{2}{\text{sensitivity}^{-1}+\text{precision}^{-1}}
      
      \end{matrix}
      $$
  - Assume our regression is predicting real numbers. Call predictions $\hat{y}_1,...\hat{y}_n$ and true values $y_1,...y_n$â€‹. Then, we define the following:
    - **MAE** Mean Absolute Error $\frac{1}{n}\sum\limits^n_{i=1}|\hat{y}_i-y_i|$
    - **MSE** Mean Squared Error $\frac{1}{n}\sum\limits^n_{i=1}(\hat{y}_i-y_i)^2$
    - **RMSE** Root-Mean-Squared Error $\sqrt{\frac{1}{n}\sum\limits^n_{i=1}(\hat{y}_i-y_i)^2}$â€‹
  
  
  ##### 11.5. Terminology for the Genrealisation
  
  - Error
    - When we develop a model, we want to minimise the error it will have on unseen data. This is, informally, the **generalisation error**.
    - We rarely have, however, a model for the unseen data, and we only have samples to deal with. We can, then, minimise the **empirical error**, which is the one we have control of.
  - Underfitting, Overfitting
    - Underfitting
      - is when the model fails to capture the complexity of the (training) data. (or fail to find a patterns in the data)
    - Overfitting
      - Is when the model fits too much to the training data and fails to generalise to unseed data. (the model is â€˜too complexâ€™)
  
  
  
  
