## WEEK IX - AI Ethics

>[ðŸ  MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[â¬…ï¸ WEEK VIII - Reinforcement Learning](year2/5ccs2int/w8.md)
>
>[âž¡ï¸ WEEK X - Revision](year2/5ccs2int/w10.md)
>
>Outlines:
>
>1. AI Ethics
>2. Application and Access of AI
>3. Harms and Ethical Considerations of AI
>4. Discrimination and Bias in AI
>5. Responsibility and Minimising harms of AI

### 9.1. Introduction to AI Ethics

##### 9.1.1. Background

##### 9.1.2. Key questions

- How is AI applied in pratice & Who has access & Who benefits. 
- What are the harms and ethical considerations of AI & Who is harmed. 
- How does bias manifest in AI & Why does it arise. 
- Who is responsible for the harms of AI.
- How do we minimise these harms.

---

### 9.2. Application and Access of AI

##### 9.2.1. How is AI applied

- For policing

  - Effects
    - Increased breadth of surveillance
      - More opportunities for police to surveil, stop-and-search specific communties they are biased against. 
    - Worse facial recognition performance on minorities due to lack of data
      - Algorithms more likely to identify a suspected criminal by mistake
      - Increased stop-and-search, wrongful arrest of specific communties
    - Lack of accountability of AI systems

- For military

  - Tasks
    - Search for specific (human) targets
      - Propagation of bias into war
      - More deaths
    - Automated defense
      - Dehumanization of war
      - Loss of moral engagement
      - Simplification of complex moral choices
      - Easier escalation
      - Arms races

- For political control

  - Tasks
    - State surveillance of opposition, ethnic & religious minorties
      - Strengthening **authoritarian** states
      - **Human rights** losses, *e.g. privacy, freedom of speech, religious freedom, ...*
  - Effects
    - Increased censorship and disinformation
      - Loss of **freedom of speech**
      - Less informed citizens
      - Political **polarisation and control**

- Revival of phrenology
  > Phrenology - disproved pseudoscience that assumes predications of gender, intelligence, criminality and internal states can be made from facial analysis. 

  - Current AI application
    - Predication of gender, sexuality, race, emotion, criminality. 
    - But these are social constructs/unobservable. (Should respect with the subjects' willing)
  - Allows oppression of already marginalized communities *(e.g., LGBT, Black)*
    - Reinforcing harmful stereotypes
    - Mis-identification, micro-aggressive ad-targeting

##### 9.2.2. Who has access to AI

- AI is expensive

  - Lots of computation power
  - Expensive hardware
  - Money to train and to house
  - Big Tech monopoly *(e.g. Google, Amazon, ...)*
- Result

  - Universities cannot compete, resources used for Big Tech's priority problems.
  - Poorer labs.counteries cannot afford. 

---

### 9.3. Harms and Ethical Considerations of AI

##### 9.3.1. AI Harms

- Lack of human control & accountability

  - Many AI algorithms are complex, non-intuitive. Therefore it is <u>hard to understand and control</u>. 
  - Authority of objectivity, automation bias. Therefore there is <u>too much delegation and no accountability</u>. 
  
- Lack of safety

  - AI often not robust, therefore it is <u>not safe when conditions change</u>. 
  - AI often embedded in physical systems, so there is a risk on <u>physical safety</u>. 

- Discrimination

  - Impact and harms larger for specific groups
  - Some possible reasons
    - Lack of consideration during design
    - Lack of data
    - Lack of testing
    - Application problems
    - ...

- Privacy invasion, surveillance

  - AI allows more pervasive privacy invasion
  - Constant gata gathering to get data for training/selling. 
  
- Environmental and societal impact

  - Modern AI is power hungry
    - 100s of GPUs
    - 1000s of hours for training
  - AI relies on datacentres
    - Construction
    - Mineral mining
    - Cooling and pollution

##### 9.3.2. Ethical Principles of AI

- Many institutions adpoting guidelines for ethical development & deployment of AI
  - Governmental *(e.g. UK EPSRC, NHS, EU, ...)*
  - Professional *(e.g. IEEE, ACM, ...)*
- **Principle**
  - Human oversight and agency
  - Safety and robustness
  - Privacy
  - Transparency
  - Fairness
  - Societal & environmental wellbeing
  - Accountability


---

### 9.4. Discrimination and Bias in AI

##### 9.4.1. Definition

- Types of Discrimination

  - **Direct discrimination** (or **disparate treatment**)
    - When somebody is disadvantaged <u>because of a personal attribute</u> *(e.g., age, gender, race, etc. )*. 
  - **Indirect discrimination** (or **disparate impact**)
    - When people with a certain personal attribute *(e.g., age, gender, race. )* are disadvantaged <u>even though the attribute is not explicitly considered in decisions</u>. 
- Bias

  - In the context of AI ethics, bias means
    - Imbalance of tendency in data (input)
    - Direct or indirect descrimination (output)

- Examples

##### 9.4.2. Causes of AI Discrimination

- Risks

  - Risk is not anticipated, tested, or alleviated
    > e.g.
    >
    > - World bias - world distribution problem
    > - *Representation bias - data collection problem*
    > - *Measurement bias - wrong categorization of people/wrong measurements*
    > - *Algorithm bias - wrong choice of algorithm*
    >- Evaluation bias - wrong choice of evaluation metric or test set
    > 
  - Risk is obvious, problematic task
    - Ethics board does not flag a problem
    - *Developer does not oppose to build/report/below the whistle*
    - Management makes decision to deploy
  
- Who is descovering these issues
  - Women, Black scholars, LGBTQ+, etc. (importance of lived experience & diversity)

---

### 9.5. Responsibility and Minimising harms of AI

##### 9.5.1. Responsibility for the harms of AI

- Related keyholders
  - Institution that designs the system
    - For allowing any of the harms to take place
  - Ethics board
    - For passing an unethical product/application
  - Developers
    - Many hamrs best anticipated by developers
    - Many hamrs can only be tested/minimized by developers
    - Responsible for algorithm transparency, inefficiency, power consumption, ...

##### 9.5.2. Minimising the harms of AI

- Awareness of ethical issues, staying informed, being critical
- Participatory design
- Team diversity
  - Marginalized communities have different lived experience
  - Important to anticipate issues
- Balanced datasets
- Testing with diverse groups
- Documenting datasets, models, algorithms, design choices
- Logging, explainable methods
- Ethics boards
- Whistleblowing

