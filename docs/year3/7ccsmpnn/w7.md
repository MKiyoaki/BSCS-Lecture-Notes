## WEEK VII - Feature Extraction

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK VI - Deep Generative Neural Networks](year3/7ccsmpnn/w6.md)
>
>[➡️ WEEK VIII - Support Vector Machines](year3/7ccsmpnn/w8.md)
>
>Outlines:
>
>1. Feature Engineering, Selection & Extraction
>2. Feature Extraction Methods
>     - PCA
>       - orthogonal directions in which the data have the most variance.
>     - Whitening
>       - orthogonal directions with equal variance.
>     - Linear Discriminant Analysis (LDA)
>       - directions in which the data from different classes is most distinct.
>     - Independent Component Analysis (ICA)
>       - statistically independent directions.
>     - Random Projections
>       - a high-dimensional space with random directions.
>     - Sparse Coding
>       - a few, non-random directions in a high-dimensional space.

### 7.1. Feature Engineering, Selection & Extraction

##### 7.1.1. Feature Engineering

- Definition

  - Modify some measurements to make them more useful for classification.

    > e.g.
    >
    > translate date to day-of-week 09/08/2019 to Friday
    >
    > > useful to make predictions about future sales based on past sales
    > >
    > > not useful to make predictions about chances of death based on date-of-birth

  - Feature engineering can result in dimensionality **increase**: length($x$) ≥ length($m$)
  
  > e.g.
  >
  > one-hot encoding, embeding, etc.

##### 7.1.2. Feature Selection

- Motivation

  - Some measurements may be more useful for classification than others.

  - Some measurements may not be selected because they are unobtainable or too costly to obtain:

    > e.g.
    >
    > the DNA of every fish

  - Some measurements may not be selected because they are uninformative:

    > e.g.
    >
    > gender for classifying fish species

  - Some measurements may not be selected because they are redundant:

    > e.g.
    >
    > size and weight might be highly correlated
    >
    > > including both may provide no extra information, but complicate the classification process unnecessarily

- Definition

  ```mermaid
  graph LR
  	m(Measurements m) --> filter[xi = mi]
  	filter --> x(Features x)
  ```

  > Feature selection results in dimensionality **reduction**: length($m$) ≥ length($x$). 

  - Feature selection is the process of choosing the most discriminative subset of data on which to perform the classification process.

  - Benifits

    - Reduces risk of overfitting

      > e.g.
      >
      > if training set contained more male than female salmon, this might be exploited by classifier

    - Reduces training time (by reducing complexity of classifier).

- Feature selection may not be obvious.

  - Since we do not know if a measurement is useful.
  - If domain knowledge cannot help it may be necessary to search for the best features:
    - perform classification with different possible subsets of features to find those that produce the least error.

##### 7.1.3. Feature Extraction

- Definition

  > Feature extraction *usually* results in dimensionality **reduction**: length($m$) ≥ length($x$)

  - Find features that are functions of the raw data.

  - Projects original data points into new feature space. Ideally, new feature space is one in which it is easier to classify data.

    > e.g. Project 2D data to a 1D feature space.

---

### 7.2. Feature Extraction Methods

##### 7.2.1. Principal Component Analysis (PCA)

- Definition

  <img src="https://numxl.com/wp-content/uploads/principal-component-analysis-pca-featured.png" alt="Principal Component Analysis (PCA) 101 - NumXL" style="zoom: 50%;" />

  - <u>Projects data onto orthogonal directions in which the data have the most variance</u>
  - Takes $N$-dimensional data and finds the $M (M≤N)$ orthogonal directions in which the data has the most variance.
    - These $M$ principal directions form a subspace.
    - We can represent an N-dimensional datapoint by its projection onto the $M$ principal directions.
    - Typically $M<N$, so the subspace has smaller dimensionality than the original space.

- Process

  - We choose the first $M$ principal components, those with the largest variance (loosing all information about where the datapoint is located in the remaining orthogonal directions).

  - Karhunen-Loève Transform (KLT)

    - One method of applying PCA.

    - Calculate mean of all data vectors
      $$
      \mu = \frac{1}{N} \sum^N_{i=1} x_i
      $$

    - Calculate covariance matrix of zero-mean data
      $$
      C = \frac{1}{N} \sum^N_{i=1} (x_i - \mu) (x_i - \mu)^T
      $$

    - Find eigenvalues $E$ & eigenvectors $V$ of $C$.
      $$
      C = VEV^T
      $$

    - Order eigenvalues from large to small, and discard small eigenvalues and their respective vectors.

    - Form matrix $\hat{V}$ of remaining eigenvectors and apply to data (with mean subtracted)
      $$
      y_i = \hat{V}^T (x_i - \mu)
      $$

##### 7.2.2. Neural Networks for PCA

- Process

  - Using zero-mean data as $\textbf{x}$. A linear neuron (i.e. a linear threshold unit without the heaviside function)

  - Output, $y$, is projected data.

  - Hebbian Learning
    $$
    \Delta w = \alpha y \textbf{x}^t
    $$

    - for one neuron
      - $\textbf{w}$ is aligned with 1st principal component
      - $\textbf{w}$ grows without bounds

  - Oja's Rule
    $$
    \Delta w = \alpha y (\textbf{x}^t - y \textbf{w})
    $$

    - for one neuron
      - $\textbf{w}$ is aligned with 1st principal component.
      - Weight decay term causes $\textbf{w}$ to approach unit length. 
    
  - Sanger's Rule
    $$
    \Delta w_{ji} = \alpha y_j (x_i - \sum^j_{k=1} y_k w_{ki})
    $$

    - For each neuron: subtract from $\textbf{x}$ the contribution of the neurons representing the 1st $j-1$ principal components, then apply oja's rule.
    - $w_j$ will learn the $j$-th principal component. 

  - Oja's Subspace Rule
    $$
    \Delta \textbf{w} = \alpha y(x - \textbf{w}^t y)^t
    $$

    - It will learn the same subspace spanned by the 1st $n$ principal components (i.e. PCs are in random order).
    - Note that Oja's subspace rule is identical to the rule used to update a Negative Feedback Network (see earlier lecture).

  - An autoencoder (with linear units) can also learn PCA, using: Negative feedback learning (oja's subspace rule), Backpropagation of error

- Example

  > e.g.
  >
  > ...

##### 7.2.3. Whitening Transform

- Definition
  - While PCA gives us a subspace where the data dimensions are uncorrelated (i.e. orthogonal): the covariance matrix in the new space is diagonal. **Whitening** makes the covariance matrix in the new space equal to the identify matrix: each dimension then has the same variance.
  - <u>Projects data onto orthogonal directions with equal variance.</u> 

[![whitening-whiteningTransform](https://theclevermachine.wordpress.com/wp-content/uploads/2013/03/whitening-whiteningtransform1.png?w=179&h=562)](https://theclevermachine.wordpress.com/wp-content/uploads/2013/03/whitening-whiteningtransform1.png)

##### 7.2.4. Linear Discriminant Analysis (LDA)

- Definition
  - Because PCA is unsupervised, it may eliminate discriminative dimensions.
  - **Linear Discriminant Analysis** seeks a projection that is maximally discriminative. This requires feature extraction to be informed by class labels, i.e. to be *supervised*.
  - <u>Projects data onto directions in which the data from different classes is most distinct.</u>
- Process
  - Between class scatter (*sb*): needs to be maximized
    - Fisher's method.

##### 7.2.5. Independent Component Analysis (ICA)

- Definition

  - PCA finds components that are **uncorrelated**.
    $$
    \langle y_i \ y_j\rangle = \langle y_i \rangle \langle y_j \rangle
    $$

  - ICA finds components that are statistically **independent**.
    $$
    \langle g_1(y_1) \ g_2(y_2)\rangle = \langle g_1(y_1) \rangle \langle g_2(y_2) \rangle
    $$

  - <u>Projects data such that new directions are independent.</u> 

  - In probabilistic terms:
    $$
    P(y_i, y_j) = P(y_i)P(y_j) \\
    P(y_i | y_j) = P(y_j)
    $$

  - For Gaussian distributions

    - uncorrelatedness is the same as independence, so PCA produces independent components.

  - For non-Gaussian distributions PCA and ICA are quite different. 

- Process

  - Oja's subspace rule

##### 7.2.6. Random Projections

- Definition

  - PCA, LDA, and ICA are typically used for dimensionality reduction. i.e., Project data to a lower dimensionality subspace.

  - Other methods nonlinearly project the data into a space with higher dimensionality:

    - In the new space feature vectors that were nonseparable *may* become separable.

  - One way to do this is by using a random projection:
    $$
    y = g(Vx)
    $$
    where $V$ is a matrix with random values.

  - <u>Projects data onto a high-dimensional space with random directions.</u> 

- Application

  - These methods usually combine:
    - feature extraction using random projections, with
    - a simple (linear) classifier
  - Both implemented using a neural network.

- Extreme Learning Machines

  - Identical to a RBF network (see earlier lecture), except Hidden Layer activation not defined in terms of a distance from a centre.
  - In augmented vector notation, output is $z = \textbf{w}\textbf{y}$. if desired output is $t$, then we want $\textbf{w}\textbf{y} \approx t$
    - For all training data: $z = \textbf{w}\textbf{Y} \approx \textbf{T}$
    - weights that minimize MSE: $\textbf{w} = \textbf{T}\textbf{Y}^T$, where
      - $\textbf{y}$ is hidden layer output for each training exemplar (each column is $\textbf{y}$ for one sample)
      - $\textbf{T}$ is required output for each training exemplar (each column is $t$ for one sample)
  - If multiple outputs (i.e. representing multiple classes), weights for each can be calculated in the same way.

##### 7.2.7. Sparse Coding

- Definition

  - Like random projections, data is projected into a high-dimensional space in the hope that this will make classes more separable.

  - Unlike random projections:

    - only a few of the dimensions are used to represent any one exemplar.
    - exemplars are projected onto a subspace (technically a manifold as mapping is nonlinear) of a high-dimensional space.
    - projection is not random.

  - We define the sparse coding as follows:
    $$
    y = g(Vx)
    $$
    where

    - $y$ contains only a few non-zero elements, 
    - $V$ is a matrix of weights

  - <u>Projects data onto a few directions in a high-dimensional space.</u>

- Process

  - Define weights $V$

    - Learn them (define a cost function to be optimised)
    - Use a random selection of exemplars from the training se
    - Use all exemplars from the training set

  - Choose non-zero elements of $y$

    - Neural Network Approach: 

      - Introduce competition so that activity of some neurons is suppressed by others.
      - Competition can be performed using a negative feedback network rather than via lateral connections.
      - Competition selects neurons (elements of $y$) that are best able to represent the input ($x$).
      - One measure of ability to represent input is: $\parallel x - V^t y\parallel_2$

    - Instead of using a neural network we can minimise an objective function, such as:
      $$
      \min_y (\parallel x - V^t y \parallel_2 + \lambda \parallel y \parallel_0)
      $$

      - $λ$ (a positive parameter) defines trade-off between accuracy and sparsity.
      - Many different optimisation methods and slightly varying objective functions have been proposed.

    - We can think of the matrix $V^t$ as a dictionary:

      - The columns of $V^t$ are *basis vectors*, or *elementary components*, or *codebook entries*, or *atoms*, or *dictionary elements*.
      - A vector, $x$, is represented as a linear combination of columns sparsely selected from the dictionary.
        - For one sample: $x \approx V^t y$
        - For all samples: $X \approx V^t Y$
      - Maximally-sparse ($y$ has one non-zero element):
        - nearest neighbour classifier if $V$ equals training data.
        - vector quantization (VQ) if $V$ is learnt.
      - Non-sparse ($y$ has any number of non-zero elements):
        - principal components analysis (PCA) if columns of $V^t$ are orthonormal and the rows of $Y$ are mutually orthogonal.
        - independent components analysis (ICA) if rows of $Y$ are statistically independent.
        - Non-negative matrix factorisation (NMF) if $X, V, Y$ contain only elements with non-negative values.