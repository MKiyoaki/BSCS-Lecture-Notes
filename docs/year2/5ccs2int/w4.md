## WEEK IV - Handling Uncertainty

>[🏠 MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[⬅️ WEEK III - Automated Planning](year2/5ccs2int/w3.md)
>
>[➡️ WEEK V - Introduction to Machine Learning](year2/5ccs2int/w5.md)
>
>Outlines:
>
>1. Uncertainty in AI
>    - MDPs
>      - Markov Chains
>      - Markov Decision Process (MDP)
>    - Expected Value
>      - Expected Value
>      - Expected Utility
>    - Expected utility of actions in MDPs
>2. Solving MDPs with Value Iteration
>      - Solving MDP

### 4.1. Uncertainty in AI

##### 4.1.1. MDPs

- Terminology

  - Certainty in a problem
  - Non-deterministic system
  - Partially-observable system
  - **Markov Chains**
    - Modelling probabilistic transitions between states, i.e., transitions are given probabilities of possible success. 
    - The next state is <u>**ONLY** determined by probabilistic transition between states</u>.
    - Markov Property - does not factor <u>the history of previous states</u>.
  - **Markov Decision Process (MDP)**
    - A sequential decision making problem for a fully observable but stochastic environment.
    - Markov Chain holds, but now with probability of *actions* between states and *rewards* for action execution.
    - Reliant on two principles, 
      1. Assume that Markov property holds for action transitions (which it should).
      2. Probability distributions are stationary and don’t change.

- In order to determine value of an action, we will need a **utility function**, that will help calculate this based on state **rewards**.

- MDP components for all $s \in S$

  - Transition Model $T(s, a, s')$
    - Similar to existing state transition, but now encodes $P(s'| s, a)$
  - Initial state of the problem $s_0$
  - Reward Function for a given state $R(s)$

- MDP solution - a **policy** $\pi$

  - For any state $s \in S$, $\pi(s)$ denotes what action should be taken in that state
  - *i.e., You can different resulting solutions from the initial to goal state, given the stochastic nature of the environment.*
  - Hence, the optimal policy $\pi^*$ will yield the highest expected utility in a given situation. 


##### 4.1.2. Expected Value

> Check 5CCS2FC2 for details

##### 4.1.3. Expected utility of actions in MDPs

- Expected utility - $E[U|s,a] = \sum\limits_{s' \in \text{nei}(s)} P(s’|s,a)U(s’)$

  - Refers to the <u>**expected value** of the utility you get while the agent is at state $s$ and taking the action $a$</u>. 
  - This is an expectation because it is a probabilisitc state machine, all we know is with what probability you might transition to each state.
- Compute the equation
  - Using the probilitistic tree
  - Each neighbour $(s')$ is associated with
    - $P(s’|s,a)$ - the probability of transitioning to state $s'$ with giving that we are at state $s$ and take action $a$
    - $U(s')$ - the utility of being at state $s'$

---

### 4.2. Solving MDPs with Value Iteration

##### 4.2.1. Solving MDP

- Goal

  - Find the optimal policy $\pi^*$ for the problem
    - Need to be **complete**, *i.e., cover all the states*
    - Need to be **optimal**, *i.e., gives best action for state $s$*
    - Need to be **stationary**, *i.e., actions depedent only on current state*
    - Need to be **proper**, *i.e., guaranteed to reach a terminal state*
  - The policy should yield an expected utility for a given state following a policy $U^\pi=E[\sum\limits^\infty_{t=0}\gamma^tR(St)]$​

    > Note that $\gamma$ is a discount factor applied to rewards, valuing current over <u>future rewards</u>. Like, how will the future decision effects with the current expected value. 

- Value Iteration Algorithm

  1. Start with the utility map $U_0$, by setting $U(s)=0$ in all states, except the *end state* where $U(s)=R(s)$. 
  2. Do one iteration to get $U_1$, 
     - Go through each state, update its expected utility taking into accounted the neighbours' utility and assuming we always take the action that leads to highest expect utility. 
     - By using the **Bellman update equation** 
       - $U_{i+1}(s) = R(s)+\gamma \max\limits_{a \in A(s)}\sum\limits_{s'}P(s'|s, a)U_i(s')$
       - i.e.,  $ U_{i+1}(s)= R(s)+\gamma \max\limits_{a \in A(s)}E[U|s, a]$
     - This process will give us a new estimate of $U(s)$ for all states, to which we call $U_1(s)$
  3. Do one more iteration to compute values of $U_2$, with using $U_1$ and the Bellman equation.
  4. **Keep repeating this until** we see that the updates are not leading to any changes, i.e. until $U_{i+1}(s) = U_i(s) \text{ for all }s$

  >Pseudo code:
  >
  >...

- Obtaining the policy

  - When the value iteration algorithm converges, i.e., $U_{i+1}(s)=U_i(s) \text{ for all }s$,  
    - It means we have the expected utility of all states $s$ that $U^\pi=E[\sum\limits^\infty_{t=0}\gamma^tR(St)]$
    - The expected sum of discounted rewards from $s$​ all the way until the goal, using an optimal policy
  - To obtain the policy from the utility values we can again look at the Bellman equation:
    - $U(s) = R(s)+\gamma \max\limits_{a \in A(s)}\sum\limits_{s'}P(s'|s, a)U(s')$​
    - $\pi(s)= \arg \max\limits_{a \in A(s)} \sum\limits_{s'}P(s'|s, a)U(s')$
