## WEEK IX - Ensemble Methods

>[ðŸ  MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[â¬…ï¸ WEEK VIII - Support Vector Machines](year3/7ccsmpnn/w8.md)
>
>[âž¡ï¸ WEEK X - Clustering](year3/7ccsmpnn/w10.md)
>
>Outlines:
>
>1. 
>

### 9.1. Introduction to Ensemble Methods

##### 9.1.1. Ensemble Methods

- Motivation

  - Idea

    - Generate a collection of *weak* classifiers from a given dataset.
    - Each *weak* classifier is a simple classifier, e.g., a half-space classifier, (can be slightly better choosing randomly/blindly), given by a different dataset (generated by resampling).
    - Combine classifiers to be a final one, e.g., by averaging or voting. (Multi-classifier system)

  - Reduces variance: Higher precision of the match; results are less dependent on peculiarities of a single training set

  - Reduces bias: more accurate or better quality of the match

  - Improves the estimate if the learning algorithm is unstable

    - A learner is said to be unstable if a small change to training set causes large change in the output classifier. 

    > e.g.
    >
    > Decision trees and neural networks are unstable learners; k- nearest neighbor and naive Bayesian classifiers are stable learners

- Definition

  - Methods that combine multiple diverse classifiers to obtain better classification accuracy.
  - Two methods: bagging and boosting
    - Bagging (bootstrap aggregating)
      - Uses multiple versions of a training set.
      - Each created by drawing $n' < n$ samples from $\mathcal{D}$ with replacement.
      - Each bootstrap data set is used to train a different weak classifier.
      - Final classification decision is based on the vote of each weak classifier.
    - Boosting
      - First create a classifier with accuracy on the training set greater than average.
      - Then add new component classifiers to form an ensemble whose joint decision rule has arbitrarily high accuracy on the training set.

---

### 9.2. Bagging & Boosting

##### 9.2.1. Procedure

- Bagging

  1. Start with dataset $\mathcal{D}$.
  2. Generate $M$ dataset $\mathcal{D}_1, ..., \mathcal{D}_M$.
     - Each distribution is created by drawing $n' < n$ samples from $\mathcal{D}$ with *replacement*.
     - Some samples can appear more than once while others do not appear at all.
  3. Learn weak classifier for each dataset.
     - Weak classifiers $f_i(\textbf{x})$ for dataset $\mathcal{D}_i, i \in \{1, 2, ..., M\}$.
  4. Combine all weak classifiers using a majority voting or averaging scheme.
     - Averaging: $f_\text{final} ( \textbf{x}) = \text{sgn} \left( \sum^M_{i=1} \frac{1}{M} f_i(\textbf{x}) \right)$

- Boosting

  - Goal: improve the accuracy of any given learning algorithm.
  - The classification performance is *boosted* by combining a number of weak classifiers (having accuracy only slightly better than chance is sufficient).
  - Add weak classifier one by one to form the overall classifier such that higher classification accuracy can be achieved.
  - Each weak classifier is trained with *informative* dataset complementary to others.

  > e.g.
  >
  > A 2 dimensional 2-category classification task with 3 weak classifiers
  >
  > ...
  >
  > The final decision of classification is based on the votes of the weak classifiers.
  >
  > > by the first two weak classifiers if they agree, and by the third weak classifier if the first two disagree.

##### 9.2.2. Adaboost

- Definition

  - **AdaBoost** is a short form of *Adaptive Boosting*.
  - It allows adding weak classifiers to the final classifier until a desirable classification accuracy is achieved.
  - Each training pattern is assigned a weight. 
    - A lower weight is assigned to patterns being identified correctly; 
    - A higher weight is assigned to patterns not being identified correctly. 
    - So AdaBoost classifier takes more care about the *difficult* patterns.
  - It constructs a strong classifier by combining weak classifiers.

  ![Adaptive Boosting, Simply Explained through Python | by Vagif Aliyev |  Medium](https://miro.medium.com/v2/resize:fit:850/1*DwvwMlOcT1T9hZwIJvMfng.png)
  
- Procedure

  1. Given dataset and labels.
  2. Initialise weighted error weights and maximum number of weak classifiers.
  3. Select the weak classifier with minimum error. If error rate > 0.5, abandon this weak classifier and output strong classifier as weighted sum of selected weak classifiers.
  4. Otherwise, update weighted error weights and calculate the weight of the selected weak classifier.
  5. Repeat 2. 3. 4. If maximum number of weak classifiers is not reached. Then, output strong classifier as weighted sum of selected weak classifiers

- Evaluation

  - Pros
    - Simple for implementation
    - Good classification accuracy and generalisation
    - Can be applied to different classifiers
  - Cons
    - Solution is suboptimal
    - Sensitive to noisy data and outliers

- Structure

  ```mermaid
  graph LR
  	x --> 1[h_1]
  	x --> 2[h_2]
  	x --> k[...]
  	x --> max[h_kmax]
  	1 -- Î±1 --> Î£
  	2 -- Î±2 --> Î£
  	k --...--> Î£
  	max -- Î±kmax --> Î£
  	Î£ --> sign
  	sign --> H
  ```

  - Final classifier: 
    $$
    H(\textbf{x}) = \text{sgn} \left( \sum_{k=1}^{k_\text{max}} \alpha_k \hat{h}_k (\textbf{x})\right)
    $$

##### 9.2.3. Stacked Generalization

- 



##### 9.2.4. 