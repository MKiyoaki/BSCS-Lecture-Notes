## WEEK VIII - Reinforcement Learning 2

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[⬅️ WEEK VII - Reinforcement Learning 1](year3/6ccs3ml1/w7.md)
>
>[➡️ WEEK IX - Evolutionary Algorithms](year3/6ccs3ml1/w9.md)
>
>Outlines:
>
>1. Adaptive Dynamic Programming
>     - Introduction to ADP
>     - Evaluation
>2. Temporal Difference Learning
>     - Motivation
>     - TD Learning
>       - Model-free learning
>3. Active Reinforcement Learning
>4. Q-Learning
>    - Model-free Active Learning
>    - Update Rule
>      - Off-policy algorithm
>5. SARSA
>      - Introduction to SARSA
>      - Update Rule
>        - On-policy algorihtm
>6. Function Approximation
>     - Function Approximation
>     - DRL
>     

### 8.1. Adaptive Dynamic Programming

##### 8.1.1. Introduction to ADP

- Motivation

  - Problems with direct utility estimation
    - Treats utilities of states as independent. But we know that they are connected.
    - Ignoring the connection means that learning may converge slowly.
    - Another approach to utility estimation: **adaptive dynamic programming (ADP)**.
      - Still doing passive reinforcement learning. But doing it smarter.

- Definition

  - We can improve on direct utility estimation by applying a version of the Bellman equation.

  - The utility of a state is the reward for being in that state plus the expected discounted reward of being in the next state, assuming that the agent chooses the optimal action:
    $$
    U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} \Pr(s'|s, a) U(s')
    $$

    - For $n$ states, there are $n$ Bellman equations (one per state).
    - We want to solve these simultaneous equations to find the utilities. But the equations are nonlinear because of the *max* operator.

  - In passive learning, we have $π$ so we know what action we will carry out.

  - Because of this, the *max* operator is removed so the equations become linear:
    $$
    U^\pi(s) = R(s) + \gamma \sum_{s'} P(s'|s, \pi(s)) U^\pi (s')
    $$

    - Bellman states a constraint on utilities. In pratice, there are two approaches
      - Directly solve the Bellman equations
      - Apply value iteration

- Directly Solving the Equations

  - This is just a set of ismultaneous equations that can be solved with a Linear Programming solver.
  - Updates all the utilities of all the states where we have experienced the transitions.
  - Note that updated values are *estimates*.
    - They are no better than the estimated values of utility and probability we had before.
    - We just get quicker convergence because the utilities are consistent.

- Value Iteration

  - We can also use value iteration to update the utilities we have for each state.

  - Update until convergence using:
    $$
    U_{i+1} (s) \leftarrow R(s) + \gamma \sum_{s'} P(s'|s, \pi(s)) U_i(s')
    $$

  - Again, the results are still estimates, and no better than the estimates we got from direct estimation or solving the Bellman equations.


##### 8.1.2. Evaluation

- The quality of the utility estimates will depend on how well we have *explored the space*.
- Roughly, this is how many times we have encountered each state.
- Regarding to results, typically ADP is quicker than direct utility estimation.
- Still passive learning. A possible solution is to define a MDP like before.

---

### 8.2. Temporal Difference Learning

##### 8.2.1. Motivation

- Motivation
  - To get the utilities, the agent started with a fixed policy, so it always knew what action to take.
  - Having gotten the utilities, it could use them to choose actions.
    - Pick the action with the best expected utility in a given state.
  - However, there is a problem with doing this.
    - The transition model is a maximum likelihood estimate (i.e., just the sample average).
    - Recall from previous lectures that these models tend to overfit.
    - Maximum likelihood action selection can be dangerous.
    -  There is no way to be sure that the action the reinforcement learner is picking does not have possible bad outcomes.
  - Solutions
    - As discussed in earlier lectures we can use better priors. Consider transitions to all reasonable states.
    - Can also learn the probability that a particular model (a set of probability and utility values) is true and make decisions based on:
      - either the expected value of actions across all models; or
      - the worst case outcome across the models.
    - Can try to ensure that the learner explores widely.

##### 8.2.2. Introduction to Temporal Difference Learning

- Definition

  - In the previous approach we used the fact that we are learning in the context of an MDP.

  - Temporal difference learning uses Bellman (constraints between states). Use the observed transitions to adjust the utilities of the states.

  - The temporal difference update for a transition from $s$ to $s_1$ is:
    $$
    U^\pi(s) \leftarrow U^\pi(s) + \alpha(R(s) + \gamma U^\pi(s') - U^\pi(s))
    $$
    where $α$ is a learning rate. It controls how quickly we update the utility when we have new information.

  - The rule is called **temporal difference (TD)** because the update occurs between successive states.

- ADP vs. TD

  - ADP can be read as a statement about the stopping condition.
    - No change in values when both sides of the equation are equal.
    - Connects the utility of s with that of *all* its successor states.
  - The TD update only adjusts the utility of $s$ with that of a single successor $s_1$.
    - In the long run, the transition from $s$ to $s_1$ will happen exactly in proportion to $P(s'|s, \pi(s))$.
    -  Thus, $U^\pi(s')$ will be averaged into $U^\pi(s)$  exactly the right amount, but we need to adjust $α$ over time.

- Features

  - Note that TD learning is **model free**. i.e., there is no transition model.
  - That makes it easier to apply (no need to count transition probabilities).
  - Learning reduces to applying the TD rule on transition from one state to another.

---

### 8.3. Active Reinforcement Learning

##### 8.3.1. Introduction

- Definition

  - The passive reinforcement learning agent is told what to do. (Fixed policy)
  - Active reinforcement learning agents must *decide* what to do. (While learning)
  - We will think about how to do this by adapting the passive ADP learner.

- Process

  - We can use exactly the same approach to estimating the transition function.

  - Sample average of the transitions we observe.

  - But computing utilities is more complex.

  - When we had a policy, we could use the simple version of the Bellman equation:
    $$
    U^\pi(s) = R(s) + \gamma \sum_{s'} P(s'|s, \pi(s)) U^\pi(s')
    $$
    When we have to choose actions, we need the utility values to base our choice of action on.

##### 8.3.2. Value Iteration for Active Learning

- Motivation

  - We know what to do to get utility values: we use value iteration.
  - At any stage, we can run:

    $$
    U_{i+1}(s) \leftarrow R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s'|s, a)U_i(s')
    $$

    to stability to compute a new set of utilities.

  - So establishing utilities is not so hard after all.
  - After running value iteration we would choose the action with the highest expected utility.
    - Greedy agent.
    - Could do that while we are learning.
    - Typically a greedy agent will not learn the optimal policy.

- Exploration

  - The issue is that once the agent finds a run that leads to a good reward, it tends to stick to it, i.e. it stops exploring.
  - To do better, we can do $ε$-greedy exploration/exploitation.
    - Advantage: Simple, and we know it works.
    - Disadvantage: As we saw earlier it can be slow.
  - A better approach is to change the estimated utility assigned to states in value iteration.
    - Manipulate the values to force the learner to explore.
    - Then, once exploration is sufficient, we just let it choose the best possible action.
    - To do this, we can use:
      $$
      U_{i+1}(s) \leftarrow R(s) + \gamma \max_{a \in A(s)} f (\sum_{s'} P(s'|s, a) U_i(s'), N(s,a))
      $$
      where:
      - $N(s, a)$ counts how many times we have done $a$ in $s$,
      - $f(u, n)$ is the exploration function.

      > e.g.
      > $$
      > f(u, n) =
      > \begin{cases}
      > R^+ &\text{if } n < N_e \\
      > u &\text{otherwise}
      > \end{cases}
      > $$
      > $R^+$ is an optimistic reward, and $N_e$ is the number of times we want the agent to be forced to pick an action in every state.
      >
      > We force the learner to pick each state/action pair $N_e$ times.
      >
      > $N_e$ becomes another parameter that has to be adjusted until we find good solutions.

  - Solution

    - A list of states $s_1,...,s_n$.

    - Each state has a utility estimate associated with it $U(s)$.

    - Each state has a set of actions associated with it, $a_1,..., a_m$.

    - Each state/action pair has a probability distribution
      $$
      P(S'|s, a_i)
      $$
      over the states $S'$ that it gets to from $s$ by doing $a_i$.


---

### 8.4. Q-learning

##### 8.4.1. Model-free active learning

- Motivation

  - The form of active reinforcement learning we have just looked at learns a transition model.
  - Regarding to a model free version, we can quite easily define an active version of temporal difference learning.

- Definition

  - **Q-learning** is a **model-free** approach to active reinforcement learning. It does not need to learn $P(s'|s, a)$. 

  - Revolves around the notion of a Q-value, just like bandit learning.

  - The difference is that for bandits we learnt the Q-value of an action.

  - Here we learn the Q-value of a **state/action pair** $Q(s, a)$. It denotes the value of doing $a$ in state $s$, so that:
    $$
    U(s) = \max_a Q(s,a)
    $$

    - Easier to learn than $U(s)$.

    - We can write that:
      $$
      Q(s, a) = R(s) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')
      $$

      - Note that the sum is over $s'$.
      - Can compute estimates of $Q(s,a)$ by running value-iteration style updates on this. But it would not be model-free.

##### 8.4.2. Update Rule

- Definition

  - We can write the update rule as:
    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( R(s) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    $$
    and recalculate every time that $a$ is executed in $s$ and takes the agent to $s_1$.

  - Running updates

    - Run an update having moved from $s$ to $s_1$:

      ```mermaid
      graph LR
      	s
      	s1
      	s --a--> s1
      ```

    - Update $Q(s, a)$, where the $a'$ are all the actions we know about in $s'$.

- Action Selection

  - Since Q-learning is an active approach to reinforcement learning, we have to choose which $a'$ to select in $s'$.
  - Again greedy selection is usually a poor choice. We could use ε-greedy, or could force exploration as we did before.

- Algorithm

  ```
  function Q-Learning-Agent(percept) returns an action
  	inputs: percept, a percept indicating the current state s' and the reward signal r'
  	persistent: Q, a table of action values indexed by state and action, initially zero
  							Nsa, a table of frequencies for state-action pairs, initially zero
  							s, a, r, the previous state, action and reward, initally null
  	if Terminal?(s) then Q[s, None] <- r'
  	if s is not null then
  		increment Nsa[s, a]
  		Q[s, a] <- Q[s, a] + alpha * (Nsa[s, a])(r + gamma max(Q[s', a']) - Q[s, a])
  	s, a, r <- s', argmax(a', f(Q[s', a'], Nsa[s', a'])), r'
   return a
   
  ```

  - Note that $α$ is a function of the number of visits to $s, a$.

  - Ensures convergence.

- Solutions
  - A list of state action pairs $\langle s_i, a_j \rangle$.
  - Each state/action pair has $Q(s_i, a_j)$.
  - For a given $s_i$, just pick the aj to maximise $Q(s_i,a_j)$.

---

### 8.5. SARSA

##### 8.5.1. Introduction to SARSA

- Definition

  - **State-Action-Reward-State-Action (SARSA)** is a variant of Q-learning.

  - Update rule is:
    $$
    Q(s,a) \leftarrow Q(s,a) + \alpha \left( R(s) + \gamma Q(s', a') - Q(s,a)\right)
    $$
    where $a'$ is the actual action taken in $s'$.

  - Rule is applied when the action $a'$ <u>has been taken</u>.

    - That is after every $s, a, r, s', a'$ cycle. (Hence the name)

- SARSA vs. Q-Learning

  - Q-Learning

    - Q-learning computes $Q(s, a)$ using the best $Q(s', a')$ that is accessible from $s$.
    - Q-learning is **off-policy**. It does not care about the policy being followed. Will update with Qps1, a1q even if a1 is not the action taken.

  - SARSA

    - SARSA uses the actual $Q(s', a')$ once $a'$ has been taken.
      - When all the Q values are learnt, there is no difference.
      - Greedy agent using the result of Q-learning will do the same as a greedy agent using the results of SARSA. Different when exploring.

    - SARSA is **on-policy**. It will only update with $Q(s', a')$ if $a'$ is taken.

  - Both slower to converge than the previous approach, active learning with ADP. 

  - No model means no ability to enforce the Bellman constraint.

##### 8.5.2. Reinforcement learning update rules

- General form of update rule:

  - New_estimate <- Old estimate + step size [Target - Old estimates]

    >  [Target - Old Estimates] is an error in the estimate which is reduced by taking a step towards *Target*.

  - TD: $U^\pi(s) \leftarrow U^\pi(s) + \alpha (R(s) + \gamma U^\pi(s') - U^\pi(s))$

  - Q-Learning: $Q(s, a) \leftarrow Q(s,a) + \alpha \left( R(s) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$

  - SARSA: $Q(s, a) \leftarrow Q(s, a) + \alpha (R(s) + \gamma Q(s', a') - Q(s, a))$

---

### 8.6. Function Approximation

##### 8.6.1. Introduction to Function Approximation

- Motivation
  - Up to now, we have explicitly recorded utility functions and Q-values.
  - However, when looking up tables, time to convergence slows rapidly as the number of states grows. For ADP time per iteration also increases.
  - Hard limit on the kinds of problem that can be solved.
  - One solution is **function approximation**.

- Definition
  - Function approximation means we use any sort of representation other than a lookup table.
  - We have some function on state that provides a value.
  - This is an approximation because we do not know what the real utility is.

- Process
  - Given a state $s$, one reasonable function is a weighted linear function of a set of features:
    $$
    \hat{U}_\theta(s) = \theta_1f_1(s) + \theta_2f_2(s) + ... + \theta_n f_n (s)
    $$
    Features are also called **basis functions**.

  - We then try to learn the $\theta_i$.

  - We can represent utilities efficiently. 

  > e.g.
  >
  > Assume utility is related to $x, y$:
  > $$
  > \hat{U}_\theta(x,y) = \theta_0 + \theta_1 x + \theta_2 y
  > $$
  >
  > If $(\theta_0, \theta_1, \theta_2) = (0.5, 0.2, 0.1)$, then $\hat{U}_\theta (1, 1) = 0.8$
  >
  > > - To learn this we might run some passive learning trials and collect direct utility estimates of some states.
  > > - Then learning $(\theta_0, \theta_1, \theta_2)$ is a supervised learning problem. It is just linear regression (so we know how to solve it).
  > > - What we just described was an offline method. 
  > >   - Offline method: do some learning and then do some acting. 
  > >   - Can do it online as well: learn while acting.
  > > - As we learn new estimates of a state, we adjust the weights to reduce the error for that state.
  
- Evaluation

  - It can also *generalise*. That is, we can get utilities for states we have not visited.

    > e.g. TD Gammon, 1992.

    - Trade-off between a function which is likely to span the true utility function and how long it takes to learn.
    
  - The real power is the generalisation. 
  
    - While we get a new utility. When we adjust to reduce the error in this utiltiy, we change the utility estimate for every other state as well. 
    - If we have a good function, that will reduce the error for all states with each update.
  
  - Of course we are not limited to linear approximations. We can use any of the learning methods we have studied that can output a value.
  
  - Including non-linear approximators like neural networks: **Deep reinforcement learning**.

