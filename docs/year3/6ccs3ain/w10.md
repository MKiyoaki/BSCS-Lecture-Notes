## WEEK X - Ethics and AI

>[🏠 MENU - 6CCS3AIN](year3/6ccs3ain.md)
>
>[⬅️ WEEK IX - Clustering](year3/6ccs3ain/w9.md)
>
>[➡️ REVISION](year3/6ccs3ain/re.md)
>
>Outlines:
>
>1. Importance of Ethics 
>1. Regulatory and Legal Constraints
>1. Ethics Dimensions

### 10.1. Importance of Ethics 

##### 10.1.1. Reasons to Consider Ethics

- Most technologies have good and evil applications, thus engineers owe a duty to our society to consider the ethics of our work. 

- With AI, there are particular aspects we need to consider:

  - Algorithms may be learnt, so that even the software developers do not know what they do or how.
  - Many machine learning methods are "black boxes".
  - Data may be biased.
  - There may be significant legal consequences to our design decisions.

- Trolley Problems

  - Thought experiments in which we are faced with a moral dilemma, which often involve the control of a trolley. 

    > e.g.
    >
    > MIT Moral Machine Experiment

##### 10.1.2. Ethical Trade-offs

- Norms vs Rules

  - AI engineers could hard-code many rules. But sometimes we need the car to use the other lane, even though it is against the law.
  - Our usual solution is not to hard-code the rules as unbreakable constraints, but to code them as *norms*. 
    - **Norm** is an accepted standard or way of behaving, which most people follow. 
  - Norms and their exceptions are studied extensively in AI.

- Decisions often involve ethical trade-offs

  >  e.g. Lives lost as a result of one action-option vs. Lives lost as a result of another action-option

##### 10.1.3. Pressures from Regulators

- Our Society is very concerned with various aspects of new technologies, such as AI.

- AI is an important current focus of regulators, 

  > e.g. European Commission, UK Government Office for AI, National regulators, etc. 

- Regulators focusing on the following aspects,

  - Fairness (and elimination of bias)
  - Transparency:
    - i.e., being able to understand how the AI system works, what input data it uses, what algorithms it uses, what outputs it produces, etc.  
  - Explainability
    - i.e., being able to change automated decisions if circumstances require it. 
  - Rectification
  - Human involvement
  - Governance of AI systems

---

### 10.2. Regulatory and Legal Constraints

##### 10.2.1. Responsibility Attribution

- Challenges in Assigning Responsibility
  - AI can not be seen as an independent ontology from the legal aspect, 
  - The automatic decision making systems are ususally black boxes thus is hard for people to understand.
  - Different nations have different attiutes towrards to AI.
  - Judicial views of computer decision-making have different views, as some courts are beginning to draw distinction between deterministic computers and AI
    - Deterministic systems
    - Autonomous systems
    - Probabilistic computing
- Legal Consideration in Pratice
  - Typical data-driven machine learning process
    - Bias can arise with history data, training data, test data, and new data
    - Bias can be inserted by the learning process
    - Bias can be inserted by the monitoring & feedback activities.
  - Examples

---

### 10.3. Ethical Dimensions

##### 10.3.1. Bias in AI

- Data-driven and Model-driven AI

  - Machine Learning/Deep Learning are usually data-driven

    - Patterns are found with no explanation as to why or what these mean
    - Identifying bias is difficult in data-driven systems. 
      - We don’t know what factors were used to make the decisions or recommendations.
      - If the program undergoes evolution or learning, then the developers may not know what code results.

  - In model-driven approaches, the AI system has a model of the application domain

    - Since Windows95, every version of Windows OS has a Bayesian Belief Network linking causes with effects in printer operations, to help diagnose the causes of printer problems.

    > e.g. A causal model connecting causes with effects.

- Identifying Bias in Data-driven Systems

  - Since we cannot control the output, we focus on what we can control the production process:
    - Looking for bias in the input, training and test data
    - Testing the algorithm for correctness (if we can)
    - Looking at flows of data BETWEEN different AI systems
    - Ensuring good AI Governance


##### 10.3.2. Transparency 

- Motivation

  - In model-driven approaches, it is usually straightforward to see how a conclusion was reached by the AI. Thus we can follow through the IF-THEN rules or reason over the causal model.
  - In contrast, many data-driven approaches are dark ("black boxes"), thus we cannot see how a conclusion was reached.
  - To gain transparency, we may have to build a second AI to mimic the workings of the first.
    - A model-driven AI to mimic the workings of the data-driven AI.

- Explainability in Model-driven AI

  - Model-driven or symbolic approaches to AI are usually able to generate explanations, 

    - Because they have a model of the application domain & are transparent

    > e.g.
    >
    > An Expert System comprising `IF ... THEN ...` Rules
    >
    > - `IF` the patient has lost sense of smell `THEN` the patient could have CV19
    > - `IF` the patient has a new persistent cough `THEN` the patient could have CV19
    > - ...

  - We can create an explanation for a particular automated diagnosis from the particular IF-THEN rules invoked in the trace of that decision

  - Similarly, for other model-driven AI, such as Bayesian Belief Networks.

- Explainability in data-driven AI

  - In AI methods that are data-driven, such as Neural Networks & Deep Learning methods, the machine is manipulating data without it knowing what the data means.
  - At no point, does the program have any understanding of what is a chin, or an ear, or a face. Thus, it is very *difficult* to create an explanation for how the decision was reached.
  - Current Machine Learning and Deep Learning methods are still very immature
    - The resulting systems are not robust to small changes in inputs
    - This makes them easy to hack
  - The data-driven approaches require lots of data, however, for many situations we do not have enough data.

##### 10.3.3. AI Governance

- Introduction

  - Companies are starting to put in place processes to govern the creation and deployment of AI systems.

  - Typically, this will involve a special internal AI Governance committee:

    - With representatives of different departments e.g., IT, Operations, Legal.
    - In the best case, including 1-2 outsiders (to avoid "group think").
    - To vet potential AI projects and to oversee their deployment.

  - Modeled on the Pharmaceutical industry, where these committees are standard.

  - Companies are also adopting company-wide policies for use of AI.

    > e.g. Vodafone AI Framework

- Singapore Model AI Governance Framework

  - The framework is a voluntary set of compliance and ethical principles and governance considerations and recommendations that can be adopted by organisations when deploying AI technologies at scale. 
  - It is NOT legally binding.
  - The Model Framework is based on two high-level guiding principles:
    - Organisations using AI in decision-making should ensure that the decision-making process is explainable, transparent and fair, and 
    - AI solutions should be human-centric.
  - The 2020 edition of the Framework includes real-life industry case studies demonstrating effective implementation of the AI Framework by organisations.

##### 10.3.4. Humans in the Loop

- Definition
  - A key question is to what extent humans should be involved in automated decision-making processes.
  - Some regulations only apply to decision-making systems with no humans in the loop.
  - The human role needs to be sincere (not just for show), or it is likely to be rejected by courts. This gives a question:
    What level of human involvement is appropriate?
- Solution
  - Reconsidering your orders. 
