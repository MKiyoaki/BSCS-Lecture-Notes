## WEEK VI - Neural Networks

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK V - Kernel Machines](year3/6ccs3ml1/w5.md)
>
>[âž¡ï¸ WEEK VII - Reinforcement Learning 1](year3/6ccs3ml1/w7.md)
>
>Outlines:
>
>1. Perceptron & Single-layer Networks
>     - Introduction to ANNs
>     - Perceptron
>     - Single-layer Networks
>     - Error Correct Method (Only for false samples)
>       $$
>       w_{i} \leftarrow w_i + \alpha (t - g(s))x_i \\
>       g(\cdot) = \text{step}(\cdot)
>       $$
>     - Delta Learning Rule
>       $$
>       w_{i} \leftarrow w_i + \alpha (t - s)x_i
>       $$
>     - Backpropagation (Generalized Delta Learning Rule)
>       $$
>       w_{i} \leftarrow w_i + \alpha (t - g(s))g'(s)x_i
>       $$
>2. Multilayer Networks
>    - Motivation: Handle nonlinearity
>    - MLP
>    - Backpropagation in MLP
>    - Momentum
>3. Design and Implementation Issues
>     - Overfitting
>     - Training Tricks
>     - Vanishing Gradient Problem
>     - Variants of NNs

> Ref: 
>
> [7CCSMPNN: Week3](year3/7ccsmpnn/w3.md)
>
> [7CCSMPNN: Week4](year3/7ccsmpnn/w4.md)

### 6.1. Perceptron & Single-layer Networks

##### 6.1.1. Introduction to Neural Networks

- Motivation
  - Inspired by biological neurons and their connectionist nature.
  - Synapses (junctions that allow signal transmission between the axons and the dendrites) are *excitatory* or *inhibitory* and may change over time.
  - When inputs reach a threshold, an *action potential* (electrical pulse) is sent along the axon to the outputs.
  
- Definition

  - **Artificial Neural Network** (ANN): a mathematical function that can produce approximations of real values, discrete values (integer or ordinal/categorical) or vectors of values.

  - ANNs are made up of nodes which have:

    - Input edges, each with some **weights**, 
    - Output edges with **weights**, 
    - An **activation level** (function or inputs)

  - Weights can be positive or negative and may change over time (through the learning)

  - **Input function** is the weighted sum of the activation levels of inputs.

  - The **activation level** is a non-linear transfer function $g$ of this input:
    $$
    \text{activation}_j = g(s_j) = g(\sum^N_{i=1} w_{i, j} x_i)
    $$

  > There are two directions in NN research historically:
  >
  > 1. Trying to simulate how the (human) brain works: aim to model the process inside the human brain
  > 2. Producing machine learning methodologies: aim to model the outcome, without trying to mimic the process

##### 6.1.2. Perceptron

- Definition

  - The simplest type of ANN, which consists of a single neuron.

  - The input of the neuron is the weighted sum of the inputs plus the bias term and the output is some function of the input.

    > e.g.
    >
    > 1 if the result is greater than some threshold and -1 otherwise

  - A perceptron can compute **linearly separable** functions.

- Example

  ```mermaid
  graph LR
  	x0[x0=1] 
  	x1
  	x2
  	sigma(Î£)
  	
  	x0 --(w0=-th)--> sigma
  	x1 --(w1)--> sigma
  	x2 --(w2)--> sigma
  	
  	sigma --s--> g
  ```

  - $x_1, x_2$ are inputs; $x_0$ is 1. 

  - $w_1, w_2$ are synaptic weights; $w_0$ is a **bias** weight.

  - $th$ is a threshold.

  - $s$ is the dot product of the input $\textbf{x}$ and weight $\textbf{w}$ vectors, plus the bias $w_0$:
    $$
    s = w_0 x_0 + w_1x_1 + w_2x_2 = -th + w_1x_1 + w_2x_2
    $$

  - $g$ is a transfer/threshold/activation function applied to $s$.

- Activation Function

  - **Sign** function:

    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Signum_function.svg/1920px-Signum_function.svg.png" alt="undefined" style="zoom:10%;" />

    $$
    g(s) = 
    \begin{cases}
    1 & s>0\\
    -1 & s<0
    \end{cases}
    $$

    - The output is either 1 or -1 depending on the input.
    - It can be used for binary classification. 

  - **Step** function:

    <img src="https://www.intmath.com/laplace-transformation/svg/svgphp-unit-step-functions-definition-1a-s0.svg" alt="undefined" style="zoom:80%;" />
    $$
    g(s) = 
    \begin{cases}
    1 & s \geq t\\
    0 & s < t
    \end{cases}
    $$

  - **Sigmoid** function:

    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/2560px-Logistic-curve.svg.png" alt="undefined" style="zoom:10%;" />
    $$
    g(s) = \frac{1}{1 + e^{-s}}
    $$

  - **Hyperbolic tangent** (tanh) function:

    <img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png" alt="img" style="zoom:40%;" />
    $$
    g(s)=\tanh(s)=\frac{e^s - e^{-s}}{e^s + e^{-s}}
    $$

-  Compute Linearly Separable Functions

  - A single perceptron can represent many boolean functions.
  - A perceptron can compute linearly separable functions.


##### 6.1.3. Single-layer Networks

- Definition
  - A perceptron is a **single-layer neural** network (i.e. one input layer and one output layer).
  - However, there can be multiple perceptrons in a single layer network (Multi-unit perceptron).
    - Individual units are trained independently.
    - We will consider (single unit) perceptrons.
  
- Binary Classification
  
  - Two class classifier: `1` or `-1`.
  
  - If the data is linearly separable, then the **error correction** method will find the linear boundary between classes.
  
  - For $n$ inputs, the boundary is a hyperplane:
    $$
    \begin{array}{rcl}
    w_0x_0 + w_1x_1 + ... + w_nx_n &= &0 \\
    \sum^n_{i=0} w_ix_i &= &0 \\
    \textbf{w} \cdot \textbf{x} &= &0\\
    \end{array}
    $$
    where $x_0 = 1$. 
  
  - The idea is to find the weights $w_0, ..., w_n$ so that the members of one class all have value `1` and the members of the other class all have value `-1`.
  

##### 6.1.4. Error Correction Method

- Process

  - We train a perceptron by adjusting its weights:
    - Begin with random weights
    - repeat until stopping condition:
      - for each training example, update each weight by
        $$
        w_i \leftarrow w_i + \Delta w_i \\
        \Delta w_i = \alpha (t - g(s))x_i
        $$
        where
        - $\alpha$ is the **learning rate**,
        - $t$ is the **target output** of the network,
        - $g(s)$ is the **prediction** produced by the neural network
      - The learning rate moderates the degree to which weights are changed at each step.

  - The bias weight $w_0$ sets the threshold at which the sign-function is applied.

  - For update steps

    - If $t = g(s)$, no update needed.

    - If $t > g(s)$ (i.e. perceptron outputs `-1` when the target is `1`):
      - the weights must be altered to increase the value of $\textbf{w} \cdot \textbf{x}$.

      - if $x_i > 0$, then increase $w_i$ (otherwise, decrease)

    -  If $t < g(s)$ (i.e. perceptron outputs `1` when the target is `-1`):
      - decrease weights associated with positive $x_i$

##### 6.1.5. Delta Learning Rule

- Motivation

  - Issue: Convergence theorem for the perceptron learning rule
    - If the boundary between classes is linear, and $\alpha$ is small enough, the error correction method will find it. The method will converge within a finite number of steps and it will classify all training examples correctly.
    - If the boundary between classes is non-linear, the error correction method can fail to find (an approximation to) it because no linear hyperplane can separate the data perfectly.
  - The **delta rule** can overcome this, by producing a boundary that approximates the non-linear decision boundary.
  - This is gradient descent once again.

- Introduction to Delta Rule

  - Consider an unthresholded perceptron (i.e. a linear unit):

  - Use an error function $E$ to show how far the output is from our desired output:
    $$
    error = E(\textbf{w}) = \frac{1}{2} \sum^M_{j=1}(t_j - s_j)^2
    $$
    where

    - $M$ is the number of instances in our dataset,
    - $t_j$ is the target evaluated for the $j$-th element,
    - $s_j$ is the prediction evaluated for the $j$-th instance.

  - The goal is to **minimise** this error.

  > Note.
  >
  > We are not considering the transfer/threshold function: we are training the unit without the transfer function by looking at the output of the weighted sum.
  >
  > We use **gradient descent** to find the weight values that minimise the error function.

- Gradient Descent

  - Definition

    <img src="https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png" alt="Gradient Descent: A comprehensive guide to Implementing in Python" style="zoom:67%;" />

    - This illustrates the solution space or **landscape** for a simple perceptron with two weights $\{\theta_0, \theta_1\}$.
    - With **gradient descent**, the idea is to move down the landscape, into the valley where the error is small.
    - The gradient points in the direction of greatest increase of the function at each point.
    - The arrow shows the negated gradient at one particular point: This is the direction in the plane producing steepest descent along the error surface.

  - Calculation

    - The gradient is computed by taking the derivative of $E$ with respect to each of the $n $ elements in the weights vector $\textbf{w}$
      $$
      \begin{array}{lcl}
      \frac{\partial E}{\partial w_i} &= &\frac{\partial}{\partial w_i} \frac{1}{2} \sum^M_{j=1} (t_j - s_j)^2 \\
      ~ &= &\frac{1}{2} \sum^M_{j=1} \frac{\partial}{\partial w_i} (t_j - s_j)^2 \\
      ~ &= &\frac{1}{2} \sum^M_{j=1} 2(t_j - s_j)^2 \frac{\partial}{\partial w_i}(t_j - s_j) \\
      ~ &= &\sum^M_{j=1} (t_j - s_j) \frac{\partial}{\partial w_i} (t_j - \sum^n_{i=0} w_ix_{i,j}) \\
      ~ &= &\sum^M_{j=1} (t_j - s_j)(-x_{i,j})
      \end{array}
      $$
      
    - The Update Rule
      $$
      w_i \leftarrow w_i + \alpha \sum^M_{j=1}(t_j - s_j) x_{i, j}
      $$

      - Since the gradient gives the direction of the steepest increase in $E$, the negative of this vector gives the direction of steepest decrease. 
      - Don't forget $w_0$.

- Batch and Stochastic Gradient Descent

  - Batch Gradient Descent

    - What we have just described is **batch** gradient descent.
      - Sum errors over all training examples before adjusting weights.

    - Presenting all training examples once is called an **epoch**.

  - We can also train by **stochastic** gradient descent.

    - Adjust weights at each example:
      $$
      w_i \leftarrow w_i + \alpha(t-s)x_i
      $$

    - Same issues with stochastic gradient descent as before: may not converge.


##### 6.1.6. Backpropagation

- After Learning

  - With the delta rule, we learn with no transfer function. However, we add the transfer function back in afterwards.
  - That means we move from a regression to a classification.
  - This gives a sharp(ish) boundary between classes, depending on the transfer function used.
  - The *bias weight,* $w_0$, sets the midpoint of the transfer function.

- Generalised delta rule (Backpropagation)

  - The delta rule updates weights based on the error in the unthresholded linear combination of inputs.

  - **Generalised delta rule** keeps the transfer function. But the transfer function has to be differentiable. 

    >  e.g., sigmoid, tanh.

  - This gives a new rule for changing weights:
    $$
    w_i \leftarrow w_i + \alpha (t - g(s)) g(s) (1 - g(s)) x_i
    $$

    - With the *sigmoid*, $g(s)(1 âˆ’ g(s))$ varies in value from 0 to 1.
      - It has value 0 when $g(s)$ is 0 or 1.
      - It has maximum value of 0.25 when $g(s)$ has value 0.5 (and the input to the sigmoid is 0).

- Example

  > e.g.
  >
  > Perceptron with sigmoid transfer function:
  >
  > - Using the chain rule:
  >   $$
  >   \frac{\partial E}{\partial w_i} = ... = -(t-g(s))
  >   $$
  >
  > - While sigmod function is $g(s) = \frac{1}{1 + e^{-s}}$, we transofrm the partial derivative as follows:
  >   $$
  >   \frac{\partial g(s)}{\partial s} = g(s)(1 - g(s))
  >   $$
  >   
  >
  > - Thus we get
  >   $$
  >   \frac{\partial g(s)}{\partial w_i} = -(t - g(s)) g(s) (1-g(s)) x_i
  >   $$


---

### 6.2. Multilayer Networks

##### 6.2.1. Motivation

- Limitation of Single-layer Networks

  - Many functions are NOT linearly separable.

    > e.g. XOR operation

  - Possible solution: a **multi-layer network**

    > e.g.
    >
    > $x_1 \text{ XOR } x_2$ 
    >
    > can be rewritten as 
    >
    > $(x_1 \text { AND } x_2) \text{ NOR } (x_1 \text { NOR } x_2)$
    >
    > > i.e.
    > >
    > > ```mermaid
    > > graph LR
    > > 	in1((0/1))
    > > 	in2((0/1))
    > > 	AND((AND))
    > > 	NOR1((NOR))
    > > 	NOR2((NOR))
    > > 	
    > > 	in1 --> AND
    > > 	in1 --> NOR1
    > > 	in2 --> AND
    > > 	in2 --> NOR1
    > > 	AND --> NOR2
    > > 	NOR1 --> NOR2
    > > 	NOR2 --> output
    > > ```

##### 6.2.2. Multi-layer Perceptrons (MLPs)

- Definition

  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1280px-Colored_neural_network.svg.png" alt="undefined" style="zoom: 25%;" />

  - A **multi-layer perceptron** is usually known as a(n) *(artificial) neural network*

    > i.e. one input layer, one output layer and one or more hidden layers of processing units. 

    - Unit for constructing multi-layer networks:
      - A unit whose output is a non-linear function of its inputs but whose output is also a differentiable function of its inputs. (Sigmoid unit)
    - A **feedforward** network has a layered structure
      - Each layer consists of units which receive their input from units from a layer directly below and send their output to units in a layer directly above the unit.
      - There are no connections within a layer.

  - A multi-layer network is a **non-linear function** between the inputs and the outputs.

  - The hidden units

    - produce hidden values

    - use non-linear threshold functions 

      > i.e. the hidden values are non-linear functions of the inputs. 

  - Typically, a non-linear threshold function or normalization is applied at the end.

    > This ensures that the output is a value between [0, 1], or [âˆ’1, 1] as before.

  - Weights are applied at each layer.

  - During learning, weights are updated so that the output matches the input.

- Non-linearity

  - Definition
    - A multi-layer network of linear functions creates a piece-wise linear boundary.
    - A smooth non-linear boundary needs non-linear outputs on the constituent units.
  - *Universal function approximator* theorem
    - A feed-forward neural network with at least one hidden layer (and sufficient number of hidden units) can approximate almost any continuous function. 

##### 6.2.3. Backpropagation in MLPs

- Definition

  - In a multi-layer network, each layer is evaluated in sequence.
  - Sometimes a **bias** is added in at each layer.
  - **Backpropagation** is the classic approach to train the model by using gradient descent.  
    - The weights are adjusted by moving backwards:
      - First from the *hidden* <- *output* layer
      - Then from the *input* <- *hidden* layer

    - The amount of adjustment is proportional to the value of the error function:
      - Big errors -> Big adjustments
      - Small errors -> Small adjustments

    - The error rate should decline during the training process.

- Process

  - Error at the output: $E = y - g(s)$

    - Can use this to change the weights between the hidden layer and the output.

  - Introduce a *modified error* which is the product of error E and the weighted sum input into out (note the use of sigmoid):
    $$
    \Delta = g(s) (1 - g(s)) (y - g(s)) = g \left( \sum^H_{j=0} h_j w_j \right) \left( 1 - g \left( \sum^H_{j=0} h_j w_j \right) \right) E
    $$

  -  The update for $w_j$ becomes
    $$
    w_j \leftarrow w_j + \alpha h_j \Delta
    $$

    - $h_j$ is responsible for some fraction of the error $âˆ†$ in every input node it is connected to.

    - Weigh $âˆ†$ according to $w_j$ to give a $âˆ†_j$ for each node in the hidden layer.
      $$
      \Delta_j = g \left( \sum^H_{j=0} h_j w_j \right) \left( 1 - g \left( \sum^H_{j=0} h_j w_j \right) \right) w_j \Delta
      $$

    - Again, this is very similar to the update for a single perceptron, where the error term is $w_jâˆ†$.

    - This then gives us a rule for updating each of the weights $w_{i,j}$ on the links between $h_j$ and input $x_i$ :
      $$
      w_{i, j} \leftarrow w_{i, j} + \alpha x_i \Delta_j
      $$


  - Typically there are multiple outputs.

    > Training examples have multiple outputs
    >
    > The neural network can have multiple outputs
    >
    > Threshold outputs are also a vector

    - Treat each output unit just like we treated the single output unit we considered.

    - Thus, we define the effect of each output $o$ on each hidden unit $j$ as follows:
      $$
      \delta_{j, o} = g \left( \sum^H_{i=0} h_j w_{i, j} \right) \left( 1 - g \left( \sum^H_{i=0} h_j w_{i,j} \right) \right) w_j \Delta
      $$

    - Then, the total update at each hidden unit $j$ is:
      $$
      \Delta_j = \sum_o \delta_{j, o}
      $$
      and we then use $âˆ†_j$ as before.

- Evaluation
  - The error surface for multilayer networks may contain many different local minima.
  - Backpropagation over multi-layer networks is only guaranteed to converge toward some local minimum and NOT necessarily to the global minimum error.
  - Despite this, backpropagation is a highly effective function approximation method in practice.

##### 6.2.4. Momentum

- Definition

  - There are many variations on the backpropagation algorithm. One common variation is to introduce a **momentum** term.

  - Keep track of the weight updates in the previous iteration and make the weight update in the nth iteration depend on theupdate in the $(n âˆ’ 1)$ iteration.

  - The update rule is defined as follows:
    $$
    \Delta_{w_j}(n) = \alpha h_j \Delta + \mu \Delta_{w_j} (n-1)
    $$
    where $Î¼$ is the momentum.

  - Then update in the $n$th iteration using:
    $$
    w_j \leftarrow w_j + \Delta_{w_j}(n)
    $$

- Evaluation

  - The descent in weight space:
    - a: small learning rate
    - b: large learning rate (see oscillations) 
    - c: large learning rate and momentum
  - Can *roll over* local minima rather than getting stuck.

---

### 6.3. Design and Implementation Issues

##### 6.3.1. Overfitting

- Motivation
  - Many issues arise when implementing a neural network.
- Overfitting
  - When the network predicts the *training* data very well but fails on the *test* data set.
  - Generalization
    - The **generalisation accuracy** is a measure of how well the network generalises to data sets outside of the training set.
    - One method of overcoming generalisation accuracy problems is to tweak the weights each iteration, by small amounts.
    - Another method is to introduce a *validation* data set (in addition to the standard training and test sets).
      - Run the network on the training data, adjusting the weights and expecting to see rapid improvement in performance on the training data.
      - Also run the network on the validation data and keep the set of weights that performs best on the validation data.
      - In the end, return the set of weights that did best on the validation set, not the ones that did best on the training set.
    - Early stopping:
      - Evaluate on validation data after each epoch and stop training when the performance of the model has not improved for several epochs.

##### 6.3.2. Training Tricks

- Dynamically modifying network structure

  - The structure of the network can change dynamically.

    > e.g.
    >
    > For example start with a network with 0 hidden nodes, then gradually add nodes as you train the network.

- Training Tips

  - Normalization: Rescale inputs and outputs to be in the range 0 to 1 or âˆ’1 to 1.
  - Initialise weights to very small random values.
  - Stochastic or batch learning.
  - Three different ways to prevent overfitting:
    - limit the number of hidden nodes or connections (i.e., Reduce the complexity of the network architecture)
    - limit the training time, using a validation set
    - weight decay (decrease each weight by some small factor during each iteration)
  - Adjust learning rate and momentum to suit the particular task.
  - When to consider Neural Networks:
    - Input is high-dimensional discrete or real-valued.
    - Output is discrete, real valued or vector of values.
    - Possibly noisy data. ANNs are quite robust to noise in the training data.
    - Form of target function is unknown.
    - Long training times are acceptable. However, evaluating a new instance is quite fast.
    - Human readability of result is unimportant.

##### 6.3.3. Vanishing Gradient

- Motivation
  - The weights of a neural network are updated through backpropagation by finding the gradients.
  - In the case of a (very) deep NN, the gradient vanishes or explodes as it propagates backward which leads to vanishing and exploding gradient.
  - Reason
    - Using the chain rule, the derivatives of each layer are multiplied from the final layer to the initial layer to compute the derivatives of the initial layers.
    - As the number of layers increases, the number of products required to update the weights increases.
    - If the gradients become very small, <u>the updates to the weights will be negligible</u>.
- ReLU Function
  - Rectifier transfer function helps to prevent vanishing gradient problem.
  - Re(ctifier) L(inear) U(nit): $\text{ReLU}(x) = \max(0, x)$

##### 6.3.4. Variants of Neural Networks

- Convolutional Neural Networks (for image data)
  - Motivation
    - Solving an image classification problem using ANN would require to convert a 2D image into a 1-dimensional vector prior to training the model and thus losing the spatial features of an image (e.g. arrangement of pixels and the relationship between them in an image).
    - Convolutional neural network (CNN) captures the spatial features from an image.
- Recurrent Neural Networks (for sequence data)
  - Motivation
    - What to use to solve problems related to:
      - Sequence output (image captioning).
      - Sequence input (sentiment analysis).
      - Sequence input and sequence output (machine translation)
      - Synced sequence input and output (video classification on each frame).
    - ANN cannot capture sequential information in the input data.
    - Recurrent Neural Network (RNN) has a recurrent connection on the hidden state which ensures that sequential information is captured in the input data.
  - RNNs process an input sequence one element at a time, maintaining in their hidden units a *state vector* that contains information about the history of all the past elements of the sequence.
    - RNN can be seen as a feedforward neural network by unrolling according to the number of time steps.
    - We can then apply backpropagation but in the interest of memory consumption and computational efficiency, other techniques are applied (i.e. backpropagation through time).
  - Variations of RNN that deal with the vanishing gradien problem that RNNs suffer from:
    - Long Short-Term Memory units (LSTM)
    - Gated Recurrent Unit (GRU)

##### 6.3.5. Open Issues

- Require lots of data.
- Lots of CPUs/GPUs needed to train.
- Explainability.



