## WEEK IX - AI Ethics

>[🏠 MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[⬅️ WEEK VIII - Reinforcement Learning](year2/5ccs2int/w8.md)
>
>[➡️ WEEK X - Revision](year2/5ccs2int/w10.md)
>
>Outlines:
>
>

### 21. Introduction to AI Ethics

##### 21.1. Background

##### 21.2. Key questions

- How is AI applied in pratice & Who has access & Who benefits. 
- What are the harms and ethical considerations of AI & Who is harmed. 
- How does bias manifest in AI & Why does it arise. 
- Who is responsible for the harms of AI.
- How do we minimise these harms.

---

### 22. Application and Access of AI

##### 22.1. How is AI applied

- For policing

  - Effects
    - Increased breadth of surveillance
      - More opportunities for police to surveil, stop-and-search specific communties they are biased against. 
    - Worse facial recognition performance on minorities due to lack of data
      - Algorithms more likely to identify a suspected criminal by mistake
      - Increased stop-and-search, wrongful arrest of specific communties
    - Lack of accountability of AI systems

- For military

  - Tasks
    - Search for specific (human) targets
      - Propagation of bias into war
      - More deaths
    - Automated defense
      - Dehumanization of war
      - Loss of moral engagement
      - Simplification of complex moral choices
      - Easier escalation
      - Arms races

- For political control

  - Tasks
    - State surveillance of opposition, ethnic & religious minorties
      - Strengthening **authoritarian** states
      - **Human rights** losses, e.g., privacy, freedom of speech, religious freedom, ...
  - Effects
    - Increased censorship and disinformation
      - Loss of **freedom of speech**
      - Less informed citizens
      - Political **polarisation and control**

- Revival of phrenology

  > Phrenology - disproved pseudoscience that assumes predications of gender, intelligence, criminality and internal states can be made from facial analysis. 

  - Current AI application
    - Predication of gender, sexuality, race, emotion, criminality. 
    - But these are social constructs/unobservable. (Should respect with the subjects' willing)
  - Allows oppression of already marginalized communities *(e.g., LGBT, Black)*
    - Reinforcing harmful stereotypes
    - Mis-identification, micro-aggressive ad-targeting

##### 22.2. Whos has access to AI

- AI is expensive
  - Lots of computation power
  - Expensive hardware
  - Money to train and to house
  - Big Tech monopoly (e.g., Google, Amazon, ...)
- Result
  - Universities cannot compete, resources used for Big Tech's priority problems.
  - Poorer labs.counteries cannot afford. 

---

### 23. Harms and Ethical Considerations of AI

##### 23.1. AI Harms

- Lack of human control & accountability
  - Many AI algorithms are complex, non-intuitive. Therefore it is <u>hard to understand and control</u>. 
  - Authority of objectivity, automation bias. Therefore there is <u>too much delegation and no accountability</u>. 
- Lack of safety
  - AI often not robust, therefore it is <u>not safe when conditions change</u>. 
  - AI often embedded in physical systems, so there is a risk on <u>physical safety</u>. 
- Discrimination
  - Impact and harms larger for specific groups
  - Some possible reasons
    - Lack of consideration during design
    - Lack of data
    - Lack of testing
    - Application problems
    - ...
- Privacy invasion, surveillance
  - AI allows more pervasive privacy invasion
  - Constant gata gathering to get data for training/selling. 
- Environmental and societal impact
  - Modern AI is power hungry
    - 100s of GPUs
    - 1000s of hours for training
  - AI relies on datacentres
    - Construction
    - Mineral mining
    - Cooling and pollution

##### 23.2. Ethical Principles of AI

- Many institutions adpoting guidelines for ethical development & deployment of AI
  - Governmental (e.g., UK EPSRC, NHS, EU, ...)
  - Professional (e.g., IEEE, ACM, ...)

---

### 24. Discrimination and Bias in AI

##### 24.1. Definition

- Types of Discrimination
  - **Direct discrimination** (or **disparate treatment**)
    - When somebody is disadvantaged <u>because of a personal attribute</u> *(e.g., age, gender, race, etc. )*. 
  - **Indirect discrimination** (or **disparate impact**)
    - When people with a certain personal attribute *(e.g., age, gender, race. )* are disadvantaged <u>even though the attribute is not explicitly considered in decisions</u>. 
- Bias
  - In the context of AI ethics, bias means
    - Imbalance of tendency in data (input)
    - Direct or indirect descrimination (output)
- Examples

##### 24.2. Causes of AI Discrimination

- Risks
  - Risk is not anticipated, tested, or alleviated
  
    > e.g.
    >
    > World bias - world distribution problem
    >
    > *Representation bias - data collection problem*
    >
    > *Measurement bias - wrong categorization of people/wrong measurements*
    >
    > *Algorithm bias - wrong choice of algorithm*
    >
    > Evaluation bias - wrong choice of evaluation metric or test set
  - Risk is obvious, problematic task
  
    - Ethics board does not flag a problem
    - *Developer does not oppose to build/report/below the whistle*
    - Management makes decision to deploy
  
- Who is descovering these issues
  - Women, Black scholars, LGBTQ+, etc. (importance of lived experience & diversity)

---

### 25. Responsibility and Minimising harms of AI

##### 25.1. Responsibility for the harms of AI

- Related keyholders
  - Institution that designs the system
    - For allowing any of the harms to take place
  - Ethics board
    - For passing an unethical product/application
  - Developers
    - Many hamrs best anticipated by developers
    - Many hamrs can only be tested/minimized by developers
    - Responsible for algorithm transparency, inefficiency, power consumption, ...

##### 25.2. Minimising the harms of AI

- Awareness of ethical issues, staying informed, being critical
- Participatory design
- Team diversity
  - Marginalized communities have different lived experience
  - Important to anticipate issues
- Balanced datasets
- Testing with diverse groups
- Documenting datasets, models, algorithms, design choices
- Logging, explainable methods
- Ethics boards
- Whistleblowing

