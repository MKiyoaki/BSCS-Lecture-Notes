## [WEEK VIII - Reinforcement Learning](https://keats.kcl.ac.uk/pluginfile.php/10920653/mod_resource/content/3/5CCS2INT___Lecture_Notes__Week8_RL.pdf)

>[🏠 MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[⬅️ WEEK VII - Supervised Learning](year2/5ccs2int/w7.md)
>
>[➡️ WEEK IX - AI Ethics](year2/5ccs2int/w9.md)
>
>Outlines:
>
>18. Intro to RL
>18. Greedy Algorithm
>18. UCB Algorithm
>

### 18. Introduction to Reinforcement Learning

##### 18.1. Definition

- Reinforcement Learning	

  - By repeatedly attempting to solve a problem or explore an environment, the agent learns about it and about the result of each of their actions.
  - A good example problem - *multi-armed bandit* (or *k-armed bandit*) problem
    - It simply refers to those slot machines you have in casinos. 
    - Each 'arm' is a different machine (or another lever on the same machine), with (potentially) different probability distributions.
  
    >e.g.
    >
    >- Consider a 2-armed bandit, with options A and B, with the following rules, 
    >  - As a player, you can either press A or B and get their respective reward. 
    >  - The outcome is probably not a constant value, but instead a value from a probability distribution. 
    >  - You can repeat the process as many times as you want, and your goal is to maximise your rewards in the long run (say, on average).
    >  - What should your strategy be? 
  
- Normal Distrubution

  - $\mathcal{N}(\mu, \sigma^2)$ - a normal distribution with <u>mean</u> $\mu$ and <u>standard deviation</u> $\sigma$ (and thus with variance $\sigma^2$). 

##### 18.2. Action-Value Methods

- Motivation

  - When deciding whether a given choice is better or worse, there are two ingredients to keep in mind, the *value* of an action and our *estimate* of it. So we need to define them. 

- Definition for solving $k\text{-armed Bandits}$

  - **Value of an action** in $k\text{-armed Bandits}$
    - Consider the $k\text{-armed Bandits}$ problem. We denote $R_t$ as the reward of taking action $a$ at time $t$. We call the <u>average reward</u> as the **value** of such action $a$ at time $t$, and denote it by
      $$
      q_⋆(a,t) = \mathbb{E}[R_t | A_t = a]
      $$

    - Moreover, when considering a stationary probability distribution, we have $q_⋆(a,t) = q_⋆(a,t')$ for any two time steps $t, t' \in \mathbb{N}$. Thus we simplify the notation and use $q_⋆(a)$ to indicate the value of action $a$​.
  
    > e.g.
    >
    > For $A \sim \mathcal{N}(10, 1), B \sim \mathcal{N}(11, 4)$, 
    >
    > > The real mean of the distributions $A$ and $B$ are 10 and 11, thus $q_⋆(A)=10, q_⋆(B)=11$​. Note that variances do not matter in this case. 
  
    > Note.
    >
    > In practice, however, we may not have access to the value (the ‘real’ average) a given action at a given time. The best we may be able to do, is to evaluate estimates the real average rewards. 
  
  - **Estimates of Average Rewards** in $k\text{-armed Bandits}$
  
    - Indicator function $\mathbb{1}_{predicate}$
      - A function that is very useful when we only want to consider certain situations. 
      - $\mathbb{1}_{predicate}$ is equal to 1 if predicate is *true*, and 0 otherwise. 
  
      > e.g. 
      >
      > $\mathbb{1}_{x>2} = \begin{cases} 1 &x > 2\\ 0 &x \leq 2 \end{cases}$
  
    - Given a $k\text{-armed Bandits}$ problem, an action $a$, and a time $t$, we denote the **estimated average reward** by action $a$ at time $t$​​ by
      $$
      Q_t(a)=\frac{\text{sum of rewards when action } a \text{ taken before }t}{\text{number of times action } a \text{ taken before }t} = \frac{\sum\limits^{t-1}_{i=1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum\limits^{t-1}_{i=1}\mathbb{1}_{A_i=a}}
      $$
  
    > e.g.
    >
    > Consider the sequence of actions and rewards in the following table. We assume that $Q_t(a) = 0$ if action $a$ has not being taken before $t$. Evaluate $Q_t(A)$ and $Q_t(B)$ for each $t \in [1, 5]$​.
    >
    > |  $t$  |  1   |  2   |  3   |  4   |  5   |
    > | :---: | :--: | :--: | :--: | :--: | :--: |
    > | $A_t$ | $A$  | $A$  | $B$  | $A$  | $B$  |
    > | $R_t$ | 0.5  | -0.3 | 0.4  | 1.0  | 1.5  |
    >
    > > We have that
    > >
    > > $\begin{matrix}Q_1(A)=0 &Q_1(B)=0 \\ Q_2(A)=0.5 &Q_2(B)=0\\ Q_3(A)=\frac{0.5+(-0.3)}{2}=0.1 &Q_3(B)=0\\ Q_4(A)=\frac{0.5+(-0.3)}{2}=0.1 &Q_4(B)=\frac{0.4}{1}=0.4\\ Q_5(A)=\frac{0.5+(-0.3)+1.0}{3}=0.4 &Q_5(B)=\frac{0.4}{1}=0.4  \end{matrix}$
  


##### 18.3. Exploration & Exploitation

- Motivation

  - The key idea is, if a choice (say, $a_2$) currently gives me a better reward on average, should I ever switch choices (say, move to $a_1$)? Note that this is applicable if I had more than 2 choices too.
  - On one hand, $a_2$ gives me better rewards, so I should stick with it. 
    - This is what is called **exploitation**.
  - On the other hand, who knows whether isn’t there anything better out there? 
    - Maybe $a_1$ gave me outliers before, and on average $a_1$ would be better than $a_2$, so I should try $a_1$ again and see. 
    - This is called **exploration**.
- Some possible strategies/algorithms.

  - A greedy algorithm. The next action is determined by the highest reward given thus far (or the higher average reward given so far by each action). 
  - A random algorithm. It just picks the next action at random.


##### 18.4. Conclusion

- Relationship between RL & MDP

  - For RL, the reward function is not known. We do not know the probabilities involved in the reward function, and thus we need to estimate it.
  - For $k\text{-armed Bandits}$, there is actually only one state. 

---

### 19. Greedy Algorithm

##### 19.1. Random Method

- Optimal action

  - The action with the highest reward value within the action options. 
  - It is safe to assume it is unique in this scenario, as the probability of getting two exactly equal values from a normal distribution is null.

- Random Method

  - Consider a $k\text{-armed Bandits}$ with actions $a_1, ... , a_k$. 
    - We define the random selection method the one that, at time $t$ selects action $A_t$ uniformly at random, i.e., for each $i, i \leq i \leq k$,
    $$
    \text{Pr}(A_t=a_i)=\frac{1}{k}
    $$

##### 19.2. Greedy Algorithm

- Definition

  - Consider a $k\text{-armed Bandits}$ with actions $a_1, ... , a_k$, with initial estimates $Q_1(a) = 0$ for all $a ∈ {a_1,...,a_k}$. 
    - We define the greedy selection method the one that, at time $t$ selects action $A_t$ according to
      $$
      A_t=\arg\max_{a \in \{a_1,..., a_k\}} Q_t(a)
      $$
      where $\arg\max_{x \in X f(x)}$ selects the value $x$ that belonging to set $X$ such that <u>a function $f(x)$ is maximised</u>. In case there is more than one action with maximum estimate, we randomise between those actions.

##### 19.2. $\epsilon$​-Greedy

- Definition

  - Consider a $k\text{-armed Bandits}$ with actions $a_1, ... , a_k$, with initial estimates $Q_1(a) = 0$ for all $a ∈ {a_1,...,a_k}$. 
    - We define the $\epsilon$-greedy selection method that, with probability $\epsilon$, chooses the next action according to the random method, and with probability $(1 − \epsilon)$, selects next action according to the greedy method.
    - $\epsilon=0$ could be treated as greedy algorithm and $\epsilon=1$ could be treated as random method. 
- Evaluation

  - There is one aspect of the $\epsilon$​-greedy algorithm that may not feel exactly right
    - When randomising, we have the same chance of choosing an action that looks more promising as choosing an action that clearly is no good. 
    - That happens even if we have sampled $a_1$ several times and are very confident of its low value (compared to the other actions). 

---

### 20. Upper Confidence Boundary (UCB) Algorithm

##### 20.1. Upper Confidence Boundary

- Motivation

  - For each action, we will have <u>a current average ($Q_t(a)$​)</u> together with <u>an associated confidence interval of that particular value</u>. 
  - The key idea is that, <u>the more I sample that action, the smaller my confidence interval becomes.</u> In that case, we will need to define the upper confidence boundary. 

- Upper Confidence Boundary

  - Consider a set of actions $\{a_1, ..., a_k\}$ in the context of a $k\text{-armed Bandits}$ problem. Let $Q_t(a)$ be the estimates of values of actions $a \in \{a_1 , ... , a_k \}$ at the start of step $t$ *(i.e., before a decision $A_t$ is made)*. 
    - In these condition, we define the **Upper Confidence Bound** method as the one in which the desicion action $A_t$ is chosen according to
      $$
      A_t = \arg\max_{a \in \{a_1, ..., a_k\}} [Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}]
      $$
      where $\ln t$ is the natural logarithm of $t$, and $N_t(a)$ is the number of times action $a$ has been chosen prior to step $t$. Finally, the parameter $c \geq 0, c \in R$, is called **exploration parameter**.

  - Special cases
    - When $N_t(a)=0$, the expression goes to $+ \infin$, and thus we consider $a$ a maximising action. That this means there is no need to have an initial value $Q_1(a)$, as we did in the greedy method. 
    - If more than one action is maximising, we define an increasing order in action index for action selection (the lower $i$ in $a_i$​). 
  - The value of the expression for a given action at a given time could be defined as 
    $$
    \text{UCB}_t(a)=Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}
    $$

##### 20.2. Connect with Exploration & Exploitation

- Explanation on the expression

  > $Q_t(a) \rightarrow \text{Exploitation Term}$
  >
  > $c \rightarrow \text{Exploration Parameter}$​
  >
  > $c\sqrt{\frac{\ln t}{N_t(a)}} \rightarrow \text{Exploration Term}$

  - What is happening is that $Q_t$, as before, represents our exploitation term, as the rest is our exploration term. 
  - The value $c$ is a parameter because it is defined a priori and does not depend on the samples and other data. 
    - Note that, the larger the exploration parameter $c$​​, the more prone to exploration our formula is going to be. 
    - A common value for $c$ is $\sqrt{2}$.

- Example

  > e.g.
  >
  > Determine $A_1, . . . , A_8$ in the *3-armed bandit* described as follows. In order for this exercise to work, you would need to sample actions $a_1, a_2, a_3$ depending on your choice $A_t$ at each time. 
  >
  > To circumvent that, below is a sequence of rewards you would get in the 5 first samples of each action.
  >
  > $\begin{matrix} a_1 : &5 &4 &7 &3 &7 \\ a_2 : &3.5 &7 &1 &1 &1 \\ a_3 : &6 &0 &4 &4 &3\end{matrix}$
  >
  > For example, if you sample with the action sequence $a_1, a_2, a_1$, you get 5, 3.5, and 4, respectively. As well as determining $A_1, . . . , A_8$, give the last $\text{UCB}$ values: the ones used to determine $A_8$, that is $\text{UCB}_8(a)$ for the three actions $a$. Use $c = 2$ as your exploration parameter.
  >
  > >1. At the very begining as $t=1$, we have $\text{UCB}_1(a_1)=\text{UCB}_1(a_2)=\text{UCB}_1(a_3)=\infin$, so we sample $a_1$ first such that $A_1=a_1$. 
  > >2. Then, $\text{UCB}_2(a_1)$ is a finite value so we will pick $A_2=a_2$. Similarly $A_3=a_3$.
  > >3. While $t=4$, we have that
  > >     $\begin{matrix} \text{UCB}_4(a_1) = &5+2\sqrt{\frac{\ln4}{1}}= &7.35\\ \text{UCB}_4(a_2) = &3.5+2\sqrt{\frac{\ln4}{1}} = &5.85 \\ \text{UCB}_4(a_3) = &6+2\sqrt{\frac{\ln4}{1}} = &8.35\end{matrix}$
  > >     So we will choose $A_4=a_3$. 
  > >4. We will then continue the calculation util we got the whole action sequences. 
  > >     $\begin{matrix} t=5: (7.53,6.03,4.79) &\rightarrow A_5=a_1\\ t=6: (6.39,6.17,4.89)&\rightarrow A_6=a_1\\ t=7: (6.94,6.28,4.97)&\rightarrow A_7=a_1\\t=8: (6.19,6.38,5.03)&\rightarrow A_8=a_2\end{matrix}$

