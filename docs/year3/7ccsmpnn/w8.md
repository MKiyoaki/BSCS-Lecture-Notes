## WEEK VIII - Support Vector Machines

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK VII - Feature Extraction](year3/7ccsmpnn/w7.md)
>
>[➡️ WEEK IX - Deep Generative Neural Networks](year3/7ccsmpnn/w9.md)
>
>Outlines:
>
>1. Introduction to Support Vector Machines
>1. Non-Linear Separable SVM
>1. Multi-

### 8.1. Introduction to Support Vector Machines

##### 8.1.1. Motivation

- Definition
  - Support Vector Machines (SVMs) based on *Linear Discriminant Functions*
    - Include margins to optimise solution
    - Can allow errors to occur in a controlled way
  - Comparable to Neural Networks
    - Allow non-linear mappings in higher dimensional feature space through use of Kernel functions
    - Advantages over NNs as simpler to select models and less susceptible to over-fitting
- Two-Class Classification Problem:
  - Given labelled training samples: $S = \{(\textbf{x}_1, y_1), (\textbf{x}_2, y_2), ..., (\textbf{x}_N, y_N)\}$, where
    - $\textbf{x}_i = \begin{bmatrix}x_{i1} \\ x_{i2} \\ ... \\ x_{id}\end{bmatrix}$
    - $y \in \{-1, 1\}, i \in \{1, 2, ..., N\}$
    - $N$ denotes the number of training samples. 

  - Goal: Design a hyperplane $f(x) = 0$ which can classify correctly (all) the training samples.
  - Process
    - To achieve the optimal hyperplane, we expect:
      - No errors, i.e., no mis-classification
      - Distance or margin between nearest *support vectors* and separating plane is maximal.


##### 8.1.2. Linear SVMs

- Definition

  - Deals with linearly separable 2-class classification problem.

  - Hyperplane is defined as follows
    $$
    f(\textbf{x}) = \textbf{w}^T \textbf{x} + w_0 = 0
    $$
    where $\textbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_d \end{bmatrix}$

  - Find $\textbf{w}$ and $w_0$ such that

    - The margin is optimal.
    - The classifier can correctly classify the samples, i.e., 

      $$
      \begin{cases}
      \textbf{w}^T \textbf{x} + w_0 \geq 1, &\forall \textbf{x} \in \text{class +1}\\
      \textbf{w}^T \textbf{x} + w_0 \leq -1, &\forall \textbf{x} \in \text{class -1}\\
      \end{cases}
      $$

- Optimal Margin

  - Distance of a point from a hyperplane
    $$
    z = \frac{|f(\textbf{x})|}{\parallel \textbf{w} \parallel}
    $$

    > e.g.
    >
    > $f(\textbf{x}) = w_1x_1 + w_2x_2 + w_0 = 2x_1 + 4x_2 - 6 = 0$
    >
    > > Distance from a point $(x_1, x_2) = (1, 3): z = \frac{2 \times 1 + 4 \times 3 - 6}{\sqrt{2^2+4^2}} = 1.789$ 

  - $\parallel · \parallel$ is the L2 norm operator (also known as Euclidean norm).

  - Achieve a maximum margin (distance) means

    - Find the largest margin $z$ <u>between the hyperplane and support vectors</u>.

    - The margin is given by:
      $$
      \begin{array}{l}
      \min_{x_i; y_i=-1} \frac{|f(x_i)|}{\parallel \textbf{w} \parallel} + \min_{x_i; y_i=+1} \frac{|f(x_i)|}{\parallel \textbf{w} \parallel} \\
      = \frac{1}{\parallel \textbf{w} \parallel} \left( \min_{x_i; y_i=-1} |f(\textbf{x}_i)| + \min_{x_i; y_i=+1} |f(\textbf{x}_i)| \right) \\
      = \frac{2}{\parallel \textbf{w} \parallel}
      \end{array}
      $$


##### 8.1.3. Constrained Optimisation Problem

- Definition
  $$
  \min_\textbf{w} J(\textbf{w}) = \frac{1}{2} \parallel \textbf{w} \parallel^2
  $$
  subject to $y_i(\textbf{w}^T \textbf{x}_i + w_0) \geq 1, i \in \{1, 2, ..., N\}$

  - Minimising $\frac{1}{2} \parallel \textbf{w} \parallel^2$ is equivalent to maximise the margin. The constraint $y_i(\textbf{w}^T \textbf{x}_i + w_0) \geq 1$ is to make sure all samples $\textbf{x}_i$ are correctly classified.

  - Method of Larange Multipliers
    $$
    \mathcal{L}(\textbf{w}, w_0, \lambda) = \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1)
    $$
    where $\lambda = \begin{bmatrix} \lambda_1 &\lambda_2 &... &\lambda_N \end{bmatrix}$, $\lambda_i \geq0, \forall i \in \{1, 2, ..., N\}$

    - The above primal problem can be transformed to the following dual problem, which is defined as follows:
      $$
      \max_{\lambda \geq 0} \min_{\textbf{w}, w_0} \left( \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1) \right)
      $$

    - Thus,
      $$
      \begin{array}{rcl}
      \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda) = 0}{\partial \textbf{w}} = \textbf{0} &\implies &\textbf{w} = \sum_{i=1}^N \lambda_i y_i \textbf{x}_i\\
      \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda) = 0}{\partial w_0} = 0 &\implies &\sum^N_{i=1} \lambda_iy_i=0
      \end{array}
      $$

    - By putting them together, we have
      $$
      \mathcal{L}(\lambda) = \frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1) = ... = \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j \textbf{x}^T_i \textbf{x}_j
      $$
      The dual problem is reduced to:
      $$
      \max_{\lambda \geq 0} \left( \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j \textbf{x}^T_i \textbf{x}_j \right)
      $$
      where $\sum^N_{i=1} \lambda_i y_i = 0, \lambda_i \geq, \forall i \in \{1, 2, ..., N\}$

    - Solution $λ_i$ to can be found by using *quadratic programming solver*.

      - It is a scalar function which does not depend explicitly on the dimension of the input space.
      - The solution of the Lagrange multipliers $\lambda_i$ may not be unique but the hyperplane characterised by $\textbf{w}$ and $w_0$ is unique.
      - For those $\textbf{x}_i$ with $\lambda \neq 0$, they are known as **support vectors**. As a result, $\textbf{w} = \sum^{N_s}_{i=1} \lambda_i y_i \textbf{x}_i, N_s \leq N$ denotes  the number of support vectors.
      - The support vectors lie on the two hyperplanes satisfying $\textbf{w}^T \textbf{x} + w_0 = ±1$. 
    
  - Rearranging the terms in $\mathcal{L}(\lambda)$.
    $$
    \begin{array}{lcl}
    \mathcal{L}(\lambda) &= &\frac{1}{2} \parallel \textbf{w} \parallel^2 - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1) \\
    ~ &= &\frac{1}{2} \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \lambda_i (y_i(\textbf{w}^T\textbf{x}_i + w_0) -1)
    
    \end{array}
    $$
    
    - To maximise $\mathcal{L}(\lambda)$ in $\lambda_i$,since $λ_i \geq 0$ and $y_i(\textbf{w}^T \textbf{x}_i + w_0)−1 \geq 0$, one possibility is to have $λ_i(y_i(\textbf{w}^T\textbf{x}_i +w_0)−1) = 0$ for all $y_i(\textbf{w}^T\textbf{x}_i + w_0)−1 \geq 0$, where $i \in \{0, 1, ..., N\}$.
    - This is supported by *Karush-Kuhn-Tucker (KKT)* conditions.
    - As $y_i(\textbf{w}^T \textbf{x}_i + w_0) −1 \geq 0$, it suggests that some $λ_i = 0$ for those $x_i$ not being a support vector.
      - Support vector $\textbf{x}_i: y_i (\textbf{w}^T\textbf{x}_i + w_0) - 1 = 0 \implies \lambda_i \neq 0$
      - Non-support vector $\textbf{x}_iy_i (\textbf{w}^T\textbf{x}_i + w_0) - 1 > 0 \implies \lambda_i =0$
    

##### 8.1.4. Summary for Linear SVM Classifier

- The linear SVM classifier (linearly separable case) can be found by solving the solution $(\textbf{w}, w_0, λ_i)$ to the following conditions:
  $$
  \begin{array}{lcl}
  \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda)}{\partial \textbf{w}} = \textbf{0} &\implies
  &\textbf{w} = \sum^N_{i=1} \lambda_i y_i \textbf{x}_i \\
  \frac{\partial \mathcal{L}(\textbf{w}, w_0, \lambda)}{\partial w_0} = 0 &\implies 
  &\sum^N_{i=1} \lambda_iy_i=0 
  \end{array}
  $$
  where
  $$
  \lambda_i \geq 0 \\
  \lambda_i(y_i(\textbf{w}^T\textbf{w}_i + w_0) -1) = 0, i \in \{1, 2, ..., N \}
  $$
  
- Hard classifier
  $$
  f(\textbf{x}) = \text{sgn}(\textbf{w}^T \textbf{x} + w_0)
  $$
  where $\text{sgn}(z) = \begin{cases}-1 &\text{if }z<0 \\ +1 &\text{if }z \geq 0\end{cases}$

- Soft classifier
  $$
  f(\textbf{x}) = h(\textbf{w}^T \textbf{x} + w_0)
  $$
  where $h(z) = \begin{cases} -1 &\text{if } z<-1 \\ z &\text{if } -1 \leq z \leq1\\ +1 &\text{if } z > 1\end{cases}$

---

### 8.2. Non-Linear Separable SVM

##### 8.2.1. Non-separable Case

- Generalize Formula

  - Three Categories of Input Samples:
    - Samples fall outside the band and are correctly classified: $y_i(\textbf{w}^T\textbf{x}_i + w_0) \geq 1$
    - Samples fall inside the band and are correctly classified: $0 \leq y_i(\textbf{w}^T\textbf{x}_i + w_0) < 1$
    - Samples are misclassified: $y_i(\textbf{w}^T\textbf{x}_i + w_0) < 0$
  - All 3 categories can be described as:
    $$
    y_i(\textbf{w}^T\textbf{x}_i + w_0) \geq 1 - \xi_i
    $$
    where $\xi_i$ is known as *slack variable*. First category: $ξ_i = 0$; Second category: $0 < \xi_i \leq 1$; Third case: $\xi_i > 1$.
  - $ξ_i$ for sample $x_i$ in the second/third category is obtained as $\xi_i = 1 - y_i(\textbf{w}^T\textbf{x}_i + w_0)$
- Objective

  - We want to <u>maximise the margin and minimise the number of misclassified point</u> (minimise the margin violations). We formulate the constrained optimisation problem as:
    $$
    \min_{\textbf{w}, w_0, \xi} J(\textbf{w}, \xi) = \frac{1}{2} \parallel \textbf{w} \parallel^2 + C \sum^N_{i=1} \xi_i
    $$
    subject to $y_i(\textbf{w}^T\textbf{x}_i + w_0) \geq 1 - \xi_i, \xi_i \geq 0$ and $i\in \{1, 2, ..., N\}$.
  
    - $\xi = \begin{bmatrix} \xi_1 &\xi_2 &... &\xi_N\end{bmatrix}$
    - $0 \leq C \leq + \infin$ is a pre-set constant scalar, which controls the influence of the two computing terms.
    - This is known as **soft-margin method**, the classifier is known as **soft-margin classifier**. 
      > Note. It is NOT same as soft classifier. 

##### 8.2.2. Constrained Optimisation Problem

- Process

  - The constrained optimisation problem is formulated as the following primal problem using the method of Lagrange multipliers:

    - Primal problem
      $$
      \mathcal{L}(\textbf{w}, w_0, \xi, \lambda, \mu) = \frac{1}{2} \parallel \textbf{w} \parallel^2 + C \sum^N_{i=1} \xi_i -  \sum^N_{i=1} \mu_i \xi_i - \sum^N_{i=1} \lambda_i (y_i (\textbf{w}^T \textbf{x} + w_0) - 1 + \xi_i))
      $$
      where $\mu = \begin{bmatrix} \mu_1 & \mu_2 &... &\mu_N\end{bmatrix}$, $\lambda = \begin{bmatrix} \lambda_1 & \lambda_2 &... &\lambda_N\end{bmatrix}$ are Lagrange multipliers, $\mu_i \geq 0, \lambda_i \geq 0$ for all $i \in \{1, 2, ..., N\}$.

    - The above primal problem can be transformed to the following dual problem:
      $$
      \min_{\textbf{w}, w_0, \xi} \max_{\lambda \geq 0, \mu \geq 0} \mathcal{L} (\textbf{w}, w_0, \xi, \lambda, \mu)
      $$

    - By replacing terms, we reduce the dual problem formula into the follows:
      $$
      \max_{\lambda \geq 0} \left( \sum^N_{i=1} \lambda_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \lambda_i \lambda_j y_i y_j \textbf{x}_i^T \textbf{x}_j \right)
      $$
      subject to $\sum^N_{i=1} \lambda_i y_i = 0$, where $0 \leq \lambda \leq C, i \in \{1, 2, ..., N\}$

  - The same remarks from linearly separable case apply.
  
  - This dual problem is the same as that of linearly separable case except a bound is given to $\lambda_i$.
    - $μ_i = 0, \xi_i \neq 0$; 
    - $μ_i \neq 0, \xi_i = 0$. 
    - $λ_i = 0$ for those samples fall outside the band and are correctly classified, i.e., the samples with $y_i(\textbf{w}^T\textbf{x} + w_0) \geq 1 (\xi_i = 0)$
    - $\lambda_i \neq 0$ for the following cases:
      - The sample with $y_i(\textbf{w}^T\textbf{x} + w_0) = 1 (\xi_i = 0)$
      - The sample with $y_i(\textbf{w}^T\textbf{x} + w_0) = 1 - \xi_i$
        i.e., samples fall inside the band and are correctly classified ($0 \leq \xi_i \leq 1$) or samples are misclassified ($\xi_i > 1$). 
    - The samples with $\lambda_i \neq 0$ contribute to the final solution $\textbf{w}$.
      - In the case of $\xi_i = 0, y_i(\textbf{w}^T\textbf{x} + w_0) > 1$, we have $\mu_i = C, \lambda_i = 0$.
      - In the case of $\xi_i = 0, y_i(\textbf{w}^T\textbf{x} + w_0) = 1$, we have $0 \leq \mu_i < C, 0 < \lambda)i \leq C$ which are governed by $C-\mu_i -\lambda_i = 0$.
      - In the case of $\xi_i > 0, y_i(\textbf{w}^T\textbf{x} + w_0) = 1 - \xi_i$, we have $\mu_i = 0, \lambda_i = C$. 
  
- Summary

  - The linear SVM classifier (non-separable case) can be found by solving the solution $(\textbf{w}, w_0, ξ_i, λ_i, μ_i)$ to the following conditions:
    $$
    \begin{array}{lcl}
    \frac{\partial \mathcal{L}(\textbf{w}, w_0, \xi, \lambda, \mu)}{\partial \textbf{w}} = \textbf{0} &\implies & \textbf{w} = \sum^N_{i=1} \lambda_i y_i \textbf{x}_i\\
    \frac{\partial \mathcal{L}(\textbf{w}, w_0, \xi, \lambda, \mu)}{\partial w_0} = 0 &\implies &\sum^N_{i=1} \lambda_i y_i = 0 \\
    \frac{\partial \mathcal{L}(\textbf{w}, w_0, \xi, \lambda, \mu)}{\partial \xi_i} = 0 &\implies & C - \mu_i - \lambda_i = 0
    \end{array}
    $$
    where
    $$
    \mu_i, \lambda_i \geq 0, \\
    \mu_i\xi_i = 0, \\
    \lambda_i(y_i(\textbf{w}^T\textbf{x}_i + w_0) - 1 + \xi_i) = 0, i \in \{1, 2, ..., N\}
    $$

  - Hard classifier:
    $$
    f(\textbf{x}) = \text{sgn}(\textbf{w}^T\textbf{x} + w_0)
    $$

  - Soft classifier
    $$
    f(\textbf{x}) = h(\textbf{w}^T\textbf{x} + w_0)
    $$


##### 8.2.3. Nonlinear SVMs

- Definition

  - While linear SVM involves feature mapping $\textbf{z}_i = \Phi(\textbf{x}_i), i \in \{1, 2, ..., N\}$, nonlinear SVM use $\textbf{z}_i$. 

    - Linear SVMs: 
      $$
      \textbf{w} = \sum_{i \in SVs} \lambda_i y_i \textbf{x}_i \implies \text{hyperplane: }\textbf{w}^T\textbf{x} + w_0 = 0
      $$

    - Nonlinear SVMs: 
      $$
      \textbf{w} = \sum_{i \in SVs} \lambda_i y_i \textbf{z}_i \implies \text{hyperplane: } 
      \begin{cases} 
      \textbf{w}^T\textbf{z} + w_0 = 0 & \text{in } \textbf{z} \text{ space}\\
      \underbrace{\sum_{i \in SVs} \lambda_i y_i \Phi(\textbf{x}_i)^T}_{\textbf{w}^T} \underbrace{\Phi(\textbf{x})}_{\textbf{z}} + w_0 & \text{in } \textbf{x} \text{ space}\\
      \end{cases}
      $$
      

  - Hard classifier:
    $$
    f(\textbf{x}) = \text{sgn}(\sum_{i \in SVs} \lambda_i y_i \Phi(\textbf{x}_i)^T \Phi(\textbf{x}) + w_0)
    $$

  - Soft classifier: 
    $$
    f(\textbf{x}) = h(\sum_{i \in SVs} \lambda_i y_i \Phi(\textbf{x}_i)^T \Phi(\textbf{x}) + w_0)
    $$

##### 8.2.4. Kernal and Kernel Functions

- Motivation

  - The original feature space is mapped to a higher-dimensional feature space through a feature mapping function $\Phi(\cdot)$.
  - Instead of computing $\Phi(\cdot)$ and then $\Phi(\cdot)^T \Phi(\cdot)$ (two-step computation), computing the kernel function $K(\cdot)$ can be done in one step saving the computational demand especially for feature mapping function of higher dimensional space.
    - When RBF kernel function is used, the SVM architecture is the same as the RBF network structure. However, the number of hidden units and the centres are determined by the optimisation procedure.
    - When sigmoid kernel function is used, the SVM architecture is the same as a three-layer fully-connected feed-forward neural network structure. However, the number of hidden units is determined by the optimisation procedure.
    - There is no systematic method to determine the best Kernel function and its parameters, and the parameter $C$ (hyper-parameters), which are usually chosen by trial and error.
  
- Definition

  - Certain Kernels that satisfy *Mercer’s Theorem* allow mapping to high-dimensional feature space implicitly:
    $$
    K(\textbf{x}_i, \textbf{x}) = \Phi(\textbf{x}_i)^T \Phi(\textbf{x})
    $$

  - Instead of finding the mapping function $Φ(·)$, it is easier to find the kernel function $K(·)$.
    - Hard classifier:
      $$
      f(\textbf{x}) = \text{sgn}(\sum_{i \in SVs} \lambda_i y_i K(\textbf{x}_i,\textbf{x})) + w_0)
      $$

    - Soft classifier: 
      $$
      f(\textbf{x}) = h(\sum_{i \in SVs} \lambda_i y_i  K(\textbf{x}_i,\textbf{x}) + w_0)
      $$

- Commonly Used Kernels

  - Linear kernel
  - Polynomial kernel of degree
  - Radial basis function (RBF) kernel (exponential kernel)
  - Multi-quadratic kernel
  - Inverse multi-quadratic kernel
  - Power kernel
  - Log kernel
  - Sigmoid Function (Hyperbolic Tangent)

- A kernel can be constructed from other kernels.

  - A linear combination of kernels can be defined as
    $$
    \sum_k \alpha_k K_k(\cdot), \alpha_k >0, \forall k
    $$

  - Product of kernels
    $$
    \prod_k \alpha K_k(\cdot)
    $$

##### 8.2.5. Application

- Speaker verification, Face detection, Hand-writing recognition, Biomedical, ...

---

### 8.3. Multi-class SVMs

##### 8.3.1. Multi-class Classification Problems

- Motivation
  - SVM classifiers is a *binary classifier*, which can handle only two-class problems.
  - Multi-class SVM classifiers can be built by combining two-class SVM classifiers.
- Definition
  - Given a dataset: $\{\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_N\}$ and each data point $x_i$ to class $C_i ∈ \{1,2,...,R\}; i \in \{ 1, 2, ..., N\}$, design a classifier which can tell which class the data $\textbf{x}_i$ belongs to.
  - Approaches combining SVMs
    - One against one
    - One against all
    - Binary decision tree
    - Binary coded
  - These approaches combining a number of two-class SVMs (linear or nonlinear) for multi-class classification.

##### 8.3.2. One-against-one Approach



##### 8.3.3. One-against-all Approach (Soft SVM classifiers)
