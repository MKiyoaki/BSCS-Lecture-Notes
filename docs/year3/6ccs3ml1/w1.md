## WEEK I - Introduction to Machine Learning

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>⏹️
>
>[➡️ WEEK II - Inductive Learning](year3/6ccs3ml1/w2.md)
>
>Outlines:
>
>1. Basic Concepts to Machine Learning
>2. Supervised Learning
>3. Unsupervised Learning
>   - Supervised Learning Scheme
>   - Clustering
>4. Performance Measurement
>   - Misclassification Rate
>   - K-Fold Cross Validation
>   - Model Selection
>   - Measurement Metrices

> Ref: [7CCSMPNN: Week1](year3/7ccsmpnn/w1.md)

### 1.1. Basic Concepts to Machine Learning

##### 1.1.1. Introduction to Machine Learning

- Definition
  - Machine Learning is the field of study that gives computers the ability to learn (from data) without being explicitly programmed. 
  - Traditional to consider that there are three kinds of ML.
    - Supervised learning
    - Unsupervised learning
    - Reinforcement learning

---

### 1.2. Supervised Learning

##### 1.2.1. Supervised Learning Scheme

- Example: Iris dataset

  - Motivation

    - Given an iris flower, which species does it come from?
      - *setosa*, 
      - *versicolor* or 
      - *virginica*

  - Successful machine learning requires 3 components:

    1. Representative **training data**: machine learning model learns from data

       - Data should be annotated (refers to meaningful labels associated with the data), or contains the right answers

    2. Sophisticated methods for extracting **features** that are used to represent the data

       - Features: easily observable properties of the data based on which the model will try to make predictions
       - The $x_i$ are attributes or features of the iris (easily observable properties of the data):
         - Petal area (length $x$ width)
         - Sepal area (length $x$ width)
       - The $x_i$ are typically $D$ dimensional vectors of numbers.

    3. Sophisticated choice of machine learning algorithm

       - Starting from a training set that contains the right answers $y_i$:
         $$
         \mathcal{D} = \{(x_i, y_i)\}^N_{i=1}
         $$

       - Training set is a set of input features $x_i$, and outputs $y_i$.

       - The $x_i$ can each be a vector of values, the $y_i$ are single elements/classes.

       - Learns a function $f$ that maps from features $x_i$ to a class $y_i: f(x_i) = y_i$. Given the **x**i , say what the correct yi is.

       - Can predict a class using a weighted combination of the input features.

- Generalization

  - Motivation
    - We want a machine learning model that performs well on new, never-before seen data, i.e.,  we want our model to **generalise** well. 
    - Important to generalise because of the long tail.

##### 1.2.2. Classification

- Definition

  - The response variable $y_i$ is typically a categorical value from some finite set, like *setosa* or *virginica*.
     A set of **labels** or **classes**. 
  - Assume $y \in \{1, ..., C\}$ where $C$ is the number of classes. In this case, the learning problem is **classification** or **pattern recognition**.
  - Common version of the problem is binary classification where $C=2$, and the labels are defined as $\{0, 1\}$. 
  - If $C > 2$ we say the problem is a multi

- Methodology

  - Think of classification is as function approximation: 
    - We assume that $y = f(x)$. Learning is then try to estimate $f$, creating an estimate function $\hat{f}$.
    - We then predicte an estimate of $y$ using $\hat{y} = \hat{f}(x)$. 
  - For problems without a clear boundary:
    - We cast the problem such that it returns a probability $P(y_i|x_i, \mathcal{D})$,  stands for the probability of being in each class, given the input and the set of training data.
    - Probabilistic classifiers provide a distribution over classes. i.e., If we have $C$ classes, we get a probability for each class.
    - Sometimes we will explicitly consider the model that we are using to make the prediction.

- Terms

  - We can decide on a single class by choosing the one with the highest probability given the input:
    $$
    \hat{y} = \arg \max_{c=1}^C P(y = c|x_i, \mathcal{D})
    $$

    - This is the most likely class label, the **mode** of the distribution.
    - This is called the **MAP** estimate, the maximum a posteriori estimate.

  - Other times we want the probability of each $y_i$ , then sample the class.

##### 1.2.3. Regression

- Definition

  - The response variable $y_i$ is typically a categorical value from some finite set, like *setosa* or *virginica*.
     A set of **labels** or **classes**. 

  - Assume $y_i$ is a real-valued scalar. In this case, the learning problem is **regression**. In ordinal regression, $y_i$ is a set of labels with some order. 

    > e.g. grades A-F

- Methodology

  - Assumes that the output $y$ is a linear combination of the input $x$, then we want to compute that
    $$
    y(\textbf{x}) = \sum_{j=1}^\mathcal{D} w_jx_j + \epsilon
    $$
    where the $w_j$ make up a **weight vector** $w$, and $\epsilon$ is the **residual error**.

  - Learning is the process of figuring out what the $w_j$ should be to give a good mapping. With different $w_j$ we get different functions $f$.

- Linear Regression

  - For a one dimensional input $x$, we write:
    $$
    \textbf{x} = (x_0, x_1) \\
    x_0 = 1 \\
    x_1 = x \\
    $$
    which is the same as saying $\textbf{x} = (1,x)$. 

  - We call $w_0$ the **intercept** or **bias** and $w_1$ is the **slope**.

  - Adding the intercept into $\textbf{x}$ makes it simpler to learn.

---

### 1.3. Unsupervised Learning

##### 1.3.1. Unsupervised Learning Scheme

- Definition

  - In unsupervised learning, there are no labeled or annotated data. Unlike supervised learning, here we are not given the right answer, or not told what the desired output is for each input.

  - Start from a set of examples:
    $$
    \mathcal{D} = \{\textbf{x}_i\}^N_{i=1}
    $$
    and look for patterns in the data.

  - The most common form of unsupervised learning is **clustering**: Looking to partition data into clusters: data within a cluster should be similar.

- Scheme

  - We will think of unsupervised learning is as **density estimation**. 
  
  - We take the task to be to build models of the form:
    $$
    p(\textbf{x}_i|\mathcal{D})
    $$
    Two differences from the supervised case:
  
    - We don't write $p(y_i |\textbf{x}_i , \mathcal{D})$, because we don't have a set of class labels to predict.
    - $\textbf{x}_i$ is typically a vector of features, so our prediction is a multivariate probability distribution. 
  
  - Labelled data is not common *in the wild*, so arguably unsupervised learning is more natural than supervised learning.

##### 1.3.2. Clustering

- Definition
  - Looking to partition data into groups / clusters: data within a cluster should be similar.

  - Let $K$ be the number of clusters.

    - We need to estimate the distribution over the number of clusters $p(K|\mathcal{D})$.

    - Often this is simplified by approximating as follows:
      $$
      K^* = \arg \max_K p(K|\mathcal{D})
      $$

    - Picking the right $K$ is important. This is an example of model selection. 

##### 1.3.3. Parametric vs. non-parametric

- Definition

  - If the technique make assumptions about the structure of the data, it is **parametric**.

    > e.g. there are four Gaussian clusters. 

  - Otherwise, it is **non-parametric**. 

- Features

  - Parametric models can be computationally simpler. But the assumptions that they make can lead to inaccuracy.

    > e.g. linear regression

  - Non-parametric models are more flexible (as there is no assumptions). But, they can be intractable for large datasets. 

- Example: k-Nearest Neighbour (kNN) Classification

  - Introduction

    - Simple **non-parametric** classifier: Looks at the $k$ points in the training set that are nearest to the test input $\textbf{x}$.
    - The simplest version of kNN chooses the class with the most points in the $k$ nearest set.

  - Scheme

    - A more sophisticated version uses the $k$ nearest points to estimate the probability of class membership:
      $$
      p(y=c|\textbf{x}, \mathcal{D}, K) = \frac{1}{K} \sum_{i \in N_K (\textbf{x}, \mathcal{D})} \mathbb{I}(y_i = c)
      $$

      - $N_K(\textbf{x}, \mathcal{D})$ are the indices of the $k$ nearest points to $\textbf{x}$ in $\mathcal{D}$, and

      - $\mathbb{I}(y_i = c)$ is an indicator function such that:
        $$
        \mathbb{I}(e) = 
        \begin{cases}
        1 &\text{if } e \text{ is true} \\
        0 &\text{if } e \text{ is false}
        \end{cases}
        $$

    - Counts how many members of each class are in the $k$ nearest set. 

      - Typically **Euclidean distance** (in a suitable dimension) is used.
      - This limits the data to being real-valued.

  - Feature

    - kNN classifiers are simple and can work well.
    - However, they scale badly: do not work well with high dimensional inputs.
    - In general, get better performance by summarising the examples rather than using them directly.

##### 1.3.4. Scalability and Problems

- Curse of dimensionality
  - Reason
    - Since the the scalability of kNN: To better distinguish between examples, add more features (dimensions). This leads to a need for more examples.
    - With more features we need more examples to determine how the features distinguish.
  - Solutions
    - One way to address the curse is to use **parametric** models.
    - Make some assumption about the best way to distinguish between examples. Means assuming something about the way examples are distributed/generated.
- Overfitting
  - Definition
    - Modeling minor variations in the input. 
    - Results in low errors on the training data and high errors on new examples.

---

### 1.4. Performance Measurement

##### 1.4.1. Misclassification Rate

- Motivation

  - We would like to know whether we have learnt well.

    > e.g. In supervised learning, this means "can we predict $y_i$ well with given $x_i$"?

- Misclassification rate

  - **Misclassification rate** gives us some idea whether we have learnt well from the training data. Can be computed as follows:
    $$
    err(F, \mathcal{D}) = \frac{1}{N} \sum^N_{i=1} \mathbb{I}(F(x_i) \neq y_i)
    $$

  - However, it doesn't say anything about new data. What we care about is **generalization error**.

  - Can estimate this by trying the classifier on test data that we didn't see in training.

- Learning curve

  - Learning curve = % correct on test set as a function of training set size

- Validation set

  - Partition the training set into three subsets: *training set*, *validation set* (aka development set), and *test set*.

    - *Validation set* to model selection and/or **parameter tuning**.

    - *Test set* to evaluate generalisation performance of best model according to validation set.

      > Typical split of the data: 80% train; 10% dev; 10% test.

    - Need to ensure enough data for training but also enough data for tuning and testing.

##### 1.4.2. K-Fold Cross Validation

- Motivation

  - Create validation set from the train set if the usable number of training examples is very small.

- Definition

  - Split data into $k$ equal subsets / folds to test on. Train on $k - 1$ sets and test on the remainder fold only.

  - Repeat $k$ times. i.e., test on each fold only once. Computer **misclassification error** averaged over all $k$ test folds.

  - The final performance is the average of the performances for each fold.

    > Common values of $k$ are 5 and 10, both giving error estimates that are very likely to be accurate.

  - The extreme case is when $k = n$, the number of data points. i.e., **Leave-one-out cross validation**. 

##### 1.4.3. Model selection and/or parameter tuning

- No free lunch theorem
  - There is no best model for all scenarios. Thus we have to do model selection.
  - And models may have different algorithms for learning, which also have tradeoffs. Speed, accuracy, complexity.

##### 1.4.4. Measurement Metrics

- Accuracy

  - *Accuracy* measures the proportion of *correct* predictions. Mainly aimed at classification problems. 
    $$
    \text{accuracy} = \frac{\text{correct}}{\text{total}}
    $$

  - Dual of the misclassification rate. 

  - Higher is better. 

- Confusion Matrix

  - Compare actual class labels with predicted labels.

    > e.g.
    >
    > | Predicted / Actual | **T**          | **F**          |
    > | ------------------ | -------------- | -------------- |
    > | **T**              | True Positive  | False Positive |
    > | **F**              | False Negative | True Negative  |

  - Can expand for any number of class labels.

- F1 Score

  - Precision

    - *Precision* is the number of true positives divided by the sum of the true positives and the false positives. 

    - i.e., Positive predictive value.

    - Compute as follows:
      $$
      \text{Precision} = \frac{TP}{TP + FP}
      $$

  - Recall

    - *Recall* is the number of true positives divided by the sum of true positives and false negatives.

    - i.e., Sensitivity, or true positive rate.

    - Compute as follows:
      $$
      \text{Recall} = \frac{TP}{TP + FN}
      $$

  - F Score Balances the recall and precision, weighting them equally.

    - Compute as follows:
      $$
      F_\beta = (1 + \beta^2) (\frac{\text{precision × recall}}{\beta^2 \text{precision + recall}})
      $$

    - Harmonic mean of precision and recall.

    - F2 and F0.5 are also commonly used.

      - $β = 2$ weights recall higher. $β = 0.5$ weights precision higher.

- ROC Curve

  - Motivation

    - As discussed above, most classification problems can be cast as a probabilistic prediction returning as probability, and we turn this by a conversion function into true or false label, according to the threshold. 
    - Moving the threshold changes TP and FP.

  - Definition

    - Plot pairs of TP and FP (as we vary the threshold).

    - Dotted line is performance of a random classifier (on average).

    - For a good system, the graph climbs steeply like this. 

      <img src="https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c42679571ef35419c9904_647603c5a947b640e4b1eecd_classification_metrics_img_header-min.png" alt="How to explain the ROC AUC score and ROC curve?" style="zoom:50%;" />

    - Area under the curve is related to the probability that the classifier will correctly classify a randomly chosen example.

- Evaluating Clusters

  - Motivation

    - For clustering, most of the time we don't know what the clusters should be. We want to find methods to evaluate the selection of $k$. 
    - External evaluation.
    - Internal evaluation: Try to establish how coherent the clusters are and how well they are separated from each other.

  - Example

    - The Davis-Bouldin index for $n$ clusters:
      $$
      DB = \frac{1}{N} \sum^n_{i=1} \max_{j \neq 1} \frac{\sigma_i + \sigma_j}{d(c_i, c_j)}
      $$
      where

      - $c_i$ is the centroid of cluster $i$, 
      - $\sigma_i$ is the average distance of points in cluster $i$ to the centroid of the cluster, and 
      - $d(·, ·)$ is a distance metric.

    - The DB index will be small when clusters are tightly grouped and far from each other.

