## WEEK V - Kernel Machines

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[⬅️ WEEK IV - Probabilisitc Models 2](year3/6ccs3ml1/w4.md)
>
>[➡️ WEEK VI - Neural Networks](year3/6ccs3ml1/w6.md)
>
>Outlines:
>
>1. Introduction to Kernel Machines
>1. Kernel and Kernel Functions
>1. Kernel-Based Methods

### 5.1. Introduction to Kernal Machines

##### 5.1.1. Kernal Machines

- Motivation
  - Consider linear classifiers, drived by gradient descent. 
    - But they can only learn linear decision boundaries. 
  - Could combine many linear classifiers to learn complex boundaries.
    - But it is difficult to train, since there are many local minima/maxima in the high dimensional weight space.
  - Kernel Machines (Support Vector Machines)
    - Efficient training.
    - Complex non-linear functions. 
  
- Process
  - Typical classifiers minimize *expected empirical loss* on training data.
  
  - Kernel machines attempt to minimize *generalisation loss*.
    - Computational Learning Theory tells us that we do this by maximising the **margin** on the examples we have seen so far: maxmize margin separator.
      
    - As linear classifier has the form $y = \textbf{w}^T\textbf{x} + b$. Changing $\textbf{w}$ will change the decision boundary (separator). We expect the separator has the max distance to the nearest datapoints (in the training set) in either class.
      
      > i.e., Finding the optimal decision boundary (separator) means finding the biggest margin.
      
      - Margin: distance of two lines that parallel with the decision boundary and cross the nearest two datapoints. 
      
        > i.e., twice the distance from the separator to the nearest datapoint.
      - Decision function fully specified by a (small) subset of training samples, the **support vectors**.
      
        - The points closest to the separator are the support vectors. 
      
    
  - Example
    
    > e.g.
    >
    > Assume $\textbf{w}^T\textbf{x}_i \geq +1$ when $y_i = +1$, and $\textbf{w}^T\textbf{x}_i \leq -1$ when $y_i = -1$. Our margin is then defined by
    > $$
    > h_1: \textbf{w}^T\textbf{x} +b = +1 \\
    > h_2: \textbf{w}^T\textbf{x} +b = -1
    > $$
    > The distance between decision boundary and $h_1$ or $h_2$ is then $\frac{1}{\parallel \textbf{w} \parallel }$. Where $\parallel \textbf{w} \parallel $ is the norm of vector $\textbf{w}$ (Eulidean length): $\sqrt{w_1^2 + w_2^2}$.
    >
    > Total distance between $h_1$ and $h_2$ is then $\frac{2}{\parallel \textbf{w} \parallel }$. 
  

##### 5.1.2. Gradient Descent for Linear Cases

- Motivation

  - We use gradient descent to find the Optimal $\textbf{w}$, look for the maximum margin separator that classifies examples correctly.

    $$
    \max_{\textbf{w}, b} \frac{2}{\parallel \textbf{w} \parallel}
    $$
    such that
    $$
    y_i (\textbf{w}^T\textbf{x}_i + b) \geq 1, i = 1, ..., m
    $$
    The constraint simply means that $\textbf{w}^T\textbf{x}_i \geq +1$ when $y_i = +1$, and $\textbf{w}^T\textbf{x}_i \leq -1$ when $y_i = -1$. 

    i.e., learnt parameter gives +1 for class `+1`, and -1 for class `-1`.

  - Learning Process: Find $\textbf{w}$

    - Easier to convert into a quadratic optimisation problem with linear constraints. 

    $$
    \min_{\textbf{w}, b} \frac{1}{2} \parallel \textbf{w} \parallel^2
    $$
    such that
    $$
    y_i (\textbf{w}^T\textbf{x}_i + b) \geq 1, i = 1, ..., m
    $$
    
    - Objective function + inequality constraints = constrained optimisation problem
    - Deal with such problems by Lagrange multipliers $\alpha_i \geq 0$ and a Lagrangian
    
    $$
    L(\textbf{w}, b, \alpha) = \frac{1}{2} \parallel \textbf{w} \parallel ^2 - \sum^m_{i=1} \alpha_i (y_i (\textbf{w}^T\textbf{x}_i + b) - 1)
    $$
    
    - The minimum is found by taking its partial derivatives and setting them equal to zero
      $$
      \frac{\partial L}{ \partial \textbf{w}} = \textbf{w} - \sum_i \alpha_i y_i \textbf{x}_i \\
      \frac{\partial L}{ \partial b} = - \sum_i \alpha_i y_i
      $$
      Substituting these (and simplifying) we get a **dual representation**. The optimal separator in the input space is found by solving the following formulation:
      $$
      \arg\max_{\alpha} \sum^m_{i=1} \alpha_i - \frac{1}{2} \sum^m_{i, j = 1} \alpha_i \alpha_j y_i y_j (\textbf{x}_i \cdot \textbf{x}_j)
      $$
      where $m$ is the number of training examples, and
    
      $$
      \alpha_i \geq 0, \sum^m_{i=1} \alpha_i y_i = 0
      $$
      
      > Note.
      >
      > For the dual, the expression $\textbf{x}_i \cdot \textbf{x}_j$ is important. These indicates two vectors of features. 
      
    - Can convert back to $\textbf{w}$ using
      $$
      \textbf{w} = \sum_i \alpha_i y_i \textbf{x}_i
      $$
    
      - There is a single, global maximum (convex).
      - The data is only involved as a dot product.
      - The weights $\alpha_i$ associated with each data point are all zero, except for the points closest to the decision boundary (support vectors).

- Decision Function

  - Since our motivation: For a specific $\textbf{x}$, we want to know whether it is $+1$ or $-1$, we can stay in the dual representation.

  - If we substitute, the decision function can be written as 
    $$
    h(\textbf{x}) = sign (\sum_i \alpha_i y_i (\textbf{x} \cdot \textbf{x}_i) + b )
    $$
     Obviously we only need to do this for the $\textbf{x}_i$ that are support vectors.

  - Establishing the dual representation involves rewriting $\{\textbf{x} : \textbf{w} \cdot \textbf{x} + b\}$ using Lagrange multipliers.



---

### 5.2. Kernel and Kernel Functions

##### 5.2.1. Non-Linearity

- High-Demision Space Cases

  > e.g., 
  >
  > We want to classify two data classes where the decision boundary is a circle. 
  >
  > We want to separate two data classes in a 3-D space, where the decision boundary is a plane. 

  - In a higher dimensional space, the separator will be a **hyperplane**.

  - Thus, we define

    - Initial space: $\textbf{x} = (x_1, x_2)$

    - Map $\textbf{x}$ to $F(\textbf{x})$ such that
      $$
      f_1 = x_1^2 \\
      f_2 = x_2^2 \\
      f_3 = \sqrt{2} x_1 x_2
      $$
      Projecting into $x_1^2, x_2^2$ space gives the linear classifier we saw before.

  - Data <u>will always be linearly separable</u> if we map them into a space with enough dimensions.

  - In general, $N$ data points, <u>will always be linearly separable in $N-1$ dimensions.</u>

##### 5.2.2. Kernel

- Definition

  - **Kernel** is the mapping from the input space $\textbf{x}$ to the higher dimension space.

  - Instead of 
    $$
    \arg\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i\alpha_jy_iy_j (\textbf{x}_i \cdot  \textbf{x}_j)
    $$
    We write
    $$
    \arg\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i\alpha_jy_iy_j (F(\textbf{x}_i) \cdot  F(\textbf{x}_j))
    $$

- We could replace $\textbf{x}$ by $F(\textbf{x})$ in any ML algorithm.

  - For cdot operation,
    $$
    F(\textbf{x}_i) \cdot  F(\textbf{x}_j)
    $$
    can be computed without first computing $F(\cdot)$ for each data point.

    - Thus, mapping into high dimensional space and then taking dot product means dot product squared.

  - Dot-product of polynomials:

    - For any degree $d$: $F(\textbf{u}) \cdot F(\textbf{v}) = (\textbf{u} \cdot \textbf{v})^d$
    - i.e., dot product to the power of $d$ means mapping into high dimensional space and then taking dot product.

- Advantage of Kernel Functions

  - Can compute what the dot product would be if we had actually projected the data.

    > e.g.
    >
    > Instead, we can use a kernel function $K(\textbf{x}, \textbf{z}) = F(\textbf{x}) \cdot F(\textbf{z})$. 
    >
    > For quadratic, $K(\textbf{x}, \textbf{z}) = (\textbf{x} \cdot \textbf{z})^2$. 

##### 5.2.3. Kernel Functions

- Definition

  - The expression $(\textbf{x}_i \cdot \textbf{x}_j)^2$ is the kernel function. Usually written as $K(\textbf{x}_i \cdot \textbf{x}_j)$. 

  - For any $K(\textbf{x}_i \cdot \textbf{x}_j)$, solving 
    $$
    \arg\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i, j}\alpha_i\alpha_jy_iy_j K(\textbf{x}_i \cdot \textbf{x}_j)
    $$
    will find a linear separator in the higher dimension feature space associated with $K(\textbf{x}_i \cdot \textbf{x}_j)$. 

- Kernel Trick

  - Plugging these kernels, optimal linear separators can be found efficiently in feature spaces with billions of dimensions.
  - The resulting linear separators, when mapped back to the original input space, can correspond to arbitrarily wiggly, non-linear decision boundaries.

- Other Kernel Functions

  - Polynomial kernels
    $$
    K(\textbf{x}_i \cdot \textbf{x}_j) = (\textbf{x}_i \cdot \textbf{x}_j)^d
    $$
    corresponds to a feature space that is exponential in $d$. 

  - Gaussian kernels
    $$
    K(\textbf{x}_i \cdot \textbf{x}_j) = \exp(-\frac{\parallel \textbf{x}_i - \textbf{x}_j \parallel^2_2}{2\sigma^2})
    $$

  - The sigmoid
    $$
    K(\textbf{x}_i \cdot \textbf{x}_j) = \tanh (\kappa (\textbf{x}_i \cdot \textbf{x}_j) + \Theta)
    $$
    where $d$ is an integer and $\sigma, \kappa, \Theta \in \mathbb{R}$

  - The radial basis function kernel
    $$
    K(\textbf{x}_i \cdot \textbf{x}_j) = \exp(-\frac{\parallel \textbf{x}_i - \textbf{x}_j \parallel^2_2}{2s^2})
    $$

    - This defines a spherical kernel with centre $\textbf{x}_i$ and radius $s$.
    - Comparing this to the Gaussian kernel, we can see this is basically the same thing.

  - Empirically, all of the above kernel functions have been found to give SV classifiers with similar accuracies and similar set of support vectors.

##### 5.2.4. Reasonable Kernels

- Defintion

  - A kernel function is **reasonable** if it is positive definite.

  - That is, for some feature space $\textbf{X}$:
    $$
    \sum_{i, j = 1}^n c_i, c_j K(\textbf{x}_i \cdot \textbf{x}_j) \geq 0
    $$
    for any $n$, and any scalars $c_i, c_j$ and any $\textbf{x}_i, \textbf{x}_j \in \textbf{X}$. 

    > Note.
    >
    > Feature spaces can be big.
    >
    > Hence the concern about the feature space relative to the number of training examples.

- Application of Kernels

  - Find optimal linear classifier with only linear kernels.
  - Kernelise any learning algorithm where we use dot products of pairs of data points.
    - $k$-nearest neighbour
    - Perceptron learning

---

### 5.3. Kernel-Based Methods

##### 5.3.1. $k$-Nearest Neighbour

- Dot Product
  - An aspect to understand: Measure of **similarity**.
    - When $\theta = 0$, $a \cdot b$ is maximum, $a, b$ are the same.
    - When $\theta = \frac{\pi}{2}$, $a \cdot b$ is zero, $a, b$ are orthogonal.
    -  So, in $k$-nearest neighbour, the dot product of one feature with another is a measure of *nearness* between them.
- Idea
  - Classify by finding the $k$ nearest examples and going with the majority classification. 
  - Instead of distance, compute similarity.

##### 5.3.2. Soft Margin Classifiers

- Motivation

  - In practice, a separating hyperplane may not exist.
    - For some problems, even in the high dimension space of the kernel, there is no hyperplane that cleanly separates the training examples.
    - A high level of noise can cause this to happen.
  - Thus, we relax the constraints under which we look for the hyperplane: **Soft margin** classifier.

- Definition

  - We do this by allowing **slack variables**:
    $$
    \xi_i \geq 0
    $$
    for all $i = 1, ..., m$

  - Then, instead of looking for
    $$
    y_i (\textbf{w} \cdot \textbf{x}_i + b) \geq 1
    $$
    for all $i$ in the original weight/feature space, we look for:
    $$
    y_i (\textbf{w} \cdot \textbf{x}_i + b) \geq 1 - \xi_i
    $$
    for all $i$.

  - A classifier that generalises well is then found by controlling both the *capacity* of the classifier, measured by $\parallel \textbf{w} \parallel$, and the amount of slack, measured by $\sum^m_{i=1} \xi_i$. 

  - This latter provides an upper bound on the number of training errors.

- Process

  - One way to achieve this, is to minimise:
    $$
    \tau(w, \xi) = \frac{1}{2} \parallel \textbf{w} \parallel^2 + C \sum^m_{i=1} \xi_i
    $$
    where $\xi_i \geq 0$, for all $i = 1, ..., m$, and

    $y_i (\textbf{w}\cdot x + b) \geq 1 - \xi_i$ for all $i$. 

    - The constant $C>0$ determines the amount of slack that is allowed.
    - This controls the trade-off between margin maximisation, which needs more slack, and  minimising training errors, which needs less slack.
    - Typically have to search to find the optimal $C$ for a given problem.

  - Next, we incorporate a kernel, rewrite in terms of Lagrange multipliers.

  - We end up solving:
    $$
    \arg\max_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i, j}\alpha_i\alpha_jy_iy_j K(\textbf{x}_i \cdot \textbf{x}_j)
    $$
    where $0 \leq \alpha_i \leq C$ for all $i = 1, ..., m$, and 
    $$
    \sum^m_{i=1} \alpha_iy_i=0
    $$

-  Evaluation

  - The only difference with the separable case is the upper bound $C$ on the $\alpha_i$
    - This limits the influence of individual data points.
    - The intuition is that this prevents outliers from having a big effect on the separator.
  - We end up with a hyperplane separator such that the **support vectors** $\textbf{x}_i$  which lie on the margins are the ones with the slack variables $\xi_i = 0$.

##### 5.3.3. Support Vector Regression

- Definition
  - While regression is a generalization of classification tasks, we are interested in the output for all classes $y \in \mathbb{R}$, instead of for only one class prediction $y \in \{1, -1\}$.
  - We do this by learning a function $f(\textbf{x})$ which approximates $y$.

- ε-insensitive Loss Function

  - We quantify the loss incurred by predicting $f(\textbf{x})$ rather than $y$ as:
    $$
    c(\textbf{x}, y, f(\textbf{x})) = |y - f(\textbf{x})|_\epsilon = \max \{0, |y - f(\textbf{x})| -\epsilon\}
    $$

  -  In effect we fit a tube with radius $ε$ to the data (data in the range with $y \pm 1$)

- Process

  - To estimate a linear regression $f(\textbf{x}) = \textbf{w} \cdot \textbf{x} + b$, we minimize 

  $$
  \frac{1}{2} \parallel \textbf{w} \parallel^2 + C \sum^m_{i=1} |y_i - f(\textbf{x}_i)|_\epsilon
  $$

  - Can transform this into a constrained optimisation problem by introducing slack variables, akin to the soft margin case. i.e., we minimize 
    $$
    \tau(w, \xi^{(*)}) = \frac{1}{2} \parallel \textbf{w} \parallel^2 + C \sum^m_{i=1} (\xi_i + \xi_i^*)
    $$
    where

    $f(\textbf{x}_i) - y_i \leq \epsilon + \xi_i$,

    $y_i - f(\textbf{x}_i) \leq \epsilon + \xi_i^*$,

    $\xi_i, \xi_i^* \geq 0$ for all $i$.

    > We have $\xi_i \implies f(\textbf{x}_i) - y_i > \epsilon$, and $\xi_i^* \implies y_i - f(\textbf{x}_i) > \epsilon$
    >
    > The conditions on the $\xi_i, \xi_i^*$ mean that errors smaller than $ε$ do not require a non-zero $\xi_i, xi|i^*$, and so don't affect the minimisation.

  -  Again we do the whole Lagrange multiplier thing, and end up with a regression estimate of:
    $$
    f(\textbf{x}) = \sum^m_{i=1} (\alpha_i^* - \alpha_i) K(\textbf{x}_i, \textbf{x}) + b
    $$

    - SVM idea, applied to regression.

##### 5.3.4. Gaussian Process Models

- Definition

  - Combination of SVM and probabilistic methods.

- Process

  - Take the basic likelihood calculation:
    $$
    p(h | \mathcal{D}) = \frac{p(\mathcal{D}|h)p(h)}{p(\mathcal{D})}
    $$

    - Model $p(\mathcal{D}|h)$ as a Gaussian.
    - This is the linear model. Thus we can kernelise it.

##### 5.3.5. Non-Parametric Methods

- Definition
  - Consider methods to be **non-parametric** if they cannot be characterised by a bounded set of parameters.
  - Instance-based methods like $k$-nearest neighbour are non-parametric because they require us to remember all the examples.
  - Grows without bound.
  
- Minkowski Distance
  - Motivation
    - If we want to classify by finding the $k$ nearest examples and going with the majority classification, we need a notion of distance.
    - **Minkowski distance**:
      $$
      L^p(\textbf{x}_j, \textbf{x}_q) = (\sum_i |\textbf{x}_{ij} - \textbf{x}_{qi}|^p)^\frac{1}{2}
      $$
      which generalises Euclidian and Manhattan distances to $p$ dimensions.
    
    - Also use the **Mahalanobis distance** which takes into account the covariance between dimensions.
    
    - We also now know that we can use the dot product to find the nearest, where the metric is similarity between vectors.
    
  - Process
  
    - Piecewise non-parametric linear regression model.
  
      - When given a test $x_q$, uses training examples immediately to the left and right of $x_q$.
      - Good for non-noisy data, poor for noisy data.
  
    - Choose $k$ through cross-validation.
  
      - Motivation
  
        - $k$-nearest neighbour linear regression to find the best line. The result might be discontinuous.
  
      - Methods
  
        - Locally weighted regression.
        - Prevents discontinuity by discounting further points more and more.
        - No sudden drop off means no discontinuity.
  
      - Kernel methods help to choose the weight of each example.
  
        - Not quite the same kind of kernel.
        - Distance metric in this case.
        - Symmetric about 0, and area under the kernel is bounded.
  
      - Quadratic kernel
        $$
        K(d) = \max(0, 1 - \left( \frac{2|d|}{k}\right)^2)
        $$
        with kernel width $k=10$, $d$ is distance from the test point 0. 
  
        - $k$ too large vs. too narrow (choose using cross-validation). 
  
    - General form of a locally weighted regression:
      $$
      \textbf{w}^* = \arg\min_\textbf{w} \sum_j K(\text{distance}(\textbf{x}_q, \textbf{x}_j))(y_j - \textbf{w} \cdot \textbf{x}_j)^2
      $$
  
      - Solve this by gradient descent.
  
      - Then the answer is:
        $$
        h(\textbf{x}_q) = \textbf{w}^* \cdot \textbf{x}_q
        $$
  
      > Note.
      >
      > We need to solve this for every query point.
      >
      > However, we only solve for points with non-zero weight those inside the kernel.

