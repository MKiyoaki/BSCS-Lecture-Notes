## WEEK IV - Probabilisitc Models 2

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK III - Probabilisitc Models 1](year3/6ccs3ml1/w3.md)
>
>[âž¡ï¸ WEEK V - Kernel Machines](year3/6ccs3ml1/w5.md)
>
>Outlines:
>
>1. Markov Chains and Markov Models
>     - Example: Weather Prediction
>     - Hidden Markov Models
>     - Example: Dice HMM
>     - Fundamental Tasks with HMMs
>2. Viterbi Algorithm
>     - Introduction to Viterbi
>     - Viterbi Algorithm
>     - Baselines
>     - Evaluation

### 4.1. Markov Chains and Markov Models

##### 4.1.1. Example: Weather Prediction

>  e.g.
>
> Assume we have two types of weather: rainy and cloudy. The weather doesnâ€™t change within the day. 
>
> Can we guess what the weather will be like tomorrow?
>
> > We can use a history of weather observations:
> >
> > $P(w_t = \text{Rainy} | w_{t-1} = \text{Rainy}, w_{t-2} = \text{Cloudy}, w_{t-3} = \text{Rainy})$
> >
> > We can represent this as a Markov Assumption (first order):
> > $$
> > P(w_t | w_{t-1}, w_{t-2}, ..., w_{1}) \approx P(w_t | w_1)
> > $$
> > The joint probability of a sequence of observations is then:
> > $$
> > P(w_t | w_{t-1}, w_{t-2}, ..., w_{1}) = \prod_{t=1}^n P(w_t | w_{t-1})
> > $$

##### 4.1.2. Hiden Markov Models

- Markov Chains
  - A Markov Chain is a stochastic process that embodies the Markov Assumption. It can be viewed as a probabilistic finite-state automaton.
  - States are fully observable, finite and discrete; transitions are labelled with transition probabilities.
  - Models **sequential** problems: your current situation depends on what happened in the past.
  - Useful for modeling the probability of a sequence of events that can be *unambiguously observed*.
  
- Components
  
  - States and Observations
  
    - A set of $N$ emitting hidden states: $S_e = \{s_1, ..., s_N\}$
      - A start state $s_0$, and
      - An end state $s_f$.
  
    - An output alphabet of $M$ observations: $K = \{k_1, ..., k_M\}$
      - A start symbol $k_0$, and
      - An end symbol $k_f$. 
  
    - A sequence of $T$ observations, each one drawn from $K$: $O = O_1, ..., O_T$
    - A sequence of $T$ states, each one drawn from $S_e$: $X = X_1, ..., X_T$
  
  - First-order Hidden Markov Model
  
    - **Markov Assumption (Limited Horizon)**
  
      - Transitions depend only on current state:
  
      $$
      P(X_t|X_1, ..., X_{t-1}) \approx P(X_t|X_{t-1})
      $$
  
    - **Output Independence**
  
      - Probability of an output observation depends only on the current state and not on any other states or any other observations:
  
      $$
      P(O_t|X_1, ..., X_t, ..., X_T, O_1, ..., O_t, ..., O_T) \approx P(O_t|X_{t})
      $$
  
  - State Transition Probabilities
  
    - A state transition probability matrix $A$, in size of $(N + 2) \times (N+2)$. 
  
      - $a_{ij}$ is the probability of moving from state $s_i$ to state $s_j$ :
        $$
        a_{ij} = P(X_t = s_j | X_{t-1} = s_i) \ \forall \sum_{j=0}^{N+1} a_{ij} = 1
        $$
  
      - Transition probability is not associated with real observations.
  
      - $a_{0i}$ describe transition probabilities out of the start state into state $s_i$. Samilair for the end state $a_{if}$.
  
      - Transitions into start state and out of end state undefined.
  
  - Emission Probabilities
  
    - An emission probability matrix $B$, in the size of $(M + 2) \times (M + 2)$. 
  
      - $b_i(k_j)$ is the probability of emitting vocabulary item $k_j$ from state $s_i$:
        $$
        b_i(k_j) = P(O_t = k_j | X_t = s_i)
        $$
  
      - Our HMM is defined by its parameters $\mu = (A, B)$. 
  
- Examples where states are hidden
  
  > e.g.
  >
  > - Speech recognition
  >   - Observations: audio signal
  >   - States: phonemes
  > - Machine translation
  >   - Observations: target words
  >   - States: source words

##### 4.1.3. Example: Dice HMM

- Scenarios
  - Assume two dices: a fair one and a loaded one.
    - The fair one has the normal distribution of outcomes, $P(O) = \frac{1}{6}$ for each number from 1 to 6.
    - The loaded one has a different distribution.
  - State Machine
    - ...
    - There are two states (fair and loaded), and two special states (start and end state).
    - Distribution of observations differs between the states.

##### 4.1.4. Fundamental Tasks with HMMs

- Fundamental tasks with HMMs

  - Labelled Learning
    - Given a parallel observation (dice outcome) and state sequence $O$ and $X$, learn the HMM parameters $A, B$.
  - Unlabelled Learning
    - Given an observation sequence $O$ (and only the set of emitting states $S_e$), learn the HMM parameters $A, B$.
  - Likelihood
    - Given an HMM $Î¼ = (A,B)$ and an observation sequence $O$, determine the likelihood $P(O|Î¼)$.
  - Decoding
    - Given an observation sequence $O$ and an HMM $Î¼ = (A,B)$, discover the best hidden state sequence $X$. 

- Estimating Parameters of an HMM: Labelled Learning

  > e.g.
  >
  > | **s0** | F    | F    | F    | L    | ...  |
  > | ------ | ---- | ---- | ---- | ---- | ---- |
  > | **k0** | 1    | 3    | 4    | 5    | ..   |

  - Given a parallel observation (dice outcome) and state sequence $O$ and $X$, learn the HMM parameters $A, B$.

    

  - Transition matrix $A$ consists of transition probabilities $a_{ij}$:
    $$
    a_{ij} = P(X_{t+1} = s_j | X_t = s_i) \sim \frac{count_{trans}(X_t = s_i, X_{t+1} = s_j)}{count_{trans}(X_t = s_i)}
    $$

  - Emission matrix $B$ consists of emission probabilities $b_i(k_j)$:
    $$
    b_i(k_j) = P(O_t = k_j | X_t = s_i) \sim \frac{count_{emission}(O_t = k_j, X_t = s_i)}{count_{emission}(X_t = s_i)}
    $$

- Decoding

  - Motivation

    - In the dice porblem, there are Two hidden states: *L* (loaded dice), *F* (fair dice). We don't know which dice is currently in use. We can only observe the numbers that are thrown.
    - We estimate transition and emission probabilities (Labelled Learning).
    - We want the HMM to find out when the fair dice was out, and when the loaded dice was out.
    - We need a **decoder**.

  - Definition

    - Finding the most likely hidden state sequence $X$ that explains the observation $O$ given the HMM parameters $Î¼$.
      $$
      \begin{array}{lcl}
      \hat{X} &= & \arg \max_X P(X, O| \mu) \\
      ~ &= & \arg \max_X P(O| X, \mu) P(X | \mu) \\
      ~ &= & \arg \max_{X_1, ..., X_T} \prod_{t=1}^T P(O_t|X_t)P(X_t|X_{t-1})\\
      \end{array}
      $$

    - Search space of possible state sequences $X$ is $O(N^T)$. It is too large for brute force search. 

  - Viterbi

    - We can use Dynamic Programming if two conditions apply:
      1. Optimal substructure property
         - An optimal state sequence $X_1...X_j...X_T$ contains inside it the sequence $X1 . . . Xj$ , which is also optimal. 
      2. Overlapping subsolutions property
         - If both $X_t, X_u$ are on the optimal path, with $u > t$, then the calculation of the probability for being in state $X_t$ is part of each of the many calculations for being in state $X_u$. 

---

### 4.2. Viterbi

##### 4.2.1. Introduction to Viterbi

- The intuition behind Viterbi
  - Because of the Limited Horizon of the HMM, we don't need to keep a complete record of how we arrived at a certain state. For the first-order HMM, we only need to record one previous step. 
  - Just do the calculation of the probability of reaching each state once for each time step.
  - Then memoise this probability in a Dynamic Programming table
  - For $o$ order HMM with $N$ states, the time cost of viterbi is $O(N^{o+1}T)$. 
    - In first order HMM, $o=1$. 

##### 4.2.2. Algorithm

- Process

  - **Memoisation** is done using a *trellis*.

    - A trellis is equivalent to a <u>Dynamic Programming table</u>.
    - The trellis is $(N + 2) \times (T + 2)$ in size, with states $j$ as rows and time steps $t$ as columns. ($T$ is length of observation / time steps)

  - Each cell $j$, $t$ records the Viterbi probability $\delta_j(t)$, the probability of the most likely path that ends in state $s_j$ at time $t$:
    $$
    \delta_j(t) = \max_{1 \leq i \leq N} [\delta_i (t - 1) a_{ij} b_j (O_t)]
    $$

    - This probability is calculated by maximising over the best ways of going to $s_j$ for each $s_i$. 
    - $a_{ij}$: the transition probability from $s_i$ to $s_j$.
    - $b_j (O_t)$: the probability of emitting observation $O_t$ from destination state $s_j$. 

- Analysis

  - The probability of a state starting the sequence at $t = 0$ is just the probability of it emitting the first symbol.

  - Main step

    ```mermaid
    stateDiagram
    	S0 --> Sf_o1
    	S0 --> Sl_o1
    	
      Sf_o1 --> Sf_o2
      Sl_o1 --> Sf_o2
    
      Sl_o1 --> Sl_o2
      Sf_o1 --> Sl_o2
    	
    	Sf_o2 --> Sf_o3
    	Sl_o2 --> Sf_o3
    
    	Sl_o2 --> Sl_o3
    	Sf_o2 --> Sl_o3
    	
    	Sl_o3 --> St
    	Sf_o3 --> St
    
    ```

    - $\psi_j(t)$ is a helper variable that stores the $t - 1$ state index $i$ on the highest probability path.
      $$
      \psi_j(t) = \arg \max_{1 \leq i \leq N} [\delta_i (t - 1) a_{ij} b_j (O_t)]
      $$

    - In the backtracing phase, we will use $\psi$ to find the previous cell/state in the best path.

    - We need to keep $N$ states at each time step, since

      - We cannot only keep the final state for the optimal path. 
      - Future timesteps may lead the current optimal path being invalid. Since the future information is unkown, we need to keep the probability of states in each timestep to make correct decision. 

  - Precision and Recall

    - Motivation

      - We can measure system success using different metrices. In particular, we use precision, recall and F-measure.
      - These metrics are imported from the field of information retrieval, where the difference beween interesting and non-interesting examples is particularly high. 
      - Accuracy doesn't work well.

    - Computation

      |           | **F** | **L** | **Total**    |
      | --------- | ----- | ----- | ------------ |
      | **F**     | a     | b     | a + b        |
      | **L**     | c     | d     | c + d        |
      | **Total** | a + c | b + d | a + b + c +d |

      - Precision of L: $P_L = \frac{d}{b+d}$ 
      - Recall of L: $R_L = \frac{d}{c + d}$
      - F-measure of L: $F_L = \frac{2P_LR_L}{P_L + R_L}$

##### 4.2.3. Baselines

- Definition
  - A baseline should represent considerably **less effort** than your proposed solution.
  - It should be easily **understandable** to other researchers.
  - The **strength** of the baseline is important: the stronger the baseline the better for you, provided you still beat the baseline.
- Baseline for HMM
  - Random guess of hidden states
    - Only knows how often the HMM is in each state on average (the observed distribution of states). 
    - Does not know anything about sequences or observations. 
  - This tests how much HMM modelling helps in this highly ambiguous setup.
  - Objective
    - Your model produced a better system. Or at least, you observed higher performance (e.g. higher F measure).
    - We use a statistical test to gather evidence that one system is really better than another system.

##### 4.2.4. Evaluation

- Variation in the data

  - Some examples will make it easier for your system to score well, some will make it easier for the other system.

- Statistical Significance Testing

  - **Null Hypothesis**: two result sets come from the same distribution. 

    - First, choose a significance level ($Î±$), e.g., $Î± = 0.01$.
    - We then try to reject the null hypothesis with confidence $1 - Î±$ (99%in this case).
    - Rejecting the null hypothesis means showing that the observed result is very unlikely to have occurred by chance.

  - Reporting significance

    - If we successfully pass the significance test, and only then, we can report like

      > The difference between System 1 and System 2 is statistically significant at $Î± = 0.01$.

    - Any other statements based on raw accuracy differences alone are strictly speaking meaningless.

  - Sign Test (non-parametric, paired)

    - The sign test uses a binary event model.
    - Events have binary outcomes:
      - Postive. i.e., System 1 beats System 2 on this example.
      - Negative. i.e., System 2 beats System 1 on this example.
      - (Tie: System 1 and System 2 do equally well on this example, or have identical results).
    - Null hypothesis: baseline and our system are equally good
      - Prob of negative outcome = prob of positive outcome.
      - Counts obey a binomial distribution with a mean of 0.5

  - Binomial Distribution $B(N, q)$

    - Call the probability of a negative outcome $q$ (here $q = 0.5$).

    - Probability of observing $X = k$ negative events out of $N$:
      $$
      P_q (X=k|N) = \left( \begin{matrix}N \\k \end{matrix} \right) q^k (1 - q)^{N-k}
      $$

    - At most $k$ negative events:
      $$
      P_q (X \leq k|N) = \sum^k_{i=0} \left( \begin{matrix}N \\i \end{matrix} \right) q^i (1-q)^{N-i}
      $$ {Â }

  - Binary Event Model and Statistical Tests

    - If the probability of observing our events under the Null Hypothesis is very small (smaller than our pre-selected significance level $Î±$, e.g., 0.01), we can safely reject the Null hypothesis.
    - The $P(X \leq k)$ we just calculated directly gives us the probability $p$ we are after.
    - This means there is only a 1% chance that System 1 does not beat System 2.

- Two-Tailed vs. One-Tailed Tests

  - Testing difference in a specific direction:
    - System 1 better than System 2: **One-tailed test**
  - A more conservative, rigorous test would be a non-directional one
    - Testing for statistically significant difference regardless of direction: **Two-tailed test**
    - This is given by $2P(X \leq k)$ (because $B(N,0.5)$ is symmetric).

- Treatment Ties

  - When comparing two systems in classification tasks, it is common for a large number of ties to occur.
  - Disregarding ties will tend to affect a studyâ€™s statistical power.
  - Can treat ties by adding 0.5 events to the positive and 0.5 events to the negative side (and round up at the end).
