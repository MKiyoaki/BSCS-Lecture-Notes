## WEEK X - Learning from Demonstration

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[⬅️ WEEK IX - Evolutionary Algorithms](year3/6ccs3ml1/w8.md)
>
>[➡️ REVISION](year3/6ccs3ml1/re.md)
>
>Outlines:
>
>1. 

### 10.1. Learning from Demonstration

##### 10.1.1. Introduction to LfD

- Motivation

  - Problem: learn a mapping from (world) state to actions. This mapping is called a **policy**. Same as in reinforcement learning (RL).
  - In RL, learning is from experience. In Learning from Demonstration (LfD), learning is from examples. LfD is a *supervised* learning of policies, which aims to <u>learn the policy from the provided samples</u>.
  - An example in LfD is a sequence of state-action pairs.
    - Model-free learning.
  - The policy is derived only from those states encountered during learning.
  - So the policy is *partial*.
  
- Definition

  - $S$ is set of states.

  - $A$ is set of actions.

  - Transition function:
    $$
    T(s'|s, a): S \times A \times S \to [0, 1]
    $$

  - $Z$ is observed state, accessed through mapping:
    $$
    M: S \to Z
    $$

    - If state is fully observable, then $M = I$, where $I$ means identity. 

    - Policy selects actions:
      $$
      \pi: Z \to A
      $$

  - Rather like an MDP, but with the extra indirection of:
    $$
    π:Z \to A
    $$
    rather than:
    $$
    \pi : S \to A
    $$

  - **Partially observable.** 

- Varieties

  - In LfD, there is an explicit **teacher** who provides **demonstrations** that the learner trains on.

  - Demonstration $d_j \in D$, such that
    $$
    d_j = \{(z^i_j, a^i_j)\}
    $$
    where $z_j^i \in Z$ and $a_j^i \in A$ and:
    $$
    i = 0, ..., k_j
    $$

  - This is the sequence of state/action pairs discussed above.

  - The set of demonstrations, $D$, is what differentiates LfD from other learning methods.

  - Demonstration-based learning methods include:

    - Learning from Demonstration (LfD)
    - Learning by Demonstration (LbD)
    - Programming by Demonstration (PbD)
    - Learning from Observations
    - ...

- Process

  - Teacher demonstrates execution of some behaviour.
  - Learner receives these demonstrations and derives a policy able to reproduce the demonstrated behaviour.
    - Make design choices
      1. Demonstration approach
      2. Problem space representation
      3. Policy derivation
    - Execution
      1. *Gathering* examples
      2. *Deriving* policy

##### 10.1.2. Demonstration Approach

- Choice of Demonstrator
  - Human performs the task themselves (Robot observes)
  - Human teleoperates the robot to perform the task (Robot observes)
  
- Demonstration technique

  - Batch learning: Policy learnt once, after all data is gathered.
  - Interactive learning: Policy learnt incrementally, as data is gathered.

- Problem space representation

  - Continuity.
  - States: continuous or discrete.
  - Actions: continuous or discrete.
  - Often defined by domain, but some scope for choice.


##### 10.1.3. Policy Derivation

- Definition

  - Mapping function

    - $D$ is a dataset of state/action pairs.

    - Directly approximate mapping from observed state to actions.
      $$
      f(): Z \to A
      $$
      which can be seen as a classification problem. 

    - From state, measured by robot, pick action.

  - System model

    - $D$ is used to determine model of the world $T(s'|s, a)$ and a reward function $R(s)$.

    - These are then used to derive policy $π$.

      > Reinforcement learning, but a bit different from what we have seen already.

  - Plans

    - Apply AI planning methods.

    - $D$, and optionally model of users' intentions, used to learn rules.

    - Rules associate pre-conditions and post-conditions with each action:
      $$
      L(\{preC, postC\}, a)
      $$
      and optionally world model $T(s'|s, a)$.

    - Planner then creates policy $π$.

##### 10.1.4. Gathering Examples

- Demonstration

  - Identity/no embodiment mapping
    $$
    g_E (Z, a) \equiv I(Z, a)
    $$

  - Demonstration is performed on actual robot.

  - Teleoperation: human teacher operates the robot and robot's sensors record data
    $$
    g_R(Z, a) \equiv I(Z, a)
    $$

    - Joystick operation.
    - Kinesthetic teaching. 

  - Shadowing: robot operates itself but attempts to mimic human
    $$
    g_R(Z, a) \not\equiv I(Z,a)
    $$

    - Mimic motions, but record own motions.
    - Follow a teacher in a navigation task.

- Imitation

  - Non-identity embodiment mapping
    $$
    g_E(Z, a) \not\equiv I(Z, a)
    $$
  
  - Demonstration is performed by human and observed on actual robot.
  
  - Sensors on teacher
    $$
    g_R(Z, a) \equiv I(Z, a)
    $$
    so get direct data from demonstrator, but not the robot.
  
  - External observation
    $$
    g_R(Z, a) \not\equiv I(Z, a)
    $$
    so sensor data is external to demonstrator and robot.

##### 10.1.5. Methods for Policy Derivation

- Motivation

  - Picking the right action directly from the data.
    - Classification: for discrete actions
    - Regression: for continuous actions

- Classification

  - Simplest idea is to pick an individual low-level action.
    - Do this in many different ways:
      - Gaussian Mixture Models (GMMs)
      - k-Nearest Neighbours (kNN)
      - Decision trees
      - Bayesian networks
    - Low-level actions are then put together in a sequence.
      - Hidden Markov Model (HMM)
      - Dynamic Bayesian Network (DBN)
  - More complex idea starts with higher level actions.
    - High level actions are sequences.
    - Classify between sequences.

- Regression

  - Use any model that maps to continuous values.
  - Could just use kNN.
  - Locally Weighted Regression.
  - Neural Networks.

- Planning

  - Represent desired behaviour (i.e., policy choice) as a **plan**.
  - Sequence of actions that lead from initial state to goal state.
  - Plan has:
    - pre-conditions
    - post-conditions
    - world state $T(s'|s, a)$
    - action sequence
  - However, it does NOT handle non-determinism very well.

- System model

  - Derives policy
    $$
    \pi: Z \to A
    $$
    from model of the world $T(s'|s, a)$ and reward $R(s)$.

    - Reinforcement learning. Aim is to maximise cumulative reward over time.
    - Reward function could be engineered or learn.

  - Start with more policy than in active learning, less policy than in passive learning.

  - Start with the partial policy and exploration.

  - Rewards

    - Engineered reward
      - Corresponds to the usual case. Sparse rewards for particular states.
      - LfD has concentrated on helping to locate rewards, reducing blind exploration.
      - Demonstration can highlight rewards.
      - *Ask for Help* requests advice when all actions are approximately equally valued.
    - Learned reward.
      - Try to learn a reward function that generalises from individual rewards.
      - Function approximation in RL.

---

### 10.2. Confidence-Based Autonomy

##### 10.2.1. Introduction to Confidence-Based Autonomy

- Motivation
  - Interactive algorithm for LfD policy learning.
  - Initially, learner has no knowledge.
  - Learner acquires policy through observing demonstrations.
  - Learner practices the task.
  - If learner encounters unfamiliar state, then learner can ask for more demonstrations.
- Definition



---

### 10.3. Ethics of Machine Learning

