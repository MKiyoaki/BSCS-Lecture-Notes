## WEEK VII - Compilation in JVM

>[üè† MENU - 6CCS3CFL](year3/6ccs3cfl.md)
>
>[‚¨ÖÔ∏è WEEK VI - Parsing Combinators](year3/6ccs3cfl/w6.md)
>
>[‚û°Ô∏è WEEK VIII - Compiling Functional Languages](year3/6ccs3cfl/w8.md)
>
>Outlines:
>
>1. Compiling Expressions
>
>     - Code Generation in JVM
>
>     - Compiling Arithmetic Expressions
>
>     - Variables - Environment
>       - Local Variables 
>         - Extended Arithmetic Expressions
>         - Boolean Expressions
>
>
>2. Compiling Instructions
>
>    - Statements
>      - Assignment
>
>    - Conditional Jumps - Labels
>      - If-else 
>      - While-do
>
>3. Optimization
>    - Optimization Methods
> 

### 7.1. Compiling Expressions

##### 7.1.1. Code Generation in JVM

- Motivation

  - The purpose of a compiler is to transform a program a human can read and write into code <u>the machine can run as fast as possible</u>. 
  - The fastest code would be machine code the CPU can run directly, but it is often good enough for improving the speed of a program to target a virtual machine instead. 

- Process

  > ```mermaid
  > graph LR
  > 	file[.while]
  > 	file --> compiler
  > 	compiler --*.j--> assembler[Jasmin/Krakatau]
  > 	assembler --*.class--> JVM
  > ```
  >
  > An *assembler* will then translate the assembly files into unreadable class- or binary-files the JVM or CPU can run, i.e. bits and bytes. 
  >
  > Unfortunately, the Java ecosystem does not come with an assembler. As a substitute we shall use the 3rd-party program Jasmin, or alternatively Krakatau. 

##### 7.1.2. Compiling Arithmetic Expressions

- Process

  `1 + 2`

  ```assembly
  ldc 1
  ldc 2
  iadd
  ```

  - The first instruction loads the constant 1 onto the stack, 
  - the next one loads 2, 
  - the third instruction adds both numbers together replacing the top two elements of the stack with the result 3. 

- For simplicity, we will consider only arithmetic operations involving integer numbers. 

  - This means our main JVM instructions for arithmetic will be `iadd`, `isub`, `imul`, `idiv` and so on. 
  - The `i` stands for integer instructions in the JVM (alternatives are `d` for doubles, `l` for longs and `f` for floats, etc. ).

- Abstract Syntax Tree

  - Our parser will take this grammar and given an input program produce an abstract syntax tree. 

  - To generate JVM code for this expression, we need to traverse this tree in **post-order** fashion and emit code for each node‚Äîthis traversal in *post-order* fashion will produce code for a stack-machine (which is what the JVM is). 

    > e.g.
    >
    > `1 + ((2 * 3) + (4 - 3))`
    >
    > ```mermaid
    > graph TD
    > 	p1[+]
    > 	p2[+]
    > 	
    > 	p1-->1
    > 	p1-->p2
    > 	p2-->*
    > 	p2-->-
    > 	*-->2
    > 	*-->31[3]
    > 	-[-]-->4
    > 	-[-]-->32[3]
    > ```
    >
    > Doing so for the tree above generates the instructions
    >
    > ```assembly
    > ldc 1
    > ldc 2
    > ldc 3
    > imul
    > ldc 4
    > ldc 3
    > isub
    > iadd
    > iadd
    > ```
    >
    > If we run these instructions, the result `8` will be on top of the stack. 

- Definition

  - The followingrecursive **compile** function can handle with post-order-traversal, which takes the abstract syntax tree as an argument:
    $$
    \begin{array}{lll}
    compile(n) &\overset{def}= &\texttt{idc }n \\
    compile(a_1 + a_2) &\overset{def}= &compile(a_1) @ compile(a_2) @ \texttt{iadd} \\
    compile(a_1 - a_2) &\overset{def}= &compile(a_1) @ compile(a_2) @ \texttt{isub} \\
    compile(a_1 * a_2) &\overset{def}= &compile(a_1) @ compile(a_2) @ \texttt{imul} \\
    compile(a_1 / a_2) &\overset{def}= &compile(a_1) @ compile(a_2) @ \texttt{idiv} \\
    \end{array}
    $$

##### 7.1.3. Compiling Variables

- Motivation

  - Our arithmetic expressions can contain variables and we have not considered them yet. 

  - To fix this we will represent our variables as **local variables** of the JVM.

  - A variable can be stored and loaded with the instructions

    ```assembly
    iload index
    istore index
    ```

    i.e., places the content of the local variable `index` onto the stack, or

    storing the top of the stack into a local variable can be done by the instruction

- Extended Compile Definition for Arithmetic Expression

  - Extended **compile function** takes two arguments: the abstract syntax tree and an environment, $E$, that maps identifiers to index-numbers.
    $$
    \begin{array}{lll}
    compile(n, E) &\overset{def}= &\texttt{idc }n \\
    compile(a_1 + a_2, E) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{iadd}\\
    compile(a_1 - a_2, E) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{isub}\\
    compile(a_1 * a_2, E) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{imul}\\
    compile(a_1 / a_2, E) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{idiv}\\
    compile(a_1 \%\ a_2, E) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{irem}\\
    compile(x, E) &\overset{def}= & \texttt{iload }E(x) \\
    \end{array}
    $$
    The last line generates the code for variables where $E(x)$ stands for looking up the environment to which index the variable $x$ maps to. 

- Extended Compile Definition for Boolean Expressions

  - Extended **compile function** takes three arguments, a boolean expression, an envornment, and a label. If the result of boolean expression is true, we will jump to the label. Details can be found at 7.2.2.
    $$
    \begin{array}{lll}
    compile(a_1 != a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmpne } lab \\
    compile(a_1 == a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmpeq } lab \\
    compile(a_1 < a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmpgt } lab \\
    compile(a_1 <= a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmpge } lab \\
    compile(a_1 > a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmplt } lab \\
    compile(a_1 >= a_2, E, lab) &\overset{def}= &compile(a_1, E) @ compile(a_2, E) @ \texttt{if\_icmple } lab \\
    \end{array}
    $$
    
  - Note that commands like `if_icmpne` stands for boolean operators. 

---

### 7.2. Compiling Instructions

##### 7.2.1. Compiling Statements

- Assignment

  - For assignment statement, we return a list of instructions and an environment for the variables
    $$
    \begin{array}{lll}
    compile(\text{skip}, E) & \overset{def}= &(\text{Nil}, E) \\
    compile(x := a, E) & \overset{def}= &(compile(a, E) @ \texttt{istore } index, E(x \mapsto index)
    \end{array}
    $$
    where $index$ is $E(x)$ if it is already defined, or if it is not, then the largest index not yet seen. 

    > e.g. 
    >
    > Suppose the command `x := x + 1`, we execute the following instructions
    >
    > ```assembly
    > iload nx
    > ldc 1
    > iadd
    > istore nx
    > ```
    >
    > where `nx` is the index corresponding to the variable `x`. 

##### 7.2.2. Conditional Jumps

- Label

  - Motivation

    - Since we are generating assembly code for the JVM, we do not actually have to giev numeric addressses, but unique symbolic **labels** instead. 
    - Labels sepcify a target for a jump, which is essentially mark a location in our code. Therefore the labels need to be unique. 

  - A label $L$ is attached to assembly code like the follows

    > e.g.
    >
    > ```assembly
    > 	instr_1
    > L:
    > 	instr_2
    > 	instr_3
    > 	...
    > ```

    where the label needs to be followed by a colon. 

  - The task of assembler is to <u>resolve the labels to actual numberic addresses</u>. 

- If-else Statement

  > e.g.
  >
  > Suppose the command `if b then cs1 else cs2`, where `b` is a boolean expression and where both `cs1`, `cs2` are the statements for each of the if-branches. 
  >
  > In the true-case the control-flow of the program needs to behave as the follows,
  >
  > ```mermaid
  > graph LR
  > 	code1[code of b]
  > 	code2[code of cs1]
  > 	code3[code of cs2]
  > 	following[following code]
  > 
  > 	code1 --> code2
  > 	code2 -.-> code3
  > 	code3 -.-> following
  > 	code2 --jump--> following
  > ```
  >
  > In the false-case the control-flow of the program needs to behave as the follows,
  >
  > ```mermaid
  > graph LR
  > 	code1[code of b]
  > 	code2[code of cs1]
  > 	code3[code of cs2]
  > 	following[following code]
  > 
  > 	code1 -.-> code2
  > 	code2 -.-> code3
  > 	code3 --> following
  > 	code1 --conditional jump--> code3
  > ```

  - The definition of If-else is as follows,
    $$
    \begin{array}{ll}
    compile(\texttt{if } b \texttt{ then } cs_1 \texttt{ else } cs_2, E) \overset{def}= &~ \\
    ~ &L_{ifelse} (label) \\
    ~ &L_{ifend}(label) \\
    ~ &(is_1, E') = compile(cs_1, E) \\
    ~ &(is_2, E'') = compile(cs_2, E') \\
    ~ &compile(b, E, L_{ifelse}) \\
    ~ &@is_1 \\
    ~ &@ \texttt{goto } L_{ifend} \\
    ~ &@ L_{ifelse} :\\
    ~ &@ is_2 \\
    ~ &@ L_{ifend}:, E'')
    \end{array}
    $$

    - In the first two lines we generate two fresh labels for the jump addresses (before and after the else-branch). 
    - Then we generate the instructions for the two branches, $is_1, is_2$. 
    - The final code will be the first code for $b$, then the `goto` for after the else-branch, the label $L_{ifelse}$, folloed by instructions for else-branchm, followed by the after-the-else branch label. 

- Loop

  - While Statement

    > e.g.
    >
    > `while b do cs`
    >
    > In the true-case the control-flow of the program is as the follows,
    >
    > ```mermaid
    > graph LR
    > 	before[previous code]
    > 	code1[code of b]
    > 	code2[code of cs]
    > 	following[following code]
    > 	
    > 	before --> code1
    > 	code1 --> code2
    > 	code2 --> following
    > 	code2 --> code1
    > ```
    >
    > In the false-case the control-flow of the program is as the follows,
    >
    > ```mermaid
    > graph LR
    > 	before[previous code]
    > 	code1[code of b]
    > 	code2[code of cs]
    > 	following[following code]
    > 	
    > 	before --> code1
    > 	code1 -.-> code2
    > 	code2 -.-> following
    > 	code1 --> following
    > ```
    >
    > 

  - The definition of while statement is as follows,
    $$
    \begin{array}{ll}
    compile(\texttt{while } b \texttt{ do } cs, E) &\overset{def}= \\
     & L_{wbegin} (label) \\
     & L_{wend} (label) \\
     &(is, E') = compile(cs_1, E) \\
     &L_{begin} \\
     &@ compile(b, E, L_wend) \\
     &@ is \\
     &@ \texttt{goto } L_{wbegin} \\
     &@ L_{wend}, E') \\
    \end{array}
    $$

  - Write command

    - `write x` can be used to print out the content of a variable. 

      ```assembly
      .method public static write(I)V
      	.limit locals 1
      	.limit stack 2
      	getstatic java/lang/System/out Ljava/io/PrintStream; iload 0
      	invokevirtual java/io/PrintStream/println(I)V
      	return
      .end method
      
      iload E(x)
      invokestatic XXX/XXX/write(I)V
      
      ```

      - The first line marks the beginning of the method, called `write`. It takes a single integer argument indicated by the (I) and returns no result, indicated by the V (for void). 
      - Since the method has only one argument, we only need a sin- gle local variable (Line 2) and a stack with two cells will be sufficient (Line 3). 
      - Line 4 instructs the JVM to get the value of the member out from the class `java/lang/System`. 
      - Line 5 copies the integer we want to print out onto the stack. In the line after that we call the method println (from the class `java/io/PrintStream`). 
      - The helper-method write can be invoked with the two instructions in Line 9 and 10. 

    - main method

      ```assembly
      .class public XXX.XXX
      .super java/lang/Object
      
      ...
      
      .method public static main([Ljava/lang/String;)V
      	.limit locals 200 
      	.limit stack 200
            ...here comes the compiled code...
      	return
      .end method
      ```

---

### 7.3. Optimization

##### 7.3.1. Optimization Methods

- Motivation

  - Every well-known compiler will perform some optimization on the code. 
  - There are some aspects towards to optimize our compiler in JVM. 

- Constant Value

  - Motivation

    - In JVM, we used the instruction `ldc some_constant` to push a constant onto stack. 

    - What this instruction does is putting the constant into a *constant pool* and then uses an index into this constant pool.

      > This means `ldc` will be represented by at least two bytes in the class file. 

    - This is suitable for large constant like string, but waste for small ones like integer. 

  - Process

    - To counter this waste, the JVM has specific instructions for small integers,

      > e.g.
      >
      > `iconst_0, ..., iconst_5`
      >
      > `bipush n`
      >
      > where the `n` is `bipush` is between -128 and 128. 

    - By having dedicated instrutions such as `iconst_0` to `iconst_5` (and `iconst_m1`), we can make the generated code size smaller.

    - As these instructions only require 1 byte as opposed the generic `ldc` which needs 1 byte plus another for the index into the constant pool. 

    - This could both make the code <u>both smaller and faster</u>, because the command `bipush` uses sepcific instruction in the underlying CPU. 

    - We can use the following Scala helper-function

      ```scala
      def compile_num(i: Int) =
      	if (0 <= i && i <= 5) i"iconst_$i" else
      	if (-128 <= i && i <= 127) i"bipush $i"
      	else i"ldc $i"
      ```

      It generates the more efficient instructions when pushing a small integer constant onto the stack. The default is `ldc` for any other constants.

- Local Variable

  - Motivation
    - JVM has specific instructions for loading and storing the first three local variables. The assumption is that most operations and arguments in a method will only use very few local variables.
  - Process
    - We can use the following instructions
      - `iload_0, ..., iload_3`
      - `istore_0, ..., istore_3`
      - `aload_0, ..., aload_3`
      - `astore_0, ..., astore_3`
    - Having implemented these optimisations, the code size of the BF-Mandelbrot program reduces and also the class-file runs faster. 

- Duplicated Operations

  - Motivation
    - For the source code
      ```assembly
      x := ...;
      write x
      
      ...
      istore 0
      iload 0
      ...
      ```
      Our code first calculates the new result the for `x` on the stack, then pops off the result into a local variable, and after that loads the local variable back onto the stack for writing out a number.
    - If we can detect such situations, then we can leave the value of `x` on the stack with for example the much cheaper instruction `dup`. 
    - However, sometimes we will be able to optimise, sometimes we will not. The compiler needs to find out which situation applies. This won't be accounted here.

- Arrays Out of Boundary

  - Motivation
    - The compiler writer has a lot of freedom about how to generate code from what the programmer wrote as program.
    - Sometimes the compiler writer is expected to go an extra mile, or even miles and change the meaning of a program. 

      > e.g.
      >
      > ```scala
      > new(arr[10]);
      > arr[14] := 3 + arr[13]
      > ```
      >
      > The program generates an array of size 10, and then tries to access the non-existing element at index 13 and even updating the element with index 14. 
      >
      > This will lead to an exception thrown from JVM. 
    
    - <u>Raising an exception at run-time</u> like the JVM is the best compromise.
    
    - Alternatively, we can add additional if conditions in the generated WHILE-program, in order to guarantee safety reading and writing. 
    
  - Process
  
    ```assembly
    	index_aexp
    	aload loc_var
    	dup2
    	arraylength
    	if_icmple L1
    	pop2
    	iconst_0
    	goto L2
    L1:
    	swap
    	iaload
    L2:
    	...
    ```
  
    - Use `if_icmple L1` and `goto L2` to prevent exception. 
    - `goto_w` problem solved for too large jumps. 

##### 7.3.2. Arrays

- Motivation

  - Extend the WHILE-language to support with arrays.
  - A bigger array can be used for optimization. 

- Create Array

  - For creating a new array we can generate the following three JVM instructions:

    ```assembly
    ldc number
    newarray int
    astore loc_var
    ```

    - First we need to put the size of the array onto the stack. 
    - The next instruction creates the array. In this case the array contains `ints`. 
    - With the last instruction we can store the array as a local variable (like the ‚Äúsimple‚Äù variables from the previous section). 

- Looking up

  - For looking up an element in an array we can use the following JVM code:
    ```assembly
    aload loc_var
    index_aexp
    iaload
    ```
  - The first instruction loads the ‚Äúpointer‚Äù, or local variable, to the array onto the stack. 
  - Then we have some instructions calculating the index where we want to look up the array. The idea is that these instructions will leave a concrete number on the top of the stack, which will be the index into the array we need.
  - Finally we need to tell the JVM to load the corresponding element onto the stack. 

- Updating

  - Updating an array at an index with a value is as follows:

    ```assembly
    aload loc_var
    index_aexp
    value_aexp
    iastore
    ```

  - The first instruction loads the local variable of the array onto the stack.
  - After that come the instructions for with which value we want to update the array. 
  - The last line contains the instruction for updating the array.

- Modify Grammar 

  - Next we need to modify our grammar rules for our WHILE-language
  
  - Extend the rule for factors in arithmetic expressions with a rule for looking up an array.
    $$
    F ::= (E) | \underbrace{Id[E]}_{new} | Id | Num
    $$
    There are two new rules for statements, one for creating an array and one for array assignment:
    $$
    \begin{array}{lll}
    Stmt &::= & ... \\
     &| &\texttt{new}(Id[Num]) \\
     &| &Id[E] := E \\
    \end{array}
    $$
    
    
