## WEEK I - Introduction to Pattern Recognition

>[üè† MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>‚èπÔ∏è
>
>[‚û°Ô∏è WEEK II - Discriminant Functions](year3/7ccsmpnn/w2.md)
>
>Outlines:
>
>1. Motivation
>2. Principles

### 1.1. Motivation for Pattern Recognition

##### 1.1.1. Pattern and Pattern Recognition

- Pattern Recognition
  - Aim is to build algorithms that can <u>recognize useful patterns in data</u>, extract useful information from data, or make decisions based on data.
- Pattern
  - Targets including the data between completely deterministic and completely random data.
  - Ingredients required for pattern recognition:
    - A pattern exists
    - We can not pin it down mathematically
    - Data exists
- Related Disciplines
  
  > AI
  >
  > > ML
  > >
  > > > PR
  > > >
  > > > > NN
  > > > >
  > > > > > DL
  
  - Machine Learning (ML)
    - Broader: also covers the sort of reasoning about data considered in artificial intelligence (AI).
  - (Artificial) Neural Networks (NN)
    - Narrower: a sub-set of techniques used for pattern recognition. 
    - Deep Learning (DL) is a sub-discipline of neural networks.
  - Statistics
    - Overlapping: a set of techniques used for pattern recognition (and other things).
- Applications
  - Disciplines in which pattern recognition is applied to specific types of data:
    - Signal Processing: 1D signals
    - Computer Vision: 2D images / 3D videos
    - Data Mining: Numeric, categorical and linguistic data

##### 1.1.2. Applications of Pattern Recognition

- Retail

  - Detailed logs can be be kept of individual purchasing habits (through online sales, loyalty cards).
  - Identifying patterns in the data can allow:
    - Offers/marketing to be targeted at specific customers (Customer Relationship Management).
    - Making recommendations. 

- Finicial

  - Financial Forecasting using previous financial data (such as price fluctuations) try to find recurring patterns that can be used to inform investment decisions.

    > e.g.
    >
    > The discovery that sharp increases in the price of gold tends to be preceded by long periods of price stability might be the basis for an investment rule.

  - Detecting fraudulent credit-card transactions

    - Keep records of transactions, merchants, and account holders.
    - Record which previous transactions were fraudulent, and which were not.
    - Find patterns in this data associated with fraudulent transactions.
    - Look for similar patterns in new transactions to identify possible fraudulent use.

  - Credit scoring

    - For processing insurance applications, credit card applications, etc.
    - Keep records of previous customers (e.g. # of accidents, make of car, year of model, income, # of dependents, mortgage amount, etc.).
    - Record which previous customers were good/bad.
    - Find patterns in this data associated with good/bad customers.
    - Look for similar patterns in new applications to predict if new customer will be good or bad (i.e. to inform decision about offering insurance or credit).

- The Internet

  - Spam Filters
    - Keep lots of old emails, identified as both spam and non-spam.
    - Find patterns (words used, attachments, etc.) that distinguish spam from non-spam.
    - Identify patterns in new emails to filter spam.

- Medical

  - Finding sequences in DNA
  - Identifying Proteins from Amino acids
  - Relating these to disorders/illnesses
  - ...

- Speech Recoginition

  - Find patterns in audio data to allow phonemes or words to be recognized. 

- Computer Vision

  - Find patterns in images that let us identify objects or letters.

- Manufacturing

  - Industrial inspection: Quality control of items being produced.
  - Condition Monitoring: Analyse sensor data to predict need for maintenance, identify faults.


##### 1.1.3. An Example of Designing a Pattern Recognition System

- Sorting Fish Task

  - Motivation: Industrial inspection: Sort fish on a conveyor according to species.
  - Image inputs with binary labels: *Sea bass*, or *Salmon*. 

- Preprocessing

  - Set up a camera to take images.

  - Isolate fish from one another and from the background (segmentation).

  - Extract **features** (from each isolated fish)

    > e.g.
    >
    > Length,
    >
    > Lightness,
    >
    > Width,
    >
    > Number fins, 
    >
    > etc.

  - Pass Features to a classifier.

- Classification

  - Suppose we choose the length of the fish as a possible feature for discrimination:

    > sea bass are typically longer than salmon

  - A simple way of classifying fish might be to see whether the length of the fish is above or below a critical value, say $l^*$

    - i.e., decide fish is sea bass if length is greater than $l^*$.
    - we call this our **model**. 
    - The model:
      - has a **parameter**(s) the value of which we need to define,
      - this will define a **decision boundary** (the point at which we decide between one category and another). 

  - Determine the value of $l^*$

    - Obtain many measurements of fish length (by taking images of fish, preprocessing them, and performing feature extraction).
    - Determine the class of each fish (by asking an expert).
    - Use this data to choose $l^*$
    - For a simple problem like this one, we can do this by looking at the histogram of lengths. 

- Improving Performance

  - Feature Selection

    - *Length* (alone) would appear to be a poor feature for correct classification, So try a different feature: *Lightness*. 

  - Data

    - To improve performance further, we might collect more **data**. 

      > i.e. increase the number of points in the feature space. 

    - This would give a better estimate of the optimal decision boundary.

    - However, collecting more data may be costly (in terms of time and effort).

    - There will always be some limit to the amount of data we have with which to design our classifier.

    - We might supplement the data with other knowledge of the task:

      > e.g.
      >
      > We might know that in winter Sea Bass are more frequently caught, while in summer Salmon are more common.

  - Decision Boundary

    - To improve performance further, we might use a more **complex decision boundary**.

      > i.e. increase the number of parameters in the model

- Generalisation

  - The aim is to design a classifier to correctly classify novel inputs

    > i.e. to generalise to new data

  - An overly complex decision boundary is unlikely to have good performance on new data.

  - It is **over-fitted** to the training data.

  - There exists a Bias/Variance Trade-off

- Summary

  - A classifier consists of a model and the parameters of that model
  - The classifier takes feature values as input
  - Some features are more discriminating than others
  - Multiple features can be used for classification
  - Training data can be used to refine the model and optimise the parameters
  - The classifier might be optimised to reduce the number of errors or the cost
  - Simple models generalise better than overly complex models

---

### 1.2. General Principles

> The Classifier Design Cycle
>
> ```mermaid
> graph LR
> 
> D[Data Collection] --input data--> F[Feature Selection]
> F --feature vectors--> M[Model Selection]
> M --> P[Parameter Tuning]
> P --> E[Evaluation]
> ```

##### 1.2.1. Data

- Design Considerations

  - What data to collect?

    > What data is it possible to obtain?
    >
    > What data will be useful?

  - How much data to collect?

  - Data Sources

    - One or more transducers (e.g. camera, microphone, etc.) extract information from the physical world.
    - Data my also come from other sources (e.g. databases, webpages, surveys, documents, etc.).

- Data Cleansing

  - Data may need to be preprocessed or cleaned (e.g. segmentation, noise reduction, outlier removal, dealing with missing values).

  - This provides the *input data*.

  - Input data can be any set of values.

    > e.g.
    >
    > - Age and salary of customers.
    > - Number of iPads sold per week.
    > - A set of images of objects.
    > - Recordings of speech.
    > - EEG recordings.
    > - Medical records.

- Data is classified as:

  - **discrete** (integer/symbolic valued) or 
    **continuous** (real/continuous valued), and
  - **univariate** (containing one variable) or 
    **multivariate** (containing multiple variable).

##### 1.2.2. Features

- Definition
  - A feature can be any aspect, quality, or characteristic of the data.
  - Features may be discrete (e.g., labels such as ‚Äúblue‚Äù, ‚Äùlarge‚Äù) or continuous (e.g., numeric values representing height, lightness).
  - Any number of features, $d$, can be used.
  - The selected features form a **feature vector**. 
- Feature Vectors
  - Terminologies
    - Each **datapoint/exemplar/sample** is represented by the chosen set of features.
    - The combination of d features is a d-dimensional column vector called a **feature vector**.
    - The $d$-dimensional space defined by the feature vector is called the **feature space**.
    - A collection of datapoints/feature vectors is called a **dataset**.
    - Datapoints can be represented as points in feature space; the result is a scatter plot.
  - How datapoints are distributed in feature space will influence how well the classifier works.
    - Ideally, the chosen features will help discriminate between classes. 
  - Certain types of distribution have names:
    - Linearly separability
    - Non-linear separability
    - Multi-modal
    - etc.
- Design Considerations
  - Ideally we would select features of the input data that:
    - preserve within-class similarity
    - discriminate between classes

##### 1.2.3. Models

- Models

  - A **model** is the method used to perform the classification 

- Design Considerations

  - We need to choose a proper sort of model, as different models will give different results.

  - However, the only way to tell which model will work best is to try them.

  - The model has **parameters** which need to be defined.

    - We need to use the data to define the parameters: There are many different algorithms for training classifiers 

      > e.g. gradient descent, genetic algorithms

    - Parameters that used for the training algorthm, i.e., **hyper-parameters**. 

- Tuning the parameters of the model

  - Need to find parameter values that give best performance (e.g. minimum classification error)
  - For simple task could exhaustively try all values.
  - For more complex tasks exhaustive search becomes impractical.

##### 1.2.4. Training

- Definition

  - The procedure used to define the parameters of the classifier is called **training**, 
    or parameter tuning, or optimisation, or learning. 
  - This is where the term machine learning comes from.
  - Machine learning is analogous to, but simpler than, learning in biology.

- Learning

  - Learning is acquiring and improving performance through experience.

  - Pretty much all animals with a central nervous system are capable of learning (even the simplest ones).

  - Motivation

    - We want computers to learn when it is too difficult or too expensive to program them directly to perform a task.
    - Get the computer to program itself by showing examples of inputs and outputs.
    - We will write a *parameterized* program, and let the learning algorithm find the set of parameters that best approximates the desired function or behaviour.

  - Types of Learning

    - **Supervised learning**

      - There is a desired output defined for each feature vector in the training set. 

        > i.e., the dataset contains $\{x, \omega\}$ pair.

      - Kinds of Supervised learning

        - **Regression**
          - Outputs are continuous variables (real numbers).
          - Allows us to perform interpolation.
        - **Classification**
          - Outputs are discrete variables (category labels).
          - Aim is to assign feature vectors to classes.

        - **Weakly-Supervised Learning**
          - Supervised learning with inexact (or inaccurate) labels, what constitutes inexact label depends on task
          - As inexact label cheaper to produce.

    - **Unsupervised learning**

      - The desired output is undefined for each feature vector in the training set.

        > i.e., date consists of $\{x\}$ only. 

      - Kinds of Supervised learning

        - Clustering
          - Discover clumps or natural groupings of points in the feature space
        - Embedding
          - Discover a low-dimensional manifold or surface near which the data lives.
        - Factorisation
          - Discover a set of components that can be combined together to reconstruct the samples in the data.

    - Others

      > e.g.
      >
      > Semi-supervised learning, Reinforcement learning, Transfer learning, etc.

- Process for Supervised Learning

  >  e.g.
  >
  > We assume that there is an unknown target function, $f$, that will map feature vectors, $x$, to class labels, $\omega$, i.e.,
  > $$
  > \omega_j = f(x_j)
  > $$
  > We have a dataset consisting of feature vector and class label pairs, i.e., $\{x_1, \omega_1\}, \{x_2, \omega_2\}, ..., \{x_n, \omega_n\}$. 
  >
  > We use a learning algorithm to find a function $g$, that approximates $f$, i.e., $g \approx f$.
  >
  > The range of functions that can be learnt is constrained by the choice of model, this range is the hypothesis set (simpler models have a smaller hypothesis set, or are more biased).
  >
  > > Learning g allows us to predict the class label of new instances, i.e.,
  > > $$
  > > \omega^*_{n+1} = g(x_{n+1})
  > > $$
  > > with given a dataset and a learning model, find an unknown target function $g: X \to \Omega$ to predict the class of new feature vectors, $\{x_{n+1}\}, \{x_{n+2}\}, ..., \{x_{n+m}\}$

- Hyper-parameters

  - The learning algorithm will also have parameters:
    - More practical algorithms may have many parameters.
    - These are called hyper-parameters (to distinguish them from the parameters being learnt by the classifier).
    - The choice of hyper-parameters may have a large influence on the performance of the trained classifier.
    - Choosing hyper-parameters:
      - exhaustive search (called **grid search**)
      - learning (called **meta-learning**)

##### 1.2.5. Evaluation

- Design Considerations

  - There are many different criteria that can be used.

    > e.g. error rate, cost, f1-score, etc.

  - We use *performance* as a general term. 

- Avoid **overfitting**. 

  - Usually achieved by testing performance using a separate set of data that has not been used for training.
  - The **test data**, or **validation data**.
    - Separate training and testing/validation datasets can be created by splitting a single dataset into (randomly selected) parts.
    - Evaluation results my vary depending on how the dataset is split.
    - A better estimate of performance can be obtained by using multiple (say, $k$) splits and averaging results (called $k$-fold cross-validation).
  - Overfitting vs. Underfitting
    - Underfitting: Test error ‚âà Training error
    - Overfitting: Test error >> Training error

- Confusion matrix

  - To explore where the prediction errors come from, we can create a **confusion matrix**. 

  - If there are only two classes, then classifier is called a **dichotomizer**.

  - Confusion matrix for a dichotomizer can be defined as follows

    | **True\Predict** | **Positive** | **Negative** |
    | ---------------- | ------------ | ------------ |
    | **True**         | TP           | FN           |
    | **False**        | FP           | TN           |

  - Some common metrices

    $\text{error rate} = \frac{FP + FN}{TP + TN + FP + FN}$
    $\text{accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = 1 - \text{error rate}$
    $\text{recall} = \frac{TP}{TP + FN}$
    $\text{precision} = \frac{TP}{TP + FP}$
    $f_1 \text{ score} = \frac{2 \times \text{recall}\times \text{precision}}{\text{recall + precision}}$

- Deployment

  - When using or testing a classifier, we will merge module selection and parameter tuning into classifier training process. 

