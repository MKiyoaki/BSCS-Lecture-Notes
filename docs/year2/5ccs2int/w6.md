## [WEEK VI - Unsupervised Learning](https://keats.kcl.ac.uk/pluginfile.php/11003605/mod_resource/content/1/5CCS2INT_Week6_with_solutions.pdf)

>[🏠 MENU - 5CCS2INT](year2/5ccs2int.md)
>
>[⬅️ WEEK V - Introduction to Machine Learning](year2/5ccs2int/w5.md)
>
>[➡️ WEEK VII - Supervised Learning](year2/5ccs2int/w7.md)
>
>Outlines:
>
>12. Unsupervised Learning
>13. K-Means Clustering Algorithm
>14. Hierachical Clustering Algorithm

### 12. Introduction to Unsupervised Learning

##### 12.1. Definition

- Unsupervised learning

  - Look for patterns in the data without having the labels of such data points.

    >e.g., if we have pictures of cats and dogs, an unsupervised algorithm may be able to identify two groups of similar pictures in the dataset, without of course being able to 'name' them.

  - A very common technique to do that is **clustering**. 

- Clustering Problems

  - Considering that there are $k$ data points on a 2D-grid and some of them into $K$​​ clusters. We want to decide the belongingness of some specific data points. 

    > e.g., 
    >
    > Assume there is a graph that $k=6, K=2$, we have that $C_1=\{p_1, p_2, p_3\}, C_2=\{p_5, p_6\}$ and we want to know whether should $k_4 \in C_1$ or $k_4 \in C_2$. 

  - There is a key point to address: <u>what does it mean to be a better cluster assignment than another?</u> 

---

### 13. K-Means Clustering Algorithm

##### 13.1. Introduction and Motivation

- Idea

  - Considering the description of clustering problems above. 
  - In order to get the optimal assignment, we want to minimise the value of <u>the matric for the within-cluster variance</u> $V(C_k)$​​. In the example of 2D graph, we use squared Euclidean distance as a measure for within-cluster variation. 
    - For a given cluster $C_k$ made of points $p_i$​, the within-cluster variation is calculated by

      $$
      V(C_k)=\frac{1}{|C_k|}\sum\limits_{i, i' \in C_k}d(p_i, p_{i'})^2 \\
      $$
      $\text{ where } d(p_i, p_{i'})^2=\sum\limits_{j=1}^D(p_{ij}-p_{i'j})^2$
    
    - Note that we don't need to count each pair $i, i'$ twice. 
    
    > e.g.
    >
    > For the previous example, if $\hat{C_2} = \{p_4, p_5, p_6\}$, we can get that
    > $V(\hat{C_2}) = \frac{1}{|C_2|}\sum_{i, i' \in C_2}d(p_i, p_{i'})^2 = \frac{1}{3}(d(p_4, p_5)^2 + d(p_4, p_6)^2 + d(p_5, p_6)^2)$

##### 13.2. The Algorithm

- Let $p_1, ..., p_n \in \mathbb{R}^D$ be a set of points in $D$ dimensions. Let $K$ be the desired number of clusters to be found. Then K-means clustering algorithm does the following:

  1. Select $k$ different data points to be initial cluster centre at the time $t=0, \mu_k^{(0)}$(Clusters at time $t$ are called $C_1^{(t)}, ..., C_K^{(t)}$). There are no clusters yet at time $t=0$. Move time to $t = 1$ then.
  2. At time $t$, assign each data point $p_i$ to the cluster $C_k^{(t)}$ with closest cluster centre $\mu_k^{(t-1)}$. Break ties assigning the point to the cluster centre with the lowest index among the closest ones.
  3. For each cluster, recalculate their cluster centre as the average of all points. 
     i.e., For each $k \in 1,...,K$, $\mu_k^{(t)}=\frac{1}{|C_k^{(t)}|}\sum\limits_{i \in C_k^{(t)}}p_i$
  4. If clusters changed, go back to step 2. moving time forward to $t:=t+1$​. If cluster have not changed, stop and return the final clusters.

> e.g.
>
> Consider the set of points as below,
>
> $P=\begin{bmatrix}2 &5 \\ 2 &6 \\ 6 &4 \\ 1 &8 \\ 4 &5 \\ 4 &4 \\ 1 &1 \\ 4 &7 \end{bmatrix}$
>
> > Including the data points $p_1=(2, 5), p_2=(2, 6), ...$
>
> Assume there are 3 clusters ($K=3$), and the initial cluster centres (centroids) as below,
> $\mu_1^{(0)} = p_1 = (2,5)$
> $\mu_2^{(0)} = p_2 = (2,6)$
> $\mu_3^{(0)} = p_3 = (6,4)$
>
> - *Check the notes to refer the graph of iteration process.* 
> - The entire k-means clustering of these $8$ data points in $K = 3$ clusters. Crosses represent cluster centres at that time, whereas colours represent clusters. 
> - Note that, in a real implementation, iteration number 5 would not even start. That is because at the end of iteration 4 we would identify that clusters have not changed from the end of iteration 3, and therefore terminate the algorithm. 

---

### 14. Hierachical Clustering Algorithm

##### 14.1. Introduction and Motivation

- Idea

  - Considering the description of clustering problems above. 
  - Instead of finding $K$ clusters, we are interested in creating a process in which the number of clusters progressively changes as steps go by. We can then stop when we reach the desired number of clusters. 

- Process

  > 1. there are $K = n$ clusters, where $n$ is the total number of data points. That is, each point is its own cluster. 
  > 2. At each time-step, we select the closest pair of clusters and merge them into one.
  > 3. If all points are in one cluster, stop. Otherwise, go back to step 2. 

##### 14.2. Define the distance

- Manhattan Distance Metric - $L_1 \text{ Norm}$​

  - Let $p_1, p_2 \in \mathbb{R}^D$ be a pair of points in $D$ dimensions. We used a second index to refer to their coordinates (e.g., $p_1=p_{11}, p_{12},..., p_{1D}$). 
  - We then define the Manhattan distance or $L_1 \text{ Norm}$ as below,

    $$
    ||p_1 - p_2||_1 := \sum\limits_{i=1}^D |p_{1i} - p_{2i}|
    $$
    $\text{where } |x| \text{ is the usual modulus of real numbers.}$

  > e.g.
  >
  > The $L_1 \text{ Norm}$ between $p_1=(3, 6, 1, -1), p_2=(-3, 6, 2, 5)$ is that
  > $||p_1 - p_2||_1 = |p_{11}-p_{21}| + |p_{12}-p_{22}| + |p_{13}-p_{23}| + |p_{14}-p_{24}|$
  > $\implies ||p_1 - p_2||_1 = |3-(-3)| + |6 - 6| + |1 - 2| + |(-1) - 5| $
  > $\implies ||p_1 - p_2||_1 = 6+0+1+6=13$

- Euclidean Distance Metric -$L_2 \text{ Norm}$

  - Let $p_1, p_2 \in \mathbb{R}^D$ be a pair of points in $D$ dimensions. We used a second index to refer to their coordinates (e.g., $p_1=p_{11}, p_{12},..., p_{1D}$). 
  - We then define the Euclidean Distance or $L_2 \text{ Norm}$​ as below,

    $$
    ||p_1 - p_2||_2 := ((\sum\limits_{i=1}^D |p_{1i} - p_{2i}|)^2)^{\frac{1}{2}}
    $$

- Process

  > e.g.
  >
  > Consider the set of points as below,
  > $P=\begin{bmatrix}-10 &8 \\ 7 &-6 \\ 8 &-10 \\ -6 &-4 \\ -8 &6 \\ 2 &-4 \end{bmatrix}$
  >
  > With using the Manhattan distance matric the table at $t=0$ will be like
  >
  > |         | **{1}** | **{2}** | **{3}** | **{4}** | **{5}** |
  > | ------- | ------- | ------- | ------- | ------- | ------- |
  > | **{2}** | 31      |         |         |         |         |
  > | **{3}** | 36      | 5       |         |         |         |
  > | **{4}** | 16      | 15      | 20      |         |         |
  > | **{5}** | 4       | 27      | 32      | 12      |         |
  > | **{6}** | 24      | 7       | 12      | 8       | 20      |
  >
  > > Since the shortest distance is {1} and {5}, which is 4, so we merge them together. 
  >
  > The table at $t=1$ will be like
  >
  > |         | **{1, 5}** | **{2}** | **{3}** | **{4}** |
  > | ------- | ---------- | ------- | ------- | ------- |
  > | **{2}** | 27         |         |         |         |
  > | **{3}** | 32         | 5       |         |         |
  > | **{4}** | 12         | 15      | 20      |         |
  > | **{6}** | 20         | 7       | 12      | 8       |

##### 14.3. Cluster Distance Criteria

- Single-Linkage Criterion

  - Let $C_1$ and $C_2$ be two clusters and $||·||$ a norm (e.g., $L_1$ or $L_2$). Then, the single-linkage criterion for distance between $C_1$ and $C_2$ is given by

    $$
    dist_{SL}(C_1, C_2) := \min\limits_{p \in C_1, q \in C_2} ||p - q||
    $$
    
    > i.e., It is the **shortest distance** between points from one cluster to the other.

- Complete-Linkage Criterion

  - Let $C_1$ and $C_2$ be two clusters and $||·||$ a norm (e.g., $L_1$ or $L_2$). Then, the single-linkage criterion for distance between $C_1$ and $C_2$​​ is given by

    $$
    dist_{CL}(C_1, C_2) := \max\limits_{p \in C_1, q \in C_2} ||p - q||
    $$
    
    > i.e., It is the **longest distance** between points from one cluster to the other.

- Average-Linkage Criterion

  - Let $C_1$ and $C_2$ be two clusters and $||·||$ a norm (e.g., $L_1$ or $L_2$). Then, the single-linkage criterion for distance between $C_1$ and $C_2$​ is given by

    $$
    dist_{AL}(C_1, C_2) :=  \frac{1}{mn}\sum\limits_{i=1}^n \sum\limits_{j=1}^m||p_i - q_j||
    $$
    
    > i.e., It is the **average distance** between points from one cluster to the other. 
    >
    > Note that $mn$ is the number of pairs of points between clusters.


##### 14.4. The Algorithm

- Let $p_1, ... , p_n ∈ \mathbb{R}^D$ be a set of points in $D$ dimensions. Consider a metric for distance between points (such as Manhattan or Euclidean). Clusters at time $t$ are called $C^{(t)}_1,C^{(t)}_2,...,$. 

- Since the number of clusters will reduce throughout the algorithm via merges, we consider that the lower index at time $t$ is used when naming the merged cluster at time $t + 1$​. Then, Hierarchical clustering algorithm does the following:
  1. Create $n$ clusters, one for each data point $p_i$​​. At time $t = 0$, clusters are therefore $C^{(t)}_1 = \{p_1\},...,C^{(t)}_n = \{p_n\}$. Move time to $t = 1$ then.
  2. At time $t$, merge clusters that have the smallest distance between each other, according to a given criterion of distance. We evaluate, for every distinct pair of clusters $C_i,C_j$:
     $$
     threshold(t) := \min\limits_{C_i^{(t)}, C_j^{(t)}} dist(C_i^{(t)}, C_j^{(t)})
     $$
     Where $dist(C_i^{(t)}, C_j^{(t)})$ can be given by the single, complete, or the average linkage criterion. 
     - If only one, take the pair of clusters distant $threshold(t)$ and merge them into one by $C_i^{(t+1)} = C_i^{(t)} \cup C_j^{(t)}$​
     - If two or more pairs with no cluster overlap are also distant $threshold(t)$, merge them too according to the same principle. 
     - If three or more clusters are on a 'chain' of $threshold(t)$ distance, merge all three of more in the same cluster, like using $C_i^{(t+1)} = C_i^{(t)} \cup C_j^{(t)} \cup C_k^{(t)}$
  3. Stop if there is only one cluster containing all points. Otherwise, go back to step 2. moving time forward to $t := t + 1$.
