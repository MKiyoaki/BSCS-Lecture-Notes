## WEEK V - Clustering

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK IX - Ensemble Methods](year3/7ccsmpnn/w9.md)
>
>[➡️ REVISION](year3/7ccsmpnn/re.md)
>
>Outlines:
>
>1. Unsupervised Learning
>2. Clustering Algorithms
>     - K-Means Clustering
>     - Fuzzy K-Means Clustering
>     - Iterative Optimization
>     - Hierarchical Clustering

### 10.1. Unsupervised Learning

##### 10.1.1. Introduction to Unsupervised Learning Scheme

- Motivation
  - This is appropriate for large "data mining" applications where the contents of a large database are not known beforehand.
  - This is also appropriate in many applications when the characteristics of the patterns can change slowly with time.
  - Improved performance can be often achieved if classifiers running in an unsupervised mode are used.
  - We can use unsupervised methods to identify features that will then be useful for categorisation.
  - We gain some insight into the nature (or structure) of the data

##### 10.1.2. Clustering

- Definition

  - A **cluster** is a group of samples of similar properties.
  - Clustering is to put samples into one of the $K$ clusters. The samples from the same cluster are similar to each other but dissimilar to samples in other clusters.

- Purpose

  - *Clustering for understanding:* 

    - By putting similar objects together, it allows us to understand the common characteristics of the objects. 

      > e.g.
      >
      > For example, Retrieve relevant information from the Internet; Analyse the market by grouping similar types of customers; Extract useful data from a larger amount genetic information.

  - *Clustering for utility:* 

    - Use the extracted information in different areas. For example, summarisation, compression, finding the nearest neigbours.

- Types

  - **Partitional clustering:** It divides the samples into clusters such that each sample is in one cluster.
  - **Hierarchical clustering:** It divides the samples into a nested clusters organised as a tree. Each cluster is the union of its sub-clusters.
  - **Agglomerative approach**: It treats each sample as a single cluster in the beginning and then groups them into clusters in a bottom-up approach.
  - **Divisive approach**: It treats the whole group of samples as a single cluster and then break them down into small clusters in a top-down approach.

---

### 10.2. Clustering Algorithms

##### 10.2.1. K-Means Clustering

- Definition

  - Problem: Given $c$ clusters/classes $(K=c)$ and $n$ samples, $\textbf{x}_i, i \in \{1, 2,..., n\}$, assign each sample to one of the  $c$ clusters.
  - Goal: Find the $c$ mean vectors, $\textbf{m}_1, ..., \textbf{m}_c$.
  - Criterion: Minimise the distance of each sample $\textbf{x}_i$ to each $\textbf{m}_j,  j \in \{ 1, 2, ..., c \}$.
  - Use <u>Euclidean distance</u> as metric and cost function $\parallel \textbf{x}_i - \textbf{m}_j \parallel_2$
  - Group sets of samples according to the closet mean.
  - An iterative process is employed to adjust $\textbf{m}_j$ until classes do not change.

- Procedure

  ```
  Algorithm: K-means clustering
  begin initialise n, c (K = c), m1, m2, ..., mc
  	do classify n samples according to the nearest mj, j ∈ {1,2,...,c};
  		recompute mj
  	until no change in mj
  	return m1, m2, ..., mc
  end
  ```

- Example

  > e.g.
  >
  > ...

- Evaluation

  - General Measures of Similarity

    - Defined a cost function as the sum of square Euclidean distances of each cluster from its mean $\textbf{m}_j$.
      $$
      J_e = \sum^c_{j=1} \sum_{\textbf{x} \in \mathcal{D}_i} \parallel \textbf{x} - \textbf{m}_j \parallel^2
      $$

    - This criterion defines clusters as their mean vectors $\textbf{m}_j$ in the sense that it minimizes the sum of the squared lengths of the error $\textbf{x} - \textbf{m}_j$.

    - The optimal partition is defined as one that minimizes $J_e$, also called *minimum variance* partition.

    - Works fine when clusters form well separated compact clouds, less when there are great differences in the number of samples in different clusters.

  - Properties

    - Clustering results are sensitive to initial locations of the centres
    - It works fine when clusters form well separated, compact, clouds
    - Fast
    - Simple to implement
    - Variations on standard $k$-means use distance metrics other than Euclidean distance
    - **K-means clustering** is also known as **C-means clustering**

##### 10.2.2. Fuzzy K-Means Clustering

- Definition

  - Same as K-means clustering, except

    - A sample can have graded, or fuzzy membership of a cluster.

      - Membership of sample $\textbf{x}_j$ in cluster $i$ is defined by $\mu_{ij}$.
      - $\mu_{ij} \in [0, 1], \sum^c_{i=1} \mu_{ij} = 1$.

    - Cluster centres: 
      $$
      \textbf{m}_i = \frac{\sum^n_{j=1} \mu_{ij}^b \textbf{x}_j}{\sum^n_{j=1} \mu^b_{ij}}
      $$
      where $b > 1$.

    - A sample can therefore partially belong to multiple clusters.

      - Not a partitional clustering method.

    - **Fuzzy K-means clustering** also known as **fuzzy C-means clustering**

  - Goal: find the cluster centres $\textbf{m}_1, \textbf{m}_2, ... , \textbf{m}_c$.

    - Each cluster centre is the the mean of all the samples weighted by the membership value $\mu_{ij}$ (or called membership grade)

    - An iterative algorithm adjusts the cluster centres, and the membership values, to minimise the weighted sum of square Euclidean distances between the centres and the samples.
      $$
      J_e = \sum^c_{i=1} \sum^n_{j=1} \mu_{ij}^b \parallel \textbf{x}_j - \textbf{m}_i \parallel
      $$
      where $b \geq 1$ (called *fuzzifier*).

    - Small $b$ causes samples to be members of one/few clusters.

    - Large $b$ causes samples to be members of many clusters

- Example

  > e.g.
  >
  > ...

##### 10.2.3. Iterative Optimization

- Motivation
  - Once a criterion function has been selected, clustering becomes a problem of **discrete optimisation**.
  - As the sample set is finite there is a finite number of possible partitions, and the optimal one can be always found by **exhaustive search**.
  - Most frequently, an iterative optimisation procedure is adopted to select the optimal partitions.
  - The basic idea lies in starting from a reasonable initial partition and "move" samples from one cluster to another trying to minimise the criterion function.
  - In general, these kinds of approaches guarantee **local**, not global, optimisation.
- Definition
  - Let us consider an iterative procedure to minimise the sum-of-squared-error criterion $J_e$,
    $$
    J_e = \sum_{j=1}^c J_j
    $$
    where $J_j = \sum_{\textbf{x} \in \mathcal{D}_j} \parallel \textbf{x} - \textbf{m}_j \parallel^2$ is the effective error per cluster.

  - It can be proved that if a sample $\hat{\textbf{x}}$ currently in cluster $\mathcal{D}_i$ is tentatively moved into $\mathcal{D}_j$, the change of the errors in the 2 clusters is
    $$
    J^*_j = J_j + \frac{n_j}{n_j + 1} \parallel \hat{\textbf{x}} - \textbf{m}_j \parallel^2 \\
    J^*_i = J_i - \frac{n_i}{n_i + 1} \parallel \hat{\textbf{x}} - \textbf{m}_i \parallel^2
    $$
    where $n_i, n_j$ denote the number of data points in clusters $i, j$, respectively.

  - Right move: 
    $$
    \frac{n_i}{n_i - 1} \parallel \hat{\textbf{x}} - \textbf{m}_i \parallel^2 > \frac{n_j}{n_j + 1} \parallel \hat{\textbf{x}} - \textbf{m}_j \parallel^2
    $$

  - After moving, update $\textbf{m}_i, \textbf{m}_j$. 

- Procedure

  ```
  ...
  ```

- Iterative Clustering

  - This procedure is a **sequential version** of the K-means algorithm, with the difference that K-means waits until *n* samples have been reclassified before updating, whereas the latter updates each time a sample is reclassified.
  - This procedure is more prone to be trapped in local minima, and depends from the order of presentation of the samples, but it is online!
  - Starting point is always a problem.

##### 10.2.4. Hierarchical Clustering

- Motivation
  - The partitional clustering methods have only formed disjoint clusters.
  - Hierarchical clustering methods split clusters into subclusters; sub-clusters into sub-subclusters and so on.
  - Hierarchical information can be extracted from the dataset.
- Agglomerative Hierarchical Clustering
  - Given $n$ samples and each sample represents a cluster, merge clusters one by one in each lever until $c$ clusters are left.
    1. First, each cluster contains exactly one sample (first level);
    2. then partition into $n-1$ clusters by merging the two nearest clusters into one (second level);
    3. the next partition into $n-2$ clusters by merging the two nearest clusters into one (third level); ... until $c$ clusters are achieved.
  - Distance measurement (to measure inter-cluster similarity)
    - **Single-link clustering**: distance between clusters is the **minimum distance** between constituent samples. 
    - **Complete-link clustering**: distance between clusters is the **maximum distance** between constituent samples.
    - **Group-average clustering**: distance between clusters is **average distance** between all pairs of samples.
    - Centroid clustering: distance between clusters is the **distance between the average of the feature vectors** in each cluster. 
- Evaluation
  - Results will vary depending on the method used to calculate cluster similarity.

