## WEEK IX - Clustering

>[🏠 MENU - 6CCS3AIN](year3/6ccs3ain.md)
>
>[⬅️ WEEK VIII - Agent Communications Languages](year3/6ccs3ain/w8.md)
>
>[➡️ WEEK X - Ethics and AI](year3/6ccs3ain/w10.md)
>
>Outlines:
>
>1. Introduction to Clustering
>1. K-Means, K-Median Algorithm
>1. 

### 9.1. Introduction to Clustering

##### 9.1.1. Clustering Problem and Algorithm

- Definition

  - The goal is to classify different data points, where
    - Points in the same cluster are close and 
    - Points of different clusters are far

- Application

  > Biological Networks
  >
  > Recommendation Systems
  >
  > ...

- Typical Algorithms Including
  - k-Means
  - k-Median
  - Agglomerative Clustering
    - Single-Linkage
    - Average-Linkage
    - Complete-Linkage
  - Divisive Clustering
    - Sparsest-Cut

---

### 9.2. K-Means Clustering

##### 9.2.1. Definition

- Senarios

  - Assume we have $n$ datapoints $x_1, x_2, ..., x_n$,

  - We want to partition them into $k$ sets $S_1, S_2, ... S_k$ such that the cost of the partition, $c(S_1, S_2, ... S_k)$, is minimized:
    $$
    c(S_1, S_2, ... S_k) = \sum_{i=1}^n \min_{j \in [k]} \{\parallel  x_i - \mu_j \parallel ^2_2\}
    $$
    where $\mu_i$ is the mid-point of each cluster, i.e., $\mu_i = \frac{1}{|S_i|} \sum_{j \in S_i} x_j$, and
    $\parallel  \cdot \parallel ^2_2$ is the squared L2 norm (squarede Euclidean distance).

  - Note that $x_i$ and $\mu_i$ are vectors for $i \in [n]$. 

  > e.g.
  >
  > If $S_1 = \{x_1, x_2\}, x_1 = \begin{bmatrix}1 \\2 \end{bmatrix}, x_2 = \begin{bmatrix}3 \\4 \end{bmatrix}$, then we can compute that $\mu_1 = \begin{bmatrix}2 \\ 3\end{bmatrix}$. 

- K-Means Clustering Algorithms

  1. Select $k$ cluster centres **arbitrarily (randomly)**.
  2. Repeat until convergence:
     2.1. AssignmentStep
      	2.1.1. Assign each point to the cluster with the nearest mean
      	2.1.2. $S_i = \{x_j | \parallel x_j - \mu_i \parallel^2_2 \leq \parallel x_i - \mu_l \parallel^2_2 \}$, for all $l \in [k]$ where each point is assigned to exactly one cluster $S_i$. 
     2.2. UpdateStep:
      	2.2.1. Recalculate the mean point of the cluster
      	2.2.2. $\mu_i = \frac{1}{|S_i|} \sum_{j \in S_i} x_j$

##### 9.2.2. Example

##### 9.2.3. K-Means++

- Motivation
  - Traditional K-Means Clustering may fall into sub-optimal situation. Once it happens it cannot be changed.
  - This can be made arbitrarily bad (by increasing the width of the rectangle).
  - Using K-Means++ can solve with it, where the approximation factor is at most $O(\log k)$. 
  
- K-Means++ Algorithms
  
  1. Set the first centre to be one of the input points **chosen uniformly at random**, i.e., $\mu_1 = uniform(x_1, x_2, ..., x_n)$. 
  2. For cluster $i = 2 \to n$:
     2.1. For each point $x_j$ compute the distance to the nearest centre, i.e., calculate $d_j = \min_j \{\parallel x_j - \mu_l \parallel_2^2\}$. 
  
     2.2. Open a new centre at a point using the weighted probability distribution that is proportional to $d_j^2$. That is,
     $$
     \mathbb{P}(\text{new centre in }x_j) = \frac{d^2_j}{\sum_l d^2_l}
     $$
     
  3. Continue with *K-Means*. 
  
  > i.e., K-Means++ will execute a pre-execution method to select a better initial cluster centre. 
  >
  > In this method, we will start from a randomly selected point for each cluster, and caluclate the distance to each point, and select a better point according to the weighted distribution, which is related to the calculated distance. 

##### 9.2.4. K-Median Clustering

- Motivation

  - Assume we have $n$ points $x_1,x_2,...,x_n$. 

  - We want to partition them into $k$ sets $S_1, S_2, ... S_k$ such that the cost of the partition, $c(S_1, S_2, ... S_k)$, is minimized:
    $$
    c(S_1, S_2, ..., S_k) = \sum^n_{i=1} \min_{j \in [k]} \{\parallel x_i - \mu_j \parallel_1 \}
    $$
    where $\mu_i$ is the mid-point of each cluster, i.e., $\mu_i = \text{median}(x_j | j \in S_i)$,

    where the median over a set of vectors is defined as the vector of medians per dimension. 

  > e.g.
  >
  > $\text{median}(\begin{bmatrix}1 \\ 1\end{bmatrix}, \begin{bmatrix}2 \\ 3\end{bmatrix}, \begin{bmatrix}2 \\ 0\end{bmatrix}) = \begin{bmatrix}2 \\ 1\end{bmatrix}$
  >
  > Note that the resulting vector does NOT need to be an existing data point (see k-medoid).

- K-Median vs. K-Means

  [![img](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/260px-Manhattan_distance.svg.png)](https://en.wikipedia.org/wiki/File:Manhattan_distance.svg)

  - k-Median minimized the following equation, 
    $$
    c(S_1, S_2, ..., S_k) = \sum^n_{i=1} \min_{j \in [k]} \{\parallel x_i - \mu_j \parallel_1 \}
    $$
    where $\mu_i = \text{median}(x_j | j \in S_i)$, 

    - i.e., K-Medians minimizes the **Manhattan** distance. 

  - k-Means minimized the following equation:
    $$
    c(S_1, S_2, ... S_k) = \sum_{i=1}^n \min_{j \in [k]} \{\parallel  x_i - \mu_j \parallel ^2_2\}
    $$
    where $\mu_i = \frac{1}{|S_i|} \sum_{j \in S_i} x_j$. 

    - i.e., K-Means minimises the **Euclidean/geometric** distance. 

---

### 9.3. DBSCAN

##### 9.3.1. Introduction to DBSCAN

- Motivation

  <img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F071b3ee2-5df1-4900-8539-a55d2ee18d8e_3221x2180.png" alt="img" style="zoom:67%;" />

  - DBSCAN with parameters $m$ and $\epsilon$, where $m$ indicates the minimal number of neighbours for a core point, and $\epsilon$ indicates a radius for a point to identify neighbours. 
  - There are three kinds of points:
    - Core point, which has at least $m$ points as neighbours.
    - Border point, which is identified as a neighbour of some core points, 
    - Noise point, neither core nor border points. 

- Process

  - Find the points in the $\epsilon$-neighbourhood of every point, and identify the **core points** with more than $m$ neighbours. 
  - Find the connected components of core points on the neighbor graph, ignoring all non-core points. 
    - i.e., If two core points are within their $\epsilon$-neighbourhood scope, merge them together into the same cluster. 
  - Assign each non-core point to a nearby cluster if the cluster is an ε-neighbor, otherwise assign it to noise.

---

### 9.4. Hierachical Clustering

##### 9.4.1. Introduction to Hierachical Clustering

- Motivation
  - In K-means clustering, it is hard to choose the expected number of cluster $k$ for some senarios. The more clusters the lower the cost/the better score, but we don't want every point as a cluster. 
  - One way of strike a good tradeoff is to use the *elbow method*. 
    - A natural definition: Pick the $k \in \{ 2, 3, ... , n - 1\}$ that maximizes $cost_{k-1} / cost_k$ where $cost_j$ is the cost of the clustering with $j$ clusters. 
    - $k=1$ and $k=n$ are ruled out as they result in trivial clusters. 
  - Flat Clustering Problem
    - So far we tried to assign the points to $k$ clusters. We haven's assumed any structure among the clusters. 
    - In Flat Clustering we may lose the structure of clusters, for example, in recomendation systems, we cannot get a tree as result. 
- Hierachical Clustering
  - Recursive partitioning of data at increasingly finer granularity represented as a tree. 
  - The leaves of the hierarchical cluster tree represent data.

##### 9.4.2. Hierachical Clustering Algorithms

- Types of Hierachical Clustering in Pratical

  - *Agglomerative* clustering (Bottom to Top)
    - Initially place each datapoint in its own clusters, 
    - Repeatedly merge most similar clusters. 
  - *Divisive* clustering (Top to Bottom)
    - Split using bisection $k$-means (or sparsest cut)
    - Recurse on each part. 

- Agglomerative Clustering

  - Consider the following senarios

    ```mermaid
    flowchart LR
    	a -- 2 --- b
    	a --4 --- c
    	b -- 3 --- c
    	c -- 2 --- d
    	b -- 1 --- d
    	c -- 5 --- e
    	
    	d -- 1 --- f((f))
    	e -- 1 --- g((g))
    	f -- 1 --- i((i))
    	g -- 1 --- h((h))
    	f -- 4 --- g
    	h -- 5 --- i
    	
    ```

  - An edge can mean similarity or dissimilarity.

    - A high similarity value means that the corresponding two nodes are very similar and should be in the same cluster. 

  - Each node starts in a separate cluster

  - The similarity between two clusters $C_1 = \{a, b\}, C_2 = \{c, d\}$ is 

    ```mermaid
    flowchart LR
    	a -- 1 --- d
    	b -- 3 ---- d
    	b -- 5 ---- c
    	a -- 2 --- c
    ```

    - Single Linkage: $\max \{ad, ac, bd, bc\} = 5$
    - Average Linkage: $\frac{\sum \{ab, ac, bd, bc\}}{4} = 2.75$
    - Complete Linkage: $\min \{ad, ac, bd, bc\} = 1$

  - Agenda
    - In summary, the algorithm will start from considering each datapoint as a cluster, then merge the cluster with the smallest distance (according to the linkage formats). 
    - More details can be found at 5CCS2INT. 

- Dendogram

  - The obtained merges can be represented in a *dendogram*. 

  > e.g.
  >
  > [![What is a Dendrogram?](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-1cc73d4/www.displayr.com/wp-content/uploads/2018/03/What-is-a-Dendrogram.png)](https://app.displayr.com/Try/Template Hierarchical Cluster Analysis?_gl=1*knqggp*_gcl_au*MTE2MDM5MzU5OS4xNzM1NjY2NDM3*_ga*MjQwNzc0ODE0LjE3MzU2NjY0Mzg.*_ga_TN4203VTVW*MTczNTY2NjQzNy4xLjAuMTczNTY2NjQzNy42MC4wLjA.)

- Divisive Heuristics

  - Motivation

    - Find a partition of the input similarity graph. 

    > e.g.
    >
    > Spilit the above graph into two parts: $\{a, b, c, d\}$ and $\{e, f, g, h, i\}$

  - Methods

    - Split using *bisection $k$-means*. i.e., Using traditional K-means clustering to get 2 clusters. or, 

    - Split using *sparsest cut*. 

      - Given a graph with nodes $V$ and edges in $E \subset V \times V$, the sparsest cut of a cut $\phi(S)$ is given by
        $$
        \phi(S) = \frac{E(S, \bar{S})}{\min(|S|, |\bar{S}|)}
        $$
        where $E(S, \bar{S})$ is the **sum of weights of edges** crossing the cut. Here $\bar{S}=V/S$.

        > i.e., $\phi(S)$ is like the weight value after normalization.

      - The sparsest cut of a graph is given by the $S^*$ that minimizes $\phi(S^*)$, i.e., 
        $$
        \phi(S^*) = \min_S \phi(S)
        $$

        > We want to minimize this value, since
        >
        > - We want to make sure the split crosses edges as less as possible (i.e., $E(S, \bar{S})$ should be as small as possible), and
        > - The splited two parts can be as samilair/ balance as possible (i.e., $\min(|S|, |\bar{S}|)$ should be as large as possible). 

  - Recurse on each part. 

  - Builds cluster-tree top-down