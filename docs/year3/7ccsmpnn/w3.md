## WEEK III - Introduction to Neural Networks

>[ðŸ  MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[â¬…ï¸ WEEK II - Discriminant Functions](year3/7ccsmpnn/w2.md)
>
>[âž¡ï¸ WEEK IV - Multilayer Perceptrons & Backpropagation](year3/7ccsmpnn/w4.md)
>
>Outlines:
>
>1. 

### 3.1. Introduction to Artificial Neural Networks

##### 3.1.1. Biological Neural Networks

- Definition

  - Brain can be seen as

    - A highly complex, non-linear and parallel information processing system.
    - It is composed of neurons lots of neurons. 

  - Neuron is a type of cell, consist of a number of different parts:

    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/2560px-Neuron.svg.png" alt="undefined" style="zoom:50%;" />

    - A cell body (or *nucleus* or *soma*)
    - Many *dendrites*
    - An *axon*
    - Many *synapses*

  - *Synapses* are the connections between the axon of one neuron and the dendrites of another neuron.

    - Each synapse is also a small, complex, biological machine.
    - Activity in the the pre-synaptic neuron causes the release of neurotransmitters from synaptic vesicles.
    - These neurotransmitters diffuse across the gap to receptors on the post-synaptic neuron and cause activity there.

  - Information Flow Through a Neuron

    - The dendrites transmit input signals from the synapses to the soma.

- Action Potential

  - Definition

    - There is a threshold for spiking.
    - Reaching *spiking threshold* requires either:
      - repetitive stimulation of the same synapses (temporal summation)
      - simultaneous stimulation of a large number of synapses (spatial summation)
      - or (most typically) a combination of both

  - Synaptic influences on spiking

    - The arrival of an impulse at a synapse may have the opposite effect.

      > i.e., it may render the post-synaptic neuron less activated by other stimuli (inhibition).

    - There are thus two types of synapse:

      1. Excitatory: tend to cause spiking in the postsynaptic neuron
      2. Inhibitory: tend to prevent spiking in the postsynaptic neuron

    - Different synapses may have stronger or weaker affects on the postsynaptic neuron

      - synapses have different strengths (or *weights*)
      - synapses may change their strength

  - Synaptic plasticity

    - Plasticity permits the nervous system to adapt to its environment: learning, memory, new computations
    - Two mechanisms:
      1. creation of new synaptic connections between neurons
      2. modification of existing synapses

- Summary

  - A network of neurons interconnected through synapses
  - Each neuron makes about 103 to 104 connections, hence, networks are very large.
  - Neurons communicate via synapses.
  - Neurons perform computations on their synaptic inputs and if certain conditions are met (the potential exceeds a threshold) they produce an output.
  - Neurons can adapt, which will change the behaviour of the network.

##### 3.1.2. Artificial Neural Networks

- Definition

  - A network of simple *processing units* which communicate by sending signals to each other over *weighted connections*.

    > processing units: analogous to neurons
    >
    > weighted connections: analogous to synapses

    - A hugely simplified model of a biological neural network.

  - An ANN is a *parallel* computational system consisting of many simple processing elements connected together in a specific way in order to perform a particular task.

    - Also known as parallel distributed processing (PDP), or connectionist models.

- Motivation

  - Biological Aspect

    - To build models of brain function by simulation:
      - to understand human intelligence.
      - to understand brain dysfunction.
    - To mimic certain cognitive capabilities of human or animals.

  - Pratical Aspect

    - ANNs are extremely powerful computational devices

    - Any *continuous functions* from input to output can be implemented in a three-layer ANN

      - Universal function approximators

        > i.e., $y = f(\textbf{x})$ for any function $f$ can be implemented by ANNs.

      - Universial classifier

        > classification can be defined sa trying to approximate $\omega = f(\textbf{x})$

- Pratical Issues

  - In practice, it is problematic to find the neural network architecture (and set its parameters) to solve a particular problem.
    - Lots of variables
      - Number of units
      - Weights connections

##### 3.1.3. Components of ANNs

- Layers

  - Definition
    - Typically, processing units are arranged into *populations* or *layers*.
    - Layers can be:
      - **visible**: receive inputs from, or send outputs to, the external environment
      - **hidden**: only receive inputs and send output to other processing units
    - Visible layers can be sub-divided into:
      - **input** layers
        - receive signals from the environment
        - for classification, this is the feature vector which is to be classified
      - **output** layers
        - which send signals to the environment
        - for classification, this is the predicted class label associated with the feature vector
        - typically, *one-hot* encoded

  - Input layers 

    - Typically are linear, as they do NOT perform any processing.

      - Hence, they are often ignored. 

      > In which case the network below would be described as 2-layer, rather than 3-layer

      - Some people count the layers of connections (not layers of neurons).

- Function

  - Definition
    - Each unit receives a vector of inputs, $\textbf{x}$ (from other units or the external environment).
    - Each input is associated with a weight, $w$, which determines the strength of influence of each input.
    - Each $w$ can be either +ve (*excitatory*) or -ve (*inhibitory*).
  - Typically all processing units, in each layer, perform the same computation.
  - Computation is typically simple.
  - Output
    - Output, $y$, is computed as simple function of $\textbf{x}$ and $\textbf{w}$.
    - This **response function** often split into two component parts:
      - A **transfer function** that determines how the inputs are integrated.
      - An **activation function** that determines the output the neuron produces. 

- Weights

  - Definition

    - Setting weights explicitly using prior knowledge.

    - Optimising connectivity to achieve some objective 

      > e.g. using a genetic algorithm, or gradient decent.

    - Training the network by feeding it training data and allowing it to adapt the connection weights.

  - Training can be:

    - supervised	

      > e.g. Delta Learning Rule

    - or unsupervised

      > e.g. Hebbian Learning Rule

- A generic artificial neural network

  - An *environment* in which the system operates (inputs to the network, outputs from the network).
  - A set of processing *units* (neurons).
  - A set of *weighted connections* between units, $w_{ji}$, which determines the influence of unit $i$ on unit $j$.
  - A *transfer function* that determines how the inputs to a unit are integrated.
  - An *activation function* that determines the output the neuron produces.
  - An activation state, $y_j$, for every unit (output).
  - A method for setting/changing the connection weights.

---

### 3.2. Types of Artificial Neural Networks

##### 3.2.1. Perceptrons (Linear Threshold Units)

- Definition

  - Linear Threshold Unit, or *Perceptrons*, includes only one node. 

  - $\textbf{x}$ and $\theta$ definea hyperplane that divides the input space into two parts.

    - This hyperplane is called the *decision boundary*.

  - A single linear threshold unit is equivalent to a single linear discriminant function

    - A linear discriminant function is a linear combination of feature values:
      $$
      g(\textbf{x}) = \textbf{w}^t\textbf{x} + w_0
      $$
      that divides feature-space using a linear decision boundary.

    - A single preceptron can therefore act as a dichotomizer:
      $$
      \begin{cases}
      \textbf{x} \leftarrow \omega_1 & \text{if } g(\textbf{x})>0 &\text{where } g(\textbf{x}) = \textbf{a}^t\textbf{y}, \textbf{y} = \begin{bmatrix}1 &\textbf{x}^t\end{bmatrix}^t \\
      \textbf{x} \leftarrow \omega_1 & \text{if } y=1 &\text{where } y=H(\textbf{w} \cdot \textbf{x})
      \end{cases}
      $$

      > Note the unfortunate clash in standard notation. 

- Equivalence to Linear Discriminant Function

  - A linear threshold unit can implement the same function as a generalised linear discriminant function:
    $$
    y = w_0 + w_1 f_1(\textbf{x}) ... + w_N f_N(\textbf{x})
    $$
    where $f_i(\textbf{x})$ for $i \in \{1, ..., N\}$ can be any scalar function of $\textbf{x}$.

    - $f_i(\textbf{x})$ can be implemented by another neuron.
    - Both the expansion of the feature-space and the discrimination can be performed by different layers of neurons in a multilayer neural network. 

- Delta Learning Algorithm

  - Learning

    - Weights for a linear threshold unit can be learnt using the same methods used to set the parameters of a linear discriminant function:
      - Perceptron Learning
      - Minimum Squared Error Learning (Widrow-Hoff)
      - Delta Learning Rule

  - Properties

    - Supervised.

    - Adjust weights in proportion to the difference between the desired output $t$, and the actual output $y$.

    - **Sequential** (online) update:
      $$
      \textbf{w} \leftarrow \textbf{w} + \eta (t-y) \textbf{x}^t
      $$
    
    - **Batch** update:
      $$
      \textbf{w} \leftarrow \textbf{w} + \eta \sum_p (t_p-y_p) \textbf{x}^t_p
      $$
    
    - Similar to Widrow-Hoff learning rule (except arbitrary margin, $b$, replaced by desired output, $t$).
    
    - Similar to Perceptron learning rule (as learning only occurs for misclassified exemplars, i.e. when $y \neq t$).
    
  - Two types of updates
  
    - Sequential update
  
      - False negative ($t = 1, y=0$)
        $$
        \textbf{w} \leftarrow \textbf{w} + \eta \textbf{x}^t
        $$
    
        - Make $\textbf{w}$ more like $x$. 
    
      - False positive ($t = 0, y = 1$)
        $$
        \textbf{w} \leftarrow \textbf{w} - \eta \textbf{x}^t
        $$
    
        - Make $\textbf{w}$ less like $x$. 
    
    -  Similar to (Multiclass) Perceptron learning rule.
    
  - Implemented using *Gradient Descent*.
  
    > e.g.
    >
    > For sequential delta learning algorithm:
    >
    > 1. Set value of hyper-parameter $Î·$
    >
    > 2. Initialise $\textbf{w}$ to arbitrary solution
    >
    > 3. For each sample, $(\textbf{x}_k, t_k)$ in the dataset in turn:
    >
    >    - update weights that
    >      $$
    >      \textbf{w} \leftarrow \textbf{w} + \eta (t_k - H(\textbf{w} \cdot \textbf{x}_k)) \textbf{x}^t_k
    >      $$
    >
    > 4. Run through all samples in turn as many times as necessary for convergence (i.e. until no changes in $\textbf{w}$).
    
    - If dataset is not linearly separable, learning will not converge. 

##### 3.2.2. Competitive Learning Networks

- Hebbian Learning Rule

  - Properties

    - Unsupervised.

    - Strengthen the connection between an active neuron and any active inputs.

    - Sequential (online) update is: 
      $$
      \textbf{w} \leftarrow \textbf{w} + \eta y \textbf{x}^t
      $$

- Definition

  - Output units compete for the right to respond to the input.
    - Competition means that activity of some neurons is suppressed by other neurons.
    - Competition can be implemented using inhibitory lateral weights between the output neurons.
    - Alternatively, competition can be implemented by a *selection process* that chooses the *winning* neuron(s).
      - Using inhibitory lateral weights requires outputs to be determined iteratively.
      - Using a selection process is simpler and more stable.

- Selection Methods

  - Simplest *selection process* is **winner-takes-all (WTA)**.

    - The neuron with the largest response is the winner (and remains active). All other neurons have their response set to zero.
    - WTA is an *activation function*. 
    - WTA can be used with Hebbian learning to perform clustering.

  - An alternative selection process is **k-winners-take-all (kWTA)**.

    - The $k$ neurons with the largest response remain active ($k$ is a hyperparameter). All other neurons have their response set to zero. 
    - kWTA is an activation function.

  - An alternative selection process is **softmax**.

    - The activation of each neuron is calculated using:
      $$
      y_j \leftarrow \frac{e^{\beta y_j}}{\sum_k e^{\beta y_k}}
      $$

      - $Î²$ is a hyperparameter.
      - The outputs of all neurons sum to 1 (like a probability distribution)

    - softmax is an activation function

    - Popular as activation function used in last layer of networks applied to classification tasks

##### 3.2.3. Negative Feedback Networks

- Definition

  - Another way to implement competition is to use **inhibitory feedback** connections between the output neurons and the input neurons.
    - Output neurons compete to receive inputs, rather than compete to produce output.
    - Using inhibitory feedback weights requires outputs to be determined iteratively.

- Process

  - Activation

    - Initialise $\textbf{y}$ to zero.
    - perform several iterations of:
      - Update input units: $\textbf{e} = \textbf{x} - \textbf{w}^T \textbf{y}$
      - Update output units: $\textbf{y} \leftarrow \textbf{y} + \alpha \textbf{w} \cdot \textbf{y}$
    - Tries to minimise response from input units $\textbf{e}$ (i.e. minimise reconstruction error).
    - Tries to find $\textbf{y}$ values that accurately reconstruct the input (in terms of minimising the reconstruction error).

  - Learning
    $$
    \textbf{w} \leftarrow \textbf{w} + \beta \textbf{y} \cdot \textbf{e}^T
    $$
    where $\alpha, \beta$ are non-negative hyperparameters. 

    - Adjust weights to accurately reconstruct the input (in terms of minimising the reconstruction error)

  - i.e., 

    - $\textbf{w}^T \textbf{y}$ is a reconstruction of the input,
    - $\textbf{e}$ is the error between the input and the reconstruction.

##### 3.2.4. Autoencoder Networks

- Definition

  - By representing the reconstruction using a separate neural population $\textbf{r}$, nd remove the inhibitory feedback connections. Neural responses no longer to be calculated iteratively. 

    - Can still learn weights so as to minimise the error between the input and the reconstruction.
    - The is called an **autoencoder**.

  - A three (two) layer neural network. Aim is for the output to reconstruct the input.

    - Hidden units impose an *information bottleneck*. 
      - limit the number of hidden nodes.
      - limit the number of hidden nodes that are simultaneously active (sparsity constraint).
    - Hidden units learn a useful representation of the data.

  - Typically, weights between hidden layer and output layer equal corresponding weights between input layer and hidden layer.

    > i.e. decoding weights = transpose of encoding weights

- Error

  - Weights are defined to minimize reconstruction error.

  - Error can be measured using a number of metrics.

    > e.g. Euclidean distance

  - Trained using **Backpropagation** of error.

- Training Process

  - To avoid overfitting, *de-noising* autoencoders, add noise to inputs used for learning.
    - Encoding performed with corrupted input $\widetilde{x}$, but decoding compared to original input $x$. 
  - *Stacked autoencoders* form the basis for some Deep Learning Architectures
    - Stacked Restricted Boltzmann Machines (RBMs: a particular type of autoencoder) are called Deep Belief Networks.
    - Deep architectures produce state-of-the-art performance on many pattern recognition tasks.
    - Deep architectures are multilayer neural networks.

