## WEEK II - Probabilities

>[🏠 MENU - 6CCS3AIN](year3/6ccs3ain.md)
>
>[⬅️ WEEK I - Introduction to AI](year3/6ccs3ain/w1.md)
>
>[➡️ WEEK III - Bayesian Networks](year3/6ccs3ain/w3.md)
>
>Outlines:
>
>1. Uncertainty
>
>   - Examples
>   - Handling
>
>2. Classical Probability
>
>   - Basic Definitions
>     - Conditional Probability $P(a|b) = \frac{P(a \wedge b)}{P(b)}$
>   - Joint Probability
>   - Conditional Independence
>     iff $P(A, B) = P(A)P(B)$
>
>3. Bayes Rules
>
>   $P(A|B) = \frac{P(B|A) P(A)}{P(B|A) P(A) + P(B| \neg A) P(\neg A)}$

### 2.1. Uncertainty

##### 2.1.1. Examples of Uncertainty

- Real world environment - Partial observability

##### 2.2.2. Handling Uncertainty

- Probability
  - Probabilistic assertions summarize effects of
    - Laziness - failure to enumerate exceptions, quanlifactions, etc. 
    - Ignorance - lack of relevant facts, initial conditions, etc. 
  - Probabilistic assumptions still have issues - Computational complexity, obtaining values, semantics, etc. 
  - **Subjective** or **Bayesian** probability
    - Probabilities relate propositions to one's own state of knowledge. 
- Utility theory - represent and infer preferences
- Decision theory - utility theory + probability theory

---

### 2.2. Classical Probability

##### 2.2.1. Probability Basics

- *Sample space* - A set $\Omega$

- *Sample point* or *Atomic event* - $\omega \in \Omega$

- *Probability space* or *Probability model* - A sample space with an assignment $P(\omega)$ for every $\omega \in \Omega$ such that
  $$
  0 \leq P(\omega) \leq 1 \\ \sum_\omega P(\omega) = 1
  $$

  > e.g.
  >
  > For a typical die, 
  >
  > $P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = \frac{1}{6}$

- *Event* - An event $A$ is any subset of $\Omega$, i.e.
  $$
  P(A) = \sum_{\{\omega \in A\}} P(\omega)
  $$

  > e.g.
  >
  > For a typical die, 
  >
  > $P(\text{die roll}<4) = P(1) + P(2) + P(3) = \frac{1}{2}$

- *Random variable* - A function from sample points to some range. 

  > e.g.
  >
  > $\text{raining(London)} \in \{\text{True, False}\}$

- *Probability distribution* for any random variable $X$ - $P$, i.e.
  $$
  P(X=x_i) = \sum_{\{\omega: X(\omega) = x_i\}} P(\omega)
  $$

  > e.g.
  >
  > For a typical die, 
  >
  > $P(\text{Odd = True}) = P(1) + P(3) + P(5) = \frac{1}{2}$
  
- *Propositions* - The terms we describe the world in mathematical langauges. 

  > e.g.
  >
  > Given the boolean random variables $A, B$: 
  >
  > > event $a$ = set of sample $\omega$ points where $A(\omega) = \text{True}$
  > >
  > > event $\neg a$ = set of sample points $\omega$ where $A(\omega) = \text{False}$
  > >
  > > event $a \wedge b$ = points $\omega$ where $A(\omega) = \text{True}$ and $B(\omega) = \text{True}$ 
  >
  > Assume we have the set of sample points $\Omega = \{1, 2, 3, 4, 5, 6\}$. 
  >
  > $A(\omega) = \text{True}$, or simply $a$ is the event that the number is odd, i.e., $\{1, 3, 5\}$. 
  >
  > $B(\omega) = \text{True}$, or simply $b$ is the event that the number is $< 4$, i.e., $\{1, 2, 3\}$. 
  >
  > $a \wedge b$ is given by the sample points in $\{1, 3\}$. 

  - Types
    
    - *Prior* or *Unconditional probabilities* of Propositions
      - The probability correspond to belief before (prior) to arrival of any new evidence. 
    - *Posterior* or *Conditional probabilities* of Propositions
      - Vice versa. 
  - Syntax
    
    > We allow arbitrary combination of logical operators (AND, OR, NOT) and comparison operators (>, <, ...)
    
    - **Propositional** or **Boolean** random variables
    
      > e.g. 
      >
      > $\text{Cavity = True}$ or $\text{cavity}$
      >
      > $\text{Cavity = False}$ or $\neg \text{cavity}$
    - **Discrete** random variables (finite or infinite)
    
      > e.g. $\text{Weather = \{sunny, rainy, cloudy, snowy\}}$
    - **Continuous** random variables (bounded or unbounded)
    
      > e.g. $\text{Weather = 21.6}$

- *Conditional Probabilities*

  - The probability for $a$ under the condition of $b$: $P(a|b) = \frac{P(a \wedge b)}{P(b)}$

- *Probability distribution* - Gives values for all possible assignments 

  - Values must be exhaustive (everything covered) and mutually exclusive (no overlap)
  - Values have to sums to 1

  > e.g.
  >
  > $\textbf{P}(\text{Weather}) = \begin{pmatrix}0.72 \\ 0.1 \\ 0.08 \\ 0.1 \end{pmatrix}$


##### 2.2.2. Joint probability

- Joint Probability Distribution

  - Joint probability distribution for a set of r.v.s (random variables) gives the probability of every atomic event on those r.v.s (i.e., every sample point)
  - Every question about a domain can be answered by the joint distribution because every event is a sum of sample points.

  > e.g.
  >
  > $P(\text{Cavity, Weather})$ is a 2 * 4 matrix of values.
  > $$
  > \begin{pmatrix}
  > 0.144 &0.02 &0.016 &0.02 \\
  > 0.576 &0.08 &0.064 &0.08
  > \end{pmatrix}
  > $$
  > Which can be interpreted as 
  >
  > |                       | $W=\text{sunny}$ | $W=\text{rain}$ | $W=\text{cloudy}$ | $W=\text{snow}$ |
  > | --------------------- | ---------------- | --------------- | ----------------- | --------------- |
  > | $\text{Cavity=True}$  | 0.144            | 0.02            | 0.016             | 0.02            |
  > | $\text{Cavity=False}$ | 0.576            | 0.08            | 0.064             | 0.08            |
  >
  > $P(\text{Cavity} | \neg\text{cloudy}) = 0.02+0.144+0.02 = 0.184$

- Inference by enumeration

  - For any proposition $\phi$, sum the atomic events where it is true
    $$
    P(\phi) = \sum\limits_{\omega : \omega \models \phi} P(\omega)
    $$

  - Notation

    - Definition of conditional probability
      $$
      P(a|b) = \begin{matrix} \frac{P(a \wedge b)}{P(b)} &\text{if } P(b) > 0 \end{matrix}
      $$
      Product rule gives an alternative formulation
      $$
      P(a \wedge b) = P(a|b)P(b) = P(b|a)P(a)
      $$

- Chain rule

  - Chain rule is derived by successive application of product rule. e.g.
    $$
    \begin{matrix}
    P(a, b, c) & = & P(a,b)P(c|b, a) \\
    ~ & = & P(a)P(b|a)P(c|b, a)
    \end{matrix}
    $$
    In general, 
    $$
    \begin{array}{lll}
    P(X_1, ..., X_n) & = & P(X_1, ..., X_{n-1})P(X_n|X_1, ..., X_{n-1}) \\
    ~ & = & P(X_1, ..., X_{n-2})P(X_{n-1}|X_1, ..., X_{n-2})P(X_n|X_1, ..., X_{n-1}) \\
    ~ & = & ... \\
    ~ & = & \prod\limits^n_{i = 1}P(X_i|X_1, ..., X_{i-1})
    \end{array}
    $$

- Normalization

  - Let $X$​ be all the variables. Typically, we want the <u>posterior joint distribution</u> of the **query variables** $Y$, given

    specific values $e$ for the **evidence variables** $E$. 

    Let the hidden variables be $H = X - Y - E$

  - Then the required summation of joint entries is done by **summing out** the hidden variables
    $$
    \begin{matrix}
    P(Y|E = e) & = & \alpha P(Y, E = e) \\
    ~ & = & \alpha \sum\limits_nP(Y,E = e, H = h) \\
    \end{matrix}
    $$
    
  - The terms in the summation are joint entries because $Y$, $E$, and $H$ together exhaust the set of random variables. 
  
  > e.g.
  >
  > We can use $\alpha = \frac{1}{P(\text{toothache})}$ to normalize. 
  >
  > $P(\text{Cavity|toothache}) = \alpha P(\text{Cavity, toothache}) = ...$

##### 2.2.3. Conditional independent probability

- Motivation

  - Using joint distribution table has a high computational complexity. 
  - To get efficient probabilistic computations, we need two things: Conditional independence, Bayes Rules. 

- Definition

  - $A, B$ are independent if and only if
    $$
    P(A, B) = P(A)P(B)
    $$

    > e.g.
    >
    > $P(\text{Toothache, Catch, Cavity, Weather}) = P(\text{Toothache, Catch, Cavity}) \odot P(\text{Weather})$
    >
    > Naive approach to store with all the values: $2 \cdot 2 \cdot 2 \cdot 4 - 1 = 31$
    >
    > > $-1$ is because the sum of all the probabilities is 1
    >
    > Independence-based approach: $(2^3 - 1) + 3 = 10$
    >
    > > $3$ is the number of options for $\text{Weather}$

- Conditional Independence

  - $a$ is conditionally independent of $b$ by $c$, i.e.,
    $$
    P(a|b, c) = P(a|c)
    $$

  - Equivalent statements
    $$
    P(a|b, c) = P(a|c) \\
    P(a, b|c) = P(a|c) \odot P(b|c)
    $$

  - Properties

    - In most cases, the use of conditional independence reduces the size of the representation of the joint distribution from exponential in $n$ to (close to) linear in $n$.
    - Conditional independence is our most basic and robust form of knowledge about uncertain environments.
    - Can often make conditional independence statements when little else is known.


---

### 2.3. Bayes' Rule

##### 2.3.1. Bayes' Rule

- Deduction

  1. Probability that some **hypothesis** $h$ is true, given that some **event** $e$ has occured.
     $$
       P(h|e) = \frac{P(e|h)P(h)}{P(e)}
     $$
     
  2. Now, since 
     $$
     P(e) = P(e|h)P(h) + P(e|\neg h)P(\neg h)
     $$
     We can rewrite this in a form in which it is commonly applied
     $$
     P(h|e) = \frac{P(e|h)P(h)}{P(e|h)P(h) + P(e| \neg h)(1 - P(h))}
     $$
     where variables are binary valued.
  
- **Bayes rule** & knowledge representation

  - Useful for assessing diagnostic probability from causal probability:
    $$
    \begin{matrix}
    P(\text{Cause|Effect}) &= &\frac{P(\text{Effect|Cause})P(\text{Cause})}{P(\text{Effect})} &\iff \\
    P(\text{Cause, Effect}) &= &P(\text{Effect|Cause})P(\text{Cause})
    \end{matrix}
    $$
    
    This can also be represented by a graph, i.e., 
    ```mermaid
    graph LR
    
    Cause(Cause) --> Effect(Effect)
    ```
    
    > e.g.
    >
    > Leave out. 


##### 2.3.2. Summary

- Probability is a rigorous formalism for uncertain knowledge

- Joint probability distribution specifies probability of every atomic event

- Queries can be answered by summing over atomic events

- For nontrivial domains, we must find a way to reduce the joint size

- Independence and conditional independence provide the tools for efficient computation along with Bayes’ rule.

  

​	
