## WEEK VII - Linear Programming

>[🏠 MENU - 5CCS2FC2](year2/5ccs2fc2.md)
>
>[⬅️ WEEK VI - Approximation and the Travelling Salesman Problem](year2/5ccs2fc2/w6.md)
>
>[➡️ WEEK VIII - Linear Programming](year2/5ccs2fc2/w8.md)
>
>Outlines
>
>- 
>
>

#### 24. Linear Programming

###### ***Theorem 24.1.*** Linear Programming **LP**

**Linear Objective Function** - The quantity to be maximised / minimised,

**Linear Constraints** - Set of linear inequalities restricting the ‘feasible’ solutions.

> e.g. 
>
> Maximise: $2x+3y$
>
> Subject to: $3x+2y \leq 15 \implies 2y-x \leq 5 \implies x+2y \leq 7$

> there may be a unique solution, infinitely many solutions or no solution

**Matrix Notation** - Linear constraints can be represented more succinctly as a matrix-vector equation $A\vec{x}=\vec{b}$

> e.g.
> $$
> \begin{bmatrix} 3 & 2 \\ -1 & 2 \\ 1 & 2\end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 15 \\ 5 \\ 7 \end{bmatrix}
> $$

**Input)** 

A set of linear constraints $A\vec{x} \leq \vec{b}$, 

A linear objective function $f:\mathbb{R}^n \to \mathbb{R}$

**Output)** A solution $x \in (\mathbb{R}^{\geq0})^n$ such that 

1. $\vec{x}$ statisfies the constraints $A\vec{x} \leq \vec{b}$, and
2. $f(\vec{x})$ is maximal. 

###### ***Theorem 24.2.*** Feasible Region

The **Feasible Region** is the set of all possible solutions satisfying the linear constraints, which could be denoted as $R = \{\vec{x} \in \mathbb{R}^n:\vec{x} \text{ is a solution}\}$

The (set of) **optimal solution(s)**, for a given objective function $f:\mathbb{R}^n \to \mathbb{R}$ is $\{\vec{x} \in R: \forall \vec{y} \in R (f(\vec{x}) \geq f(\vec{y}))\} \subseteq R$

***Example 24.1.*** Balancing Energy Demands

>|               | Natural Gas | Coal | Solar | Wind | Nuclear |
>| :-----------: | :---------: | :--: | :---: | :--: | :-----: |
>| CO2 Emissions |     500     | 1000 |   0   |  0   |    0    |
>|     Cost      |     40      |  40  |  22   |  22  |   120   |
>|   Capacity    |     32      |  8   | 13.5  |  24  |   10    |
>
>Steps:
>
>1. For each mode of energy production introduce a new numerical variable $G,C,S,W,N$ to denote the **energy production (kWh)**. 
>
>2. We must ensure that each mode does not exceed its **capacity**, i.e.,
>
>   $G \leq 32,\ C \leq 8,\ S \leq 13.5,\ W \leq 24,\ N \leq 10$
>
>3. The total energy production must meet the **energy demands**, i.e., 
>
>   $G+C+S+W+N \geq \text{total energy demands}$
>
>4. We also don't want to exceed target **CO2 Emissions**, i.e.,
>
>   $0.5G + 1C \leq \text{target CO2 emissions}$
>
>5. Finally, we want to minimise the **total cost**, i.e., 
>
>   $\text{Minimize } 40G+80C+22S+22W+120N$

---

#### 25. The Simplex Method

###### ***Theorem 25.1.*** Dantzig's Simplex Method

- **Background**

- **Slack Variables**

  We can introduce a slack variable for each constraint, turning each inequality into an equation called the **slack form**. 

  For each choice of the **independent variables** $x, y$, we can calculate the value of the dependent slack variables $s_1,s_2,s_3$. 

  >e.g.
  >
  >$3x+2y \leq 15\\2y-x \leq 5 \\ x+2y \leq 7$ $\implies$ $3x+2y+s_1 = 15 \\ 2y-x+s_2 = 5 \\ x+2y+s_3 = 7$ $\implies$ $2 = 15-3(3)-2(2) \\ 4 = 5 + (3) - 2(2) \\ 0 = 7 - (3) - 2(2)$ while $x=3, y=2$
  >
  >

- **The Cost Variable**

  We can treat the objective function as an equation for some new dependent variable $C$ called the **cost equation**

  > e.g. 
  >
  > $C = 2x+3y$

- **Tableaux** - a matrix representation of a system of linear equations

  >e.g. 
  >
  >Linear Constraints $3x+2y \leq 15\\2y-x \leq 5 \\ x+2y \leq 7$
  >
  >Cost Equation $C = 2x+3y$ 
  >
  >$\implies$ Tableau $\begin{bmatrix}3x+2y &+1s_1 &\ &\ &\  &=15 \\ -1x+2y &\ &+1s_2 &\ & &=5 \\ 1x+2y &\ &\ &1s_3 & &=5 \\ -2x+3y &\ &\ &\ &+1C &=0\end{bmatrix} \implies \begin{bmatrix} 3 & 2 &1 &0 &0 &0 &15 \\ -1 &2 &0 &1 &0 &0 &5 \\ 1 &2 &0 &0 &1 &0 &7 \\ -2 &-3 &0 &0 &0 &1 &0 \end{bmatrix}$
  >
  >

- **Steps**

  1. Construct the initial tableau from the slack form of the linear program, together with the equation for the cost $C$. 

  2. Identify the column with the most negative coefficient in the final row, corresponding to the objective function. 

  3. Calculate the **row equations** by dividing each of the entries in the final column by the entries in the pivot column. 

     >$\begin{bmatrix} 3 & 2 &1 &0 &0 &0 &15 \\ -1 &2 &0 &1 &0 &0 &5 \\ 1 &2 &0 &0 &1 &0 &7 \\ -2 &-3 &0 &0 &0 &1 &0 \end{bmatrix} \implies \begin{matrix} \frac{15}{2} \\ \frac{5}{2} \\ \frac{7}{2} \\ \ \end{matrix}$

  4. Identify the row with the **smallest** row quotient where both the numerator and denominator are **positive**. 

  5. The **pivot value** is the value in the pivot column and in the pivot row. Apply the following row transformation: 

     *Pivot row* - $R_{\text{pivot}} \leftarrow (\frac{1}{\text{current pivot value}}) \times R_{\text{pivot}}$

     *All non-pivot rows* - $R_i \leftarrow R_i - (\frac{\text{current } R_i \text{ value}}{\text{current pivot value}}) \times R_{\text{pivot}}$

     >$ \begin{bmatrix} 3 &2 &1 &0 &0 &0 &15 \\ \bold{-1} &\underline{\bold{2}} &\bold{0} &\bold{1} &\bold{0} &\bold{0} &\bold{5} \\ 1 &2 &0 &0 &1 &0 &7 \\ -2 &-3 &0 &0 &0 &1 &0 \end{bmatrix} \implies \begin{bmatrix} 4 &0 &1 &-1 &0 &0 &10 \\ -0.5 &1 &0 &0.5 &0 &0 &2.5 \\ 2 &0 &0 &-1 &1 &0 &2 \\ -3.5 &0 &0 &1.5 &0 &1 &7.5 \end{bmatrix}$ 
     >
     >by the following steps: 
     >
     >$R_1 \leftarrow (R_1 - \frac{2}{2}R_2) \\ R_2 \leftarrow \frac{1}{2}R_2\\ R_3 \leftarrow (R_3 - \frac{2}{2}R_2)\\ R_4 \leftarrow (R_4 - \frac{3}{2}R_2)$

  6. Repeat until you run out of valid pivot columns

     > $\implies \begin{bmatrix} 0 &0 &1 &1 &-2 &0 &6 \\ 0 &1 &0 &0.25 &0.25 &0 &3 \\ 1 &0 &0 &-0.5 &0.5 &0 &1 \\ 0 &0 &0 &-0.25 &1.75 &1 &11 \end{bmatrix} \implies \begin{bmatrix} 0 &0 &1 &1 &-2 &0 &6 \\ 0 &1 &-0.25 &0 &0.75 &0 &1.5 \\ 1 &0 &0.5 &0 &-0.5 &0 &4 \\ 0 &0 &0.25 &0 &2.25 &1 &12.5 \end{bmatrix}$
  
  7. Read off the optimal solution
  
     > $&\ &\ &s_1 &+s_2 &+-2s_3 &\ &=6 \\ &\ &y &+-0.25s_1 &\ &+0.75s_3 &\ &=1.5 \\ &x &\ &+0.5s_1 &\ &+0.75s_3 &\ &=4 \\ &\ &\ &0.25s_1 &\ &+2.25s_3 &+C &=12.5  $
  

###### ***Theorem 25.2.*** (Klee-Minty '72) The worst-case termination time for the Simplex Method exponential. 

Maximise: $2^nx_0+2^{n-1}x_1+...+2x_{n-1}+x_n$

Subject to: 

$x_0 &\leq& 5 \\ 4x_0+x_1 &\leq& 25 \\ ... &\ & ... \\ 2^nx_0+2^{n-1}x_1+...+2x_{n-1}+x_n &\leq& 5^{n+1} \\ x_0, x_1,...,x_n &\geq& 0$

###### ***Theorem 25.3.*** (Khachiyan '79) Linear Programming is solvable in Polynomial Time. 

---

#### 26. Integer Programming

###### ***Theorem 26.1.*** Integer Programming **IP**

**Input)** 

A set of linear constraints $A\vec{x} \leq \vec{b}$

A linear objective function $f:\mathbb{R}^n \to \mathbb{R}$

**Output)** A solution $\vec{x} \in (\mathbb{Z}^{\geq 0})^n$ such that 

1. $\vec{x}$ satisfies the constraints $A\vec{x} \leq \vec{b}$ and
2. $f(\vec{x})$ is **maximum**

***Example 26.1.*** Maximal Matchings

>Considering there are the following workers and the following tasks,
>
>workers: 👩🏽👩🏽👴🏼👵🏼👵🏼👵🏼
>
>tasks: *A* (need 4), *B* (need 2), *C* (need 1), *D* (need 2)
>
>1. For each edge $(i,j)$ between worker $i$ and task $j$, let $x_{i,j}$ be a **numerical variable** and add the constraint $x_{i,j} \leq 1 \text{ for all edges }(i, j)$
>2. Construct a constraint for each worker $i \in $ (👩🏽, 👴🏼, 👵🏼, )

###### ***Theorem 26.2.*** Integer Programming is **NP-hard**. 

###### ***Theorem 26.3.*** The Boolean Satisfiability **SAT** problem is polynomially reducible to the Integer Programming Problem. 

***Proof.*** 

>Given a formula $F$ in Conjunctive Normal Form. 
>
>1. We want to **construct** an integer program IP$_F$ such that 
>
>   $F \text{ is satisfiable} \iff \text{IP}_F \text{ has an integer solution}$
>
>2. For each propositional variable $P$, let $x_P, x_{\neg P}$ be **numerical variables**
>
>3. Convert each clause into a linear constraint as follows: 
>
>   > $(P \vee Q \vee \neg R) \\ (\neg P \vee Q \vee R) \\ (\neg Q \vee R)$ $\implies$ $(x_P+x_Q+x_{\neg R}) &\geq 1 \\ (x_{\neg P}+x_Q+x_R) &\geq 1 \\ (x_{\neg Q}+x_R) &\geq 1 $
>
>4. For each propositional variable add the following constraint
>
>   >$(x_P+x_{\neg P}) &\leq 1 \\ (x_Q+x_{\neg }) &\leq 1 \\ (x_R+x_{\neg R}) &\leq 1 \\ (x_+x_{\neg S}) &\leq 1 $  or  $(x_P+x_{\neg P}) &\geq 1 \\ (x_Q+x_{\neg }) &\geq 1 \\ (x_R+x_{\neg R}) &\geq 1 \\ (x_+x_{\neg S}) &\geq 1 $
>
>5. It is easy to verify that
>
>   $F \text{ is satisfiable} \iff \text{IP}_F \text{ has an integer solution}$
>
>   **SAT** $\leq_P$ **IP**
>
>6. Q.E.D.

###### ***Theorem 26.4.*** Branch-and-Bound

###### ***Theorem 26.5.*** Other Variants of Linear Programming



