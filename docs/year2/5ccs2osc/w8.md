## WEEK VIII - User-Level Memory Allocation

>[ðŸ  MENU - 5CCS2OSC](year2/5ccs2osc.md)
>
>[â¬…ï¸ WEEK VII - Memory Virtulisation Optimization](year2/5ccs2osc/w7.md)
>
>[âž¡ï¸ WEEK IX - Access Control](year2/5ccs2osc/w9.md)
>
>Outlines:
>
>20. Heap in Memory Management
>     - Heap Allocation
>     - Introduction to `malloc`
>     - Mechanism
>     - Features
>21. Stack in Memory Management
>     - Stack Allocation
>       - Buffer Overflow
>     - Mechanism
>22. Communication between Processes
>     - Introduction to Files*
>     - Process Communication and Identification
>     - I/O Operation

### 20. Heap in Memory Management

##### 20.1. Heap for memory management

- Definition

     - **Heap** and **Stack** are the two definitions of the memory management. The memory used by app is not a continuous memory space but in different memory areas, including
       - heap
       - stack
       - static (BSS and data)
       - ...
     - **Heap**
       - Each process contains a heap area, which only stores with primitive types objects and the reference of objects. Objects are stored at stacks. When using allocator (e.g., alloc/new/copy...,etc. ) to create a new object, heap will allocate a memory segment. 
       - Heap is private for each process. 
       - We need to manage this part of memory. 
     - **Stack**
       - Stores the function-related informaiton, like local variables and parameters, etc, when a function is invoked.
       - Stores the objects, no references and primitive types will be stored in stack. 
       - ...

##### 20.2. Introduction to `malloc`

> Ref:
>
> [5åˆ†é’Ÿçœ‹æ‡‚malloc - Zhihu](https://zhuanlan.zhihu.com/p/105126066)

- Memory allocation within user processes

   >- We know that things like 
   >
   >  ```java
   >  int[] n = new int[42]; // Java
   >  ```
   >
   >  ```C++
   >  int *pn = new int[42]; // C++
   >  ```
   >
   >  are equivalent like this
   >
   >  ```C++
   >  int *pn = malloc(42*sizeof(int));
   >  ```
   >
   >- Formally, the OS has nothing to do with these. Both an operator `new` implementation and a `malloc` implementation exist as library code, inside user processes. 

   - `void* malloc(size_t size)`
     - A method that allocate size bits of uninitialized **virtual** memory, owned by the process. 
     - If the allocation is successful, return a pointer that points to the new allocated memory. Need to apply with `free()` or `realloc()` before dereference the corresponding pointers. 
     
       > Note: dereference means to get the corresponding value of the reference. 
     
     - If the allocation is failed, return *nullptr*. 
     - A `malloc()` and a `free()` should appear in pairs. 
    - `void free(void* ptr)`
      - A method that receive a pointer that need to be dereferenced/*nullptr* but do no processing with that. Just to prevent memory leak. 

- Reasons to learn

   >- They do *use* the OS to get their memory. *(e.g. in page-sized chunks and larger)*
   >- An OS kernel will internally have its own implementation of `malloc` or similar, to allocate memory for its own use.
   >- These allocators exhibit another instance of fragmentation. 
   >- Higher-level language implementations almost always include a `malloc`-like allocator and/or use the one provided by the C library on the host system.

##### 20.3. Mechanism of heap memory management

- Goal

    - The job of user-level memory allocator 
       1. To <u>parcel out small pieces</u> from a bigger chunk of memory given by the OS *(i.e., the heap data)*. 

          > Since the OS cares only about big chunks of memory (at least a page), while programs deal in smaller chunks. 

       2. And to request more memory from the OS, if the big chunk is exhuasted.
       3. And to reclaim (recycle) chunks when it knows that they are unused.

- Manual memory management

   - In modern language runtimes, garbage collections (GCs) are intimately connected to the just-in-time compiler.
   - Here we will consider only manual memory management, where programmers have to do `free(pn)`/`delete[] pn` (in C or C++). 


- Mechanism

    - For memory allocator
      - The OS use a linked list to maintained the memory spaces that are allocated, which tracks <u>which areas are free and which used</u>. A memory allocator works within them (like one or more big blocks of memory). 

        > e.g.
        >
        > ```CPP
        > int *pn = malloc(42 * sizeof(int));
        > int *pn = new int[42]; 
        > ```
        >
        > both mean that
        >
        > - find a free range of >= 168 bytes.
        > - chop out 168 bytes from the range.
        > - mark those as used. 
        > - return a pointer to their start.
        >
        > Note that in OS's view and malloc's view, some memory space allocating status may different. There is a further layer of allocation, and a further layer of *fragmentation*.

      - When the OS receive a request from the program, it will iteratively check the linked list, with doing the following, 
        1. Find a heap index where the space is larger than the request need, 
        2. Delete this index from the linked list that stores the information of free area. 
        3. Allocate the corresponding space to the program. 
    - Meanwhile, a `free(pn)` dose the opposite: mark the chunk as free. 
      - The data will be recorded into the some data structures that used by `malloc()`. 
      - A simple data structure *free list*, each free list node records the size of the free area and a pointer to the next free area. Adjacent free areas are usually coalesced.

##### 20.4. Features

- Contignous memory allocation

     - Memory that the `malloc()` function allocates is contiguous memory chunk in the virtual memory address space. but any underlying physical pages <u>are unlikely to be contiguous physically</u>. 
     - i.e., `malloc()` may allocate pieces of contiguous virtual memory chunks even though they are not contiguous from each other. 
     
- `malloc` and system calls

     - Two types of appoach in memory allocation: *best-fit* or *first-fit*. 

       > Since allocation involves a search.
       
     - Consider how `malloc` uses system calls
       - As the heap is full and suffering fragmentation. A moderately big request to `maloc()` will not be satisfiable, so the `malloc()` implementation will ask the OS for more memory. 
       - One way on Unix-like OSs is to <u>use a system call</u> `brk()` that ask to *move the program break*. Moving the break to a later position allocates memory in the intervening space.
         - The program break is the end position of data in the virtual memory space. It is part of the per-process state maintained by the OS. 
         - It need not necessarily allocate physical memory. 
       - Another method is `mmap()`, as modern OSs provide more flexible ways to ask the OS for more memory, that don't care about the program break. 
         - A `malloc()` implementation <u>might do this and end up with multiple non-contiguous arenas</u>. But we can extend the free list in our case *(A real `malloc()` won't use just a single free list)*. 


- Performance of a `malloc`

     - Measurement
       - How fast is allocation/freeing and how much external/internal fragementation, etc. All these need to be measured *per workload*. 
       - A workload is a *(program, input)* pair. 
       - External fragmentation can be measured by <u>the difference (or 1âˆ’ the ratio)</u> between 
         - the biggest contiguous chunk we can allocated, and
         - the total space we have free (across all  arenas).

- Concurrent of `malloc`
     - A free list, or other data structure used for the same purpose, MUST be **concurrent**. Since Many threads might be calling `malloc()` at the same time.


---

### 21. Stack in Memory Management

##### 21.1. Stack for memory management

- Definition

    - **Stack**
      - A thread will be allocated to a stack. 
      - When a function is invoked (e.g., the initial method of an app, `main()`, a *stack frame* will be puted into the stack, including the related information of this method, <u>like parameters, return address, local variable, etc.</u> When another function is invoked, another stack frame will be puted as into the stack. 
      - After the function is returned, the stack frame will be destoried. 
      - The stack automatically allocate the memory which is like LIFO principle, so we don't need to manage the memory in the stack. 
      - Local buffer `buf` is stored on the stack. 

        > e.g.
        >
        > ```C++
        > char buf[] = "Hello, world!\n"; 
        > printf(buf);
        > return 0;
        > ```


##### 21.2. Mechanism of stack management

- Definition

     - Stack frame - the state kept by each procedure activation. Sometimes called an *activation record*. 
     - Frame pointer (`rbp` on x86-64) - we can go through the stack by following the chain of stack frames. 


- Mechanism

     - When loading a function, add to each stack frame with 
       - The caller's program counter
       - The caller's frame pointer
       - Any local variables
         > e.g. Local buffer like `"Hello, world\n"`.
     - Then the stack encodes with the following about the execution, 
       - The history
       
         > i.e. What as-yet-uncompleted calls have got us to this point. Could be useful for printing a backtrace, for debugging. 
       - The continuation
       
         > i.e. Where we will go (back) to next, when we return, for each active call. 


- Buffer overflows redux

   - Buffer overflow
     - If the input is larger than the capacity of buffer, it may lead to buffer overflows. It may break the data and functions and even lead to security issues. 
     - Like stack-smashing attack, which is a particular kind of buffer overflow attack.
   - Redux
     - In C++ and C, some library interfaces using bounds check to prevent that.
       
         > e.g. `strncpy()` instead of `strcpy()`, `vector::at()` instead of `vector::operator[]`, etc. 
       
         > C++ and C complier won't always add bound checks automatically since the designing principles of them are effective. They also have a higher level permission on accessing lower structures. 


---

### 22. Communication between Processes

##### 22.1. Introduction to Files*

> Note:
>
> This chapter is not in the lecture but just help to understand. 

- Definition

     - **File** - a set of meaningful information or data. 
       - Including the following properties
         - Filename - identifier for the user. 
         - Identifier - the OS uses identifiers to identify the different files. 
         - Type
         - Location - the path that where the file is stored.
         - Size
         - Created Time
         - Updated Time
         - Information of the file owner
         - Protect information - record the permission of different users to this file. 
       - Structure of the File
         1. Unstructured File - consisted with a series of data streams. 
         2. Structured File - consisted with a group of records, and each record consist with a number of data items. 

- The Structure of (Structured) File

     - Logic Structure - the organization for the internal data of a file
       1. Sequential File
       2. Indexed File
       3. Indexed sequential file
     - Physical Structure - how the data of a file is stored externally


##### 22.2. Process Communication and Identification

- Processes are inherently communicative

   - Objects
     - Can communicate with the OS
       - By *syscall*. 
       - System calls can be used for requesting resources, executing operations, etc. 
     - Can communicate with other processes
   - Mechanism
     - Processes make requests <u>in terms of identifiable objects</u>. 
     - The communcation <u>is identified by using **handles**</u>. 

- Introduction to Handles

   - Definition
     - Handle - An abstract reference to a resource, which is used when app references blocks of memory or objects that are managed by other systems, like OS. 
     - A resource handle can be an opaque identifier. 
   - Identifier in the OS
     - Processes must use **opaque handles** to identify OS-level objects
       - Since processes don't share memory with OS, so the identifier cannot use memory address. 
       - Since the OS mediates access to the hardware and abstract away the details, so the identifier cannot use device-level identifiers. 
       - Some objects are OS constructs *(e.g., threads, processes)*. 
   - Approaches to handles

     > e.g.
     >
     > - InfOS - an arbitrary 64-bit integer as handles
     > - Unix-like versions - use `int` for file handles

- File Descriptors (FD)

   - Definition
     - FD - a process-unique identifier *(i.e., handle)* for a file or other input/output resource.
     - Can refer to many kinds of objects. 
       - Most of them support `read()` and `write()`. 
       - Some support with further methods. 
       >e.g.
       >
       >- terminal *(like keyboard or display)*
       >- files on disk
       >- network connections
       >- devices
     - By default, a Unix process has at least three open file descriptors
       - 0 - standard input
       - 1 - standard output
       - 2 - standard error
   - Communication
     - FD could be thought as a logical communication channel
     - Might be one-way or two-way
       > i.e., read-only, write-only, read-write
     - It could access an *abstraction* of a device
   
- Inter-process Communcation (IPC)

   - Definition
      - **IPC** - A unidirectional data channel provided by the OS, that using standard input and output. i*.e. Data communication between processes*.
   - Communication
      - Though files can be shared, can get network sockets on *a special loopback network card*, but there are also <u>IPC-only abstractions.</u> 
   
         >e.g.
         >
         >- pipes (one-way in-memory channel)
         >- pseudo-terminals (two-way in-memory channel)
         >- other abstractions, varying by OS
   
   - Memory-map
      - An IPC mechanism for sharing data and communication between different processes.
      - With page tables (or other MMU support), we can share just the memory we want to share, even writable by one or both processes. 
         - By memory-map request. *e.g. Unix `mmap()` system call*
         - `malloc()` use it to get *anonymous* memory (memory without mapping a named file).
      - To share writable memory, it will ask for `MAP_SHARED`. 


##### 22.4. I/O Operation

- Device Drivers

  - Definition
    - Device Drivers are the OS's code for talking to a specific device. 
    - They provide a higher-level, device-agnostic interface to the rest of the OS. The OS exposes this and usually abstracted a bit further, to user programs via system calls. 

      > For I/O, OSs provide an abstracted interface onto I/O devices. As I/O devices are many and varied, it's not workable for each program to have knowledge of each device. 

    - Devices must also be *shared*, i.e. multiplexed, securely and (usually) concurrently.

- I/O Steps

  1. Programmed I/O & polling
     - **Polling** - While reading some data from disk (on x86), since the disk is slow, we can just keep asking whether itâ€™s ready. This process is Polling. 
     - For predictable and slow devices, we could use command like *poll after n ms*. 
  2. Interrupts
     - To avoid wasting millions of CPU cycles waiting, we use hardware interrupts. 
     - Interrupts is a feature of the CPU and the devices.
       - **Interrupt-driven I/O**
         - Use programmed I/O to transfer input. 
         - It lets the CPU carry on, devices later interrupt when finished and jump toawell-defined handler place in memory: an interrupt handler. 
       - An **interrupt controller** multiplexes the interrupt pin between multiple devices
       
         > On modern x86 platforms this is called an APIC. 
       
     - Process during an interrupt
       - When an device interrupt is raised, Interrupt controller sets `INTR` to `logic 1`. 
       - CPU stops what itâ€™s doing and then
         - save registers on stack
         - read interrupt `#` (which device) over data bus
         - look up interrupt handler in table
         - jump to the handler address stored in the table
         - handler completes the I/O, then uses a special â€˜return from interruptâ€™ instruction *(`iret` on x86)*
       - CPU reloads saved state from stack
  3. Optimization on the preivous steps
       - The handler simply reads the data (now ready) from the device, into actual memory. 
         - The CPU is still tied up talking to a slow device, but for much less time than polling, because the data is ready (in the device buffer). 
           - Direct memory access (DMA)
             - A **direct memory access (DMA) controller** allows <u>a device to read/write data from memory by itself</u>. 
             - This saves even more CPU time, by avoiding these in and out (or similar) instructions
               > Note on some architectures all I/O is *memory-mapped*, means there are no special instructions but just loads from/stores to special physical memory address. 
             - We still get an interrupt on completion, but the data has already been transferred.
       - Use of timer interrupts for preemptive scheduling
         - Timer interrupts
           > Interrupts aren't just for doing I/O with the outside world. OSs make heavy use of timer interrupts
           - A device sitting on an I/O bus
           - We program it to ask it to interrupt us after a certain time/with a certain frequency. 
           - The OS contains a scheduler using some algorithm *(e.g., FIFO or Round Robin)*
             - the running process is interrupted so something else can be scheduled. 
             - When the timer interrupt fires, its interrupt handler calls the scheduler to pick another process to run. 
