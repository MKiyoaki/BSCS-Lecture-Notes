## WEEK VI - Deep Generative Neural Networks

>[ðŸ  MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[â¬…ï¸ WEEK V - Deep Discriminative Neural Networks](year3/7ccsmpnn/w5.md)
>
>[âž¡ï¸ WEEK VII - Feature Extraction](year3/7ccsmpnn/w7.md)
>
>Outlines:
>
>1. 

### 6.1. Introduction to Generative Neural Networks

##### 6.1.1. Motivation of Generative Models

- GANs
  - Generative Models: produce net contents. 
  
- Application
  - Text to image generation
  - Image to image translation
  - Increasing image resolution
  - Predicting next video frame
  - ...

---

### 6.2. Generative Model Taxonomy

##### 6.2.1. Overview

> Maximum Likelihood
>
> > Explicit Density
> >
> > > Trackable Density
> > >
> > > Approximate Density
> > >
> > > > Variational Autoencoder
> > > >
> > > > Markov Chain
> >
> > Implicit Density
> >
> > > Direct (GAN)
> > >
> > > Markov Chain

- Maximum Likelihood

  - **Maximum Likelihood (MLE)** estimates the parameters of the model, which gives the highest probability to get the observed data samples. 
  - For a given model $p_\text{model}(x|\theta)$ with the parameter of $\theta$, the MLE is defined as follows:

  $$
  \theta^* = \arg\max_\theta \mathbb{E}_{x \sim p_\text{data}} \log p_\text{model} (x|\theta)
  $$

  - For generative models, the objective is to generate data samples similar to those in the training set. This is typically achieved by finding the model parameters that maximize the likelihood of the training data.

- Generative Modeling

  - Density Estimation
    - The objective is to learn the probability distribution $p(x)$ for a given dataset.
    - MLE can be used to estimate the probabiltiy distribution.
  - Sample Generation
    - Generate new data samples basing on the learnt distribution, sampling $x \sim p(x)$. 
  

##### 6.2.2. Trackable Density

- Definition

  - An explicit formula is needed. 

  - Explicit model based on a chain
    $$
    p_\text{model}(\textbf{x}) = p_\text{model}(x_1) \prod^n_{i=2} p_\text{model}(x_i | x_1, ..., x_{i-1})
    $$
    where $p_\text{model}(\textbf{x})$ is the likelihood of the input image $\textbf{x}$, $p_\text{model}(x_i | x_1, ..., x_{i-1})$ is the probability of $i$-th pixel value given all previous pixels. 

  > e.g. Examples:
  >
  > Fully visible belief nets: NADE, MADE, <u>PixelRNN</u>
  >
  > Neural Autoregress
  >
  > Change of variable models

- Evaluation

  - Cons:
    - Propagation is needed to generate a sample.
    - Generation is not controlled by a latent code. 

##### 6.2.3. Approximate Density

- Autoencoder

  - Autoencoder (AE) is a generative network, which contains encoder and decoder modules. 

    ![EveryThing about AutoEncoders | by Tejpal Kumawat | Medium](https://miro.medium.com/v2/resize:fit:600/0*LtrxkZrn87VTYML6.png)

    - Encoder is used to compress the input data $x$ into a low dimensional latent vector $z$. While decoder recover the latent vector $z$ into the estimate of original data sample $\hat{x}$. 
    - The network training process is to minimize the reconstruction error.

      > e.g.
      >
      > L2 loss function:
      > $$
      > J(\theta) = \sum^N_{i=1} \parallel x_i - \hat{x}_i \parallel^2
      > $$

  - Variational Autoencoder performs better than standard AE. 

- Variational Autoencoder (VAE)

    - Definition

      - VAE introduces probability distribution to model the latent space, in order to rebuild and generate the data. 
        $$
        p_\theta (x) = \int p_\theta (z) p_\theta (x | z) dz
        $$
        
        where
        
        - $p_\theta(x)$ is the density function (model),
        - $z$ is the latent code,
        - $\theta$ is the model parameter vector.
        
      - This is an <u>intractable density</u> function and thus cannot be optimised directly but have to derive and optimise the lower bound of the likelihood instead.
      
    - Difference with AE

      - AE gives the latent output as a fixed vector $z$, while VAE gives a distribution $q(z|x)$.
      - For VAE, latent space samples a latent code from the distribution space $z \sim q(z|x)$.
      - Decoder recovers the input according to $p(x|z)$.
      


---

### 6.3. Generative Adversarial Networks (GANs)

##### 6.3.1. Introduction to GANs

- Motivation
  - We want to sample from complex and high-dimensional training sample distribution.
  - Thus, we can sample from a simple distribution, e.g., random noise, and transform it to training distribution by a generator network.
  - Simple idea for generating samples $x$ using latent code $z$. 
    - However, there is no guidance to train the generator.
    - Randomly generated latent code $z$ generates output noise.
- Definition
  - GAN introduces a two-player structure (actor-critic approach).
  - Consists of two main elements:
    - **Generator**
      - Maps random noise $z$ to produce a meaningful sample.
      - If not well trained, the produced sample is a noise.
    - **Discriminator**
      - To decide if the sample $x$ is real 
  - 

---

### 6.4. GAN Problem