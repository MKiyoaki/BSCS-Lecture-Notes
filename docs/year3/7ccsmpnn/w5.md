## WEEK V - Deep Discriminative Neural Networks

>[🏠 MENU - 7CCSMPNN](year3/7ccsmpnn.md)
>
>[⬅️ WEEK IV - Multilayer Perceptrons & Backpropagation](year3/7ccsmpnn/w4.md)
>
>[➡️ WEEK VI - Deep Generative Neural Networks](year3/7ccsmpnn/w6.md)
>
>Outlines:
>
>1. Deep Neural Networks
>1. 

### 5.1. Deep Neural Networks

##### 5.1.1. Introduction to Deep Neural Networks

- Definition
  - A *deep* network is a neural network with more than 3 layers. Typically far more than 3 layers.
  - A *shallow* network is a neural network with less or qual than 3 layers.
  - The term **deep learning** refers to the process of training a deep neural network. 
- Motivation
  - Three-layer nerual networks are able to approximate any continuous non-linear function arbitrarily well.
    - Universal function approximators.
    - Can solve any pattern recognition task, in theory
  - However, to solve complex tasks (i.e., perform complex mapping), we need larger networks with more parameters. 
  - The success of VGGNet shows that <u>a deep network ususally has a better performance than a wide network</u>. 
- Deep Networks
  - Deep network aims to provide a hierarchy of representations with increasing level of abstraction.
    - It is the natural of dealing with many tasks.
  - Deep network allows **recurrent structures**. i.e., Recurrent nerual networks (RNNs). 
    - Recurrent networks process temporal information, where the value of $\textbf{x}$ changes over time. 
    - Training a recurrent network is achieved by unfolding the network and using backpropagation on the unfold network. 
    - There is one layer in the unfolded network for each time-step, so recurrent networks for processing long sequences are equivalent to very deep networks. 

##### 5.1.2. Difficulties in Pratices

- Vanishing Gradient

  ```mermaid
  graph LR
  	input --> y1
  	y1((y1)) --w21--> y2((y2))
  	y2 --w32--> y3((y3))
  	y3 --w43--> y4((y4))
  	y4 --w54--> y5((y5))
  	y5 --> output
  ```

  - Definition
    - Consider a deep network with one neuron per layer. While applying backpropagation: each time the error is propagated further backwards it is multiplied by a factor of the form $\phi'(w_{ji}x_i)w_{ji}$.
      - When using standard approach of activation function, the derivative of activation function results $\phi'(w_{ji}x_i)$ is small for most values of $\textbf{wx}$.
      - When using standard approach of weight initialization, the term $w_{ji}$ may also be small.
    - Thus this factor may getting more and more **smaller** when backpropagation. 
      - Neurons in the earlier layers learn much more slowly than neurons in later layers.
      - <u>Early layers contribute nothing to solving the task</u> and hence a deeper network cannot improve with the performance. 

- Exploding Gradient Problem

  - Definition
    - Consider a deep network with one neuron per layer. Each time the error is propagated further backwards it is multiplied by a factor of the form $\phi'(w_{ji}x_i)w_{ji}$. 
      - If the weights $w_{ji}$ are initialized to, or learn, large values, then this factor could be far more than 1. 
    - If not careful, the gradient tends to get **larger** as we move backward through hidden layers. This means:
      - Neurons in the earlier layers make, large, often random changes in their weights.
      - Later layers can NOT learn due to constantly changing output of earlier layers and, hence making network deeper makes the performance worse. 

- Solutions

  - Since the issues of gradients, backpropagation is inherently unstable.
  - To train deep networks it is necessary to mitigate this issue, by using:
    - activation functions with non-vanishing derivatives
    - better ways to initialize weights
    - adaptive variations on standard backpropagation
    - batch normalization
    - skip connections

  - Besides, in order to adapt large amount of parameters in the network, we need:
    - very large labelled datasets
    - large computational resources


##### 5.1.3. Solutions

- Activation Functions with Non-vanishing Derivatives

  - Rectified Linear Unit (ReLU)
    $$
    \phi'(net_j) = 
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    0 &\text{if } net_j < 0
    \end{cases}
    $$

  - Leaky Rectified Linear Unit (LReLU)
    $$
    \phi'(net_j) =
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    a &\text{if } net_j < 0
    \end{cases}
    $$

  - Parametric Rectified Linear Unit (PReLU)
    $$
    \phi'(net_j) =
    \begin{cases}
    1 &\text{if } net_j \geq 0 \\
    a_j &\text{if } net_j < 0
    \end{cases}
    $$

- Better Ways to Initialize Weights

  - For weights connecting $m$ inputs to $n$ outputs:

    | Methods       | Activation Function | Choose Weight from Uniform distribution with range of       | Choose Weight from Normal distribution with mean=0, deviation of |
    | :------------ | ------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
    | Xavier Glorot | sigmoid/tanh        | $\left( -\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right)$ | $\sqrt{\frac{2}{m+n}}$                                       |
    | Kaiming He    | ReLU/LReLU/PReLU    | $\left( \sqrt{-\frac{6}{m}}, \sqrt{\frac{6}{m}}\right)$     | $\sqrt{\frac{2}{m}}$                                         |

- Adaptive Versions of Backpropagation

  - Motivation
  
    - Backpropagation struggles to deal with gradients in the cost function $J(\textbf{w})$ that are too small or too large.
  
      > Recall that backpropagation performs gradient descent to find parameters that minimize a cost function. 
  
    - Variation in the magnitude of the gradient may occur between:
  
      - Different layers (due to vanishing and exploding gradient)
      - Different parts of the cost function for a single neuron
      - Different directions for a multi-dimensional function
  
  - 
  

---

### 5.2. Convolutional Neural Networks (CNNs)

##### 5.2.1. 