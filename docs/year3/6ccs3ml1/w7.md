## WEEK VII - Reinforcement Learning 1

>[ðŸ  MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[â¬…ï¸ WEEK VI - Neural Networks](year3/6ccs3ml1/w6.md)
>
>[âž¡ï¸ WEEK VIII - Reinforcement Learning 2](year3/6ccs3ml1/w8.md)
>
>Outlines:
>
>1. 

### 7.1. Introduction to Reinforcement Learning

##### 7.1.1. Reinforcement Learning Characteristics

- Terminologies

  - **Reinforcement Learning (RL)**: learning what to do (i.e. how to map situations to actions) to maximise a numerical reward signal.

  - **State space**: an agent occupies a given state at a given time.

  - **Action**: An action moves the agent from one state to another.

    - In RL, there is no involvement of any human interaction.

  - **Environment**: An agent is placed in an environment, is given a goal and learns how to behave in this environment by trial and error using feedback from its own actions and experiences.

  - **Reward** or **Reinforcement**: Feedbacks from the environment to guide agents to choose good actions.

    > e.g.
    >
    > In chess, the reinforcement is received only at the end of the game.
    >
    > In table tennis, each point scored can be considered a reward.

- Properties

  - Goal-oriented learning
  - Interaction with an uncertain environment
  - Learning how to map situations to actions to maximise a numerical reward signal
  - Learner is not told which actions to take
  - Trial and error search
  - Possibility of delayed reward
  - **Exploration** vs. **exploitation**
    - Exploration finds more information about the environment and might enable higher future reward.
    - Exploitation exploits known information to maximise reward, leading to an immediate reward.

- Potential Problems

  - *Evaluative* vs. *instructive* feedback
    - Evaluative feedback: how good was the action. Does not say whether that was the best thing to do.
    - Instructive feedback: what was the best action. Does not rate the action taken.
  - *Associative* vs. *non-associative* learning
    - Assocative maps inputs to outputs and learns the best output for each input.
    - Non-associative learns the one best output.

  > RL works with evaluative feedback. We will start with non-associative learning.

##### 7.1.2. $n$-armed Bandits

- Definition

  - A typical scenario to study with RL.

  - Playing a pair of slot machines. Choice is between playing machine $A$ and machine $B$. 

    - With many machines, you must choose which lever to play on each successive coin: the one that has paid off best, or maybe one that you have not tried.
    - The goal is to maximise the total reward in the long run.
    - Reward probabilities are not known.

  - Terminologies

    - Choose repeatedly from one of $n$ actions. Each choice is called a **play**.

    -  **Action values**: after each play $a_t$ , you get a reward $r_t$ , where:
      $$
      E\langle r_t | a_t \rangle = Q^*(a)
      $$
    
  - Distribution of $r_t$ depends only on $a_t$ .
    
  - Objective is to maximise the reward in the long term. To solve the $n$-armed bandit problem, you must *explore* a variety of actions and then *exploit* the best of them.

- Exploration vs. exploitation

  - Suppose you form **action value estimates **$Q_t(a)$ which estimates $Q^*(a)$, i.e. what each action is worth at each point in time.

    - Have a $Q_t(a)$ estimate for every action $a$.

  - If you maintain estimates of the action values, then there is (at least) one action whose estimated value is greatest at any time step (i.e. greedy).

  - The greedy action at $t$ is:
    $$
    a^*_t = \arg \max_a Q_t(a)
    $$

    - Picking $a^*_t$ is *exploitation*.
      - Exploitation maximises the expected reward on one step. 
      - You cannot exploit all the time.

    - Picking $a_t \neq a^*_t $ is *exploration*.
      - Exploration may produce the greater total reward in the long run.
      - You cannot explore all the time.

    - You can never stop exploring but you should eventually reduce exploring.

  - Action-value Methods

    - **Sample-average** method: maintain a list of all rewards received and average them for each action.

    - If by the $t$-th play, action a had been chosen $k_a$ times prior to $t$, yielding rewards $r_1, r_2, . . . , r_{k_a}$ then:
      $$
      Q_t(a) = \frac{r_1 + r_2 + ... + r_{k_a}}{k_a}
      $$
      If $k_a = 0$, set $Q_t(a)$ to some default value. 

    - We have:
      $$
      \lim_{k_a \to \infin} Q_t(a) = Q^*(a)
      $$

    - Thus, if we update often enough, the estimate of the value of each action $Q_t(a)$ will converge to the actual value of the action $Q^*(a)$.

  - Incremental Implementation

    - The sample-average method uses too much memory. Instead, use **incremental implementation**.

    - If $Q_k$ is the average of the first $k$ rewards and $r_{k+1}$ is the $k + 1$-th reward, then:
      $$
      \begin{array}{lcl}
      Q_{k+1} &= &\frac{1}{k+1} \sum^{k+1}_{i=1} r_i \\
      ~ &= & \frac{1}{k+1} \left( r_{k+1} + \sum^k_{i=1} r_i \right) \\
      ~ &= & \frac{1}{k+1} (r_{k+1} + kQ_k) \\
      ~ &= & \frac{1}{k+1} (r_{k+1} + kQ_k + Q_k - Q_k) \\
      ~ &= & \frac{1}{k+1} (r_{k+1} + (k + 1)Q_k - Q_k) \\
      ~ &= & Q_k + \frac{1}{k+1} (r_{k+1} - Q_k)
      \end{array}
      $$

##### 7.1.3. Epsilon-Greedy Algorithm

- Definition

  - Given an estimate of a reward for each action, we then have to decide what to do.

  - Greedy action selection:
    $$
    a_t = a^*_t = \arg \max_a Q_t(a)
    $$

    - Greedy action selection always exploits current knowledge to maximise immediate reward. But we do not want to just exploit.
    - Epsilon-greedy gives us a way to balance exploration-exploitation.

- Process

  - Pick $\epsilon \in (0, 1)$.

  - Epsilon-greedy action selection:
    $$
    a_t =
    \begin{cases}
    a^*_t & \text{with the probability } \Pr(1 - \epsilon) \\
    \text{random action} & \text{with the probability } \Pr(\epsilon) \\
    \end{cases}
    $$

    - Most of the time be greedy, but explore sometimes.
    - This is the simplest way to try to balance exploration and exploitation.

  > e.g.
  >
  > Bandit with $n = 10$ possible actions.
  >
  > > Each $Q^*(a)$ is chosen randomly from a normal distribution:
  > > $$
  > > \mathcal{N}(\mu = 0, \sigma = 1)
  > > $$
  > > Each $r_t$ is also chosen from a normal distribution:
  > > $$
  > > \mathcal{N}(\mu = Q^*(a_t), \sigma = 1)
  > > $$
  > > The 10-armed testbed: 1000 plays and 2000 randomly generated 10-armed bandit tasks.
  > >
  > > Basically the agent with $\epsilon = 0.01$ has a better performance. 

##### 7.1.4. Softmax

- Definition

  - Epsilon-greedy makes a random choice among non-optimal actions. However, it is good not to pick actions with poor outcomes sometimes.

  - **Softmax** picks non-optimal actions based on their reward.

  - Common to use the Gibbs distribution to pick the action.

    - Choose action $a$ on the $r$-th play with probability:
      $$
      \frac{e^{\frac{Q_t(a)}{\tau}}}{\sum_{b=1}^n \frac{Q_t(b)}{\tau}}
      $$
      where $Ï„$ is the **temperature**.

- Temperature

  - When temperature is high, all actions are approximately equally likely.

  - As temperature tends to 0, action selection tends to greedy selection.

  - Can vary $Ï„$ over time. 

    > e.g., high to start, low as the agent thinks it is converging.


---

### 7.2. Markov Decision Process

##### 7.2.1. Gernalized Scenarios

- Motivation

  - The bandit model makes a key simplifying assumption: the agent is always in the same state. So we only have to learn about one action.
  - In general, agents can be in multiple states, and the best action varies with state.
  - To handle this, we need a model of this kind of scenario.

- Definition

  - Consider an agent with a set of possible actions $\mathcal{A}$.

    -  Each $a \in \mathcal{A}$ has a set of possible outcomes $s_a$.
    - Each state $s_a$ has a value associated to it called **utility** $u(s_a)$.

  - The action $a^*$ which a *rational* agent should choose is that <u>which maximises the agent's utility</u>.
    $$
    a^* = \arg \max_{a \in \mathcal{A}} u(s_a)
    $$

    - The problem is that in any realistic situation, we do not know which $s_a$ will result from a given $a$, so we do not know the utility of a given action.
    - Instead we have to calculate the **expected utility** of each action and make the choice on the basis of that.

- Expected Utility

  - Given an action $a_i$ with a set of possible outcomes $s_{a_i}$ , the expected utility is the sum of the utility of each state in $s_{a_i}$ weighted by the probability of getting there by doing $a_i$:
    $$
    \mathbb{E}(u(a_i)) = \sum_{s' \in s_{a_i}} u(s') \Pr(s_{a_i} = s')
    $$

    - Select $a_i$ with the highest expected utility.

##### 7.2.2. Sequential Decision Making Problems

- Motivation

  - We have a method we can apply to individual decisions by agents. A bit more general than the n-armed bandit case. However, it is not enough.
  - Agents are not usually in the business of taking single decisions. The best overall result is not necessarily obtained by a greedy approach to a series of decisions.
  - The current best option is not the best thing in the long-run.

- State Transition

  - In some scenarios (e.g., Grid World), there are states and actions. Action leads to the transition of states and gives a corresponding reward. 

  - We can write a **transition function** to describe these actions. A transition model tells the agent the new state given a current state and an action.

  - Since the actions are stochastic, the transition function looks like:
    $$
    P(s'|s, a)
    $$
    where $a$ is the action that takes the agent from $s$ to $s_1$.

  - Transitions are assumed to be *Markovian*. They only depend on the current and next states.

  - We could write a large set of probability tables that would describe all the possible actions executed in all the possible states. This would completely specify the actions.

- Markov Decision Process (MDP)

  - The overall sequential decision making problem the agent faces here is a **Markov decision process (MDP)**.
  - Mathematically we have a tuple to define MDP, which contains
    - a set of *states* $s \in \mathcal{S}$ with an initial state $s_0$.
    - a set of *actions* $\mathcal{A}(s)$ in each state,
    - a *transition* function $P(s_1|s, a)$,
    - a reward function $R(s)$.
  - A solution to an MDP is a **policy**, which we write as $\pi$. It represents a mapping from perceived states of the environment to actions to be taken when in those states.
    - A choice of action for *every* state. That way, if we get off track, we still know what to do.
    - In any state $s$, $Ï€(s)$ identifies what action to take.
    - The goal is to find the optimal policy.

##### 7.2.3. Solving Optimal Policy

- The optimal policy $Ï€^*$ is the policy with the highest expected value. At every stage the agent should do $\pi^*(s)$.

  - Since actions are stochastic, policies will not give the same reward every time, so we need to compare the expected value.
  - The only way we will consider here is by calculating the utility for each state.

- Here we have the values of states if the agent executes an optimal policy. We write these values for every state $s$ as $U^{\pi^*}(s)$.

  - If we have these values, the agent has a simple decision process.

  - It just picks the action a that maximises the expected utility of the next state:
    $$
    \pi^*(s) = \arg \max_{a \in \mathcal{A}(s)} \sum_{s'} P(s'|s, a) U^{\pi^*} (s')
    $$
    Only have to consider the next step.

- Value Iteration

  - Motivation

    - The big question is how to compute $U^{\pi^*}(s)$. For this module we just have to know the method to calculate it. 
    - To compute these values, we use **value iteration**.

  - Process

    - Execute the following procedure:

      1. Assign an arbitrary utility $U_0(s)$ to every state.

      2. For every state, carry out the following update:
         $$
         U_{i+1} (s) \leftarrow R(s) + \gamma \max_{a \in \mathcal{A}(s)} \sum_{s'} P(s'|s, a) U_i(s')
         $$
         where $\gamma \in (0, 1]$ is the **discount rate**.

         This update computes the utility of each state by doing a maximum expected utility calculation on the current utility of the states around it.

      3. Continue updating until the utilities of states do not change.

      > Note.
      >
      > In computing $U_{i+1}(s)$ for each state we use the $U_i(s')$ for the states around it.

    - After an infinite number of applications, the utilities are guaranteed to converge on the optimal values.

    - In practice we need many times of updates.

##### 7.2.4. Passive learning

- Motivation

  - If the agent does not know the transition model and the reward function, we needs to *learn* the transition model and reward.
  - In **passive reinforcement learning** the agent's policy is fixed.
    - Agent learns utility $U^\pi(s)$ by carrying out runs through the environment, following some policy $\pi$.
    - In state $s$, it always executes the action $Ï€(s)$.
  - Agent does not make a choice about how to act. That is, it does not choose how to act while learning.

- Direct Utility Estimation

  - The utility of a state is a function of the rewards of all the states that are visited later.
  - We can estimate the utility of a state by the rewards generated along the run from that state by **direct utility estimation**.
  - Each run gives us one or more samples for the reward from a state.
    - As the agent moves, it can calculate a sample estimate of $P(s'|s, \pi(s))$. 
    - Each time it moves, creates a new sample for one state.
    - Over time, the agent builds up estimates of the utlities and $P(s'|s, \pi(s))$, for every $s, s_1$ for the given $\pi(s)$.

- Solution

  - Output from learning

    - A list of states $s_1,..., s_n$.
    - Each state has a utility estimate associated with it: $U(s)$.
    - Each state has a set of actions associated with it: $a_1 , ..., a_m$.
    - Each state/action pair has a probability distribution: $P(s_1|s, \pi(s))$ over the states $s_1$ that it gets to from $s$ by doing $\pi(s)$.
    - May not encounter every state.

  - Agent choose action as follows:

    - Computes each step using one-step lookahead on the expected value of actions.

    - Picks the action $a$ with the greatest expected utility.

      > Its data on actions will be limited because it has only been trying $\pi(s)$.



