## WEEK I - Processes, Scheduling and Threads

>[🏠 MENU - 5CCS2OSC](year2/5ccs2osc.md)
>
>⏹️
>
>[➡️ WEEK II - Multithreading in Java](year2/5ccs2osc/w2.md)
>
>Outlines:
>
>1. Multitasking
>    - Related Definitions
>    - Processes
>      - RAM
>2. Scheduling Processing
>    - Definition
>    - Algorithms
>      - Two types - preemptive and non-preemptive
>        - FCFS
>        - SJF
>    - Scheduling
>      - Round-Robin
>      - Multilevel Queue
>3. Threads
>    - Definitions

### 1. Introduction to Multitasking

##### 1.1. Definition

- Terminology

  - **multitasking**
    - A solution for CPU to <u>switch between multiple tasks</u>. 
    - Allowing each of the tasks to execute for a period of time. 
    - The CPU was still able to execute only one task at a time but could create the illusion that it was executing multiple tasks at the same time by doing so quickly and frequently.
  - **concurrency** - multiple tasks being able to make progress but <u>**NOT** necessarily literally at the same time</u>. 
  - **task** - some activity that the CPU is undertaking.
  - **program** - the contents of an *executable file* of some sort. 
  - **process**
    - The thing that the the operating system create when it loads a program to run. 
    - <u>An instance of a program</u> that is being executed by the computer.

    > If we start the same program multiple times then we will get multiple processes for the same program.

- Type of tasks

  - **CPU-bound** - A task spends the majority of its time executing code on the CPU.
  - **I/O-bound** - A task spends a lot of time using I/O devices. *e.g., storage, networks, printers, etc.*

##### 1.2. Processes

- RAM

  - When a computer system <u>runs a process</u> it must create certain areas in the **RAM**.
  - The code (text) and data segments from the executable file must be copied into the RAM.
  - The OS also allocates *two memory segments* for each process: 
    - The **stack** is used for <u>local variables and method parameters</u>, including the primitive types or references.
    - The **heap** is used for <u>storing objects</u> (in Java).

- The stack and the heap

  >The code and data in a program are ‘static’ - meaning (in this context) that the memory requirements for those sections don’t change. 
  >
  >*Local variables, method parameters and objects* created by `new` or `malloc` <u>do NOT exist in an executable file.</u> They only exist when they are created and they are not created until a program is running as a process *(and maybe not even then some methods might never be called, for example)*.

  - Created by the OS to allow a process to dynamically create the necessary variables, parameters and objects.

- Process Layout

  > [![File:Program memory layout.pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Program_memory_layout.pdf/page1-234px-Program_memory_layout.pdf.jpg)](https://upload.wikimedia.org/wikipedia/commons/5/50/Program_memory_layout.pdf)
  >
  > The layout of a process in RAM. The memory addresses start at 0 (bottom) and end at ‘max’ (top). As stated above, both of these structures are dynamic - they grow and shrink. 

  - For each process that the OS has created, there will be a block of memory. 

  - If two processes relate to the same program, *i.e., they have the same code and data segments*, then the OS <u>might share these</u> across multiple processes. 

  - The <u>stack and heap are **NOT** shared</u>.

  - The OS will manage that area of the RAM for each process that is running.

- Process Control Block (PCB)

  >![What is Process Control Block (PCB) in Operating System? - Binary Terms](https://binaryterms.com/wp-content/uploads/2019/08/Process-Control-Block.jpg)
  >
  
  - The structure in which <u>the OS keeps data related to each process</u> is called the **Process Control Block (PCB)**. Since the OS must *keep track* of the processes it starts.
  - PCBs are usually stored by the OS using a linked list.
  - Notable entries
    - `Process Id` - each process has a unique ID number
    - `Program counter` - the address of the next instruction for the process
    - `Process state` - whether the process is READY, WAITING, etc
    - `General purpose registers` - the contents of the CPU registers for the process
  
- Process State

  >```mermaid
  >stateDiagram
  >	new --> ready : admitted
  >	ready --> running : scheduler dispatch
  >	running --> ready : interrupt
  >	running --> waiting : I/O or event wait
  >	running --> terminated : exit
  >	waiting --> ready : I/O or event completion
  >```

  - When a process is first created its state is **new**. 
    - At this point the OS will make a decision about whether to admit the new process. 
    - It might be forced to wait if the system is already fully committed.
  - Once a process has been admitted it will transition to the **ready** state. 
    - This means it will wait to be executed.
  - When selected to execute via scheduler dispatch, the process will transition to the **running** state.
    - A running process it can be interrupted by the OS. This means the OS needs to run another process. The process that was interrupted will go back in the **ready** queue.
    - A running process might also issue an I/O request of some kind. If that is the case it will transition to the waiting state. Once the I/O request has been completed, the process will be put back in the ready queue.
  - When the process exits it will transition to the **terminated** state.

---

### 2. Scheduling processes

##### 2.1. Introduction

>In a modern computer system, there will always be many processes running simultaneously. The question arises, therefore, how does the operating system decide which process(es) should be running at any given moment?
>
>These decisions are made by schedulers. There are a number of schedulers. *For example, the long-term scheduler selects which processes will be loaded from external storage into the RAM.* The CPU scheduler makes the decisions about which of the processes in RAM will run.
>
>For modern OS: Both Linux and Windows use preemptive priority scheduling. MacOS appears to use a multilevel feedback queue.

##### 2.2. Context Switching

- Process

  >![img](https://afteracademy.com/images/what-is-context-switching-in-operating-system-context-switching-flow.png)

  - Process when CPU changes the process
    1. When a process is running on the CPU
       - The value of the program counter (which holds the address of the next instruction) and 
       - The CPU’s registers contain values that are specific to that process.
    2. If a process has been <u>interrupted</u> so that another process can run
       - These CPU register values must be saved in the PCB so that they can be restored next time the process runs.
    3. The next process that is being allowed to run may have its own values of these registers that need to be restored too (if it’s not running for the first time).
  - Time cost
    - Modern OSs and CPUs can perform a context switch in about $1-2 μ$s.

##### 2.3. Scheduling algorithms

- Definition

  - CPU scheduling algorithms select processes to execute. How they make these decisions depends on the <u>overall scheduling scheme and the type of process ‘favoured’ by the algorithm</u>.
  - In older operating systems, processes were expected to yield to each other: 
    1. A form of cooperative multitasking. 
    2. Older Windows and Mac operating systems used this approach. 
    3. If a process did not yield, it would never relinquish the CPU. This did not require a scheduler.
  - As systems started to run more and more processes, classes of algorithms developed based on the ideas of 
    1. **preemptive multitasking**
    2. **non-preemptive multitasking**
  - `Average waiting time (AWT)` - The **average of all weighting times**. 
    - The waiting time of a process is the amount of time spent in the ready queue.
    - Typically we want to minimise average waiting time.

- Types

  - With **preemptive multitasking**, the OS will <u>interrupt a running process to allow another process to run</u>.
  - With **non-preemptive multitasking**, a process will <u>control the CPU until it terminates or enters a waiting state</u>.

  >Note that Modern scheduling algorithms are usually **preemptive**.

##### 2.4. **First come, first served (FCFS)** Algorithm

- Features

  - The most basic scheduling algorithm simply operates a regular *first in, first out (FIFO)* queue. 
  - <u>FCFS is **non-preemptive**.</u>
  - The process that has been in the queue the longest is selected to run.
  - We can analyse the performance of scheduling algorithms to check if they are meeting the needs of a system.
- Example

  >e.g.
  >
  >Consider four processes P1, P2, P3, P4 and their associated burst times. The two tables below show the processes arriving in the queue in two different orders. ‘AWT’ = average waiting time.
  >
  >| Process | Brust Time (ms) | Wait Time (ms) |
  >| :-----: | :-------------: | :------------: |
  >| **P1**  |       27        |       0        |
  >| **P2**  |        4        |       27       |
  >| **P3**  |        6        |       31       |
  >| **P4**  |        5        |       37       |
  >| **AWT** |                 |       24       |
  >
  >| Process | Brust Time (ms) | Wait Time (ms) |
  >| :-----: | :-------------: | :------------: |
  >| **P2**  |        4        |       0        |
  >| **P4**  |        5        |       4        |
  >| **P3**  |        6        |       9        |
  >| **P1**  |       27        |       15       |
  >| **AWT** |                 |       7        |
  >
  >We can see that **the order of arrival of the processes** could have a big impact on the average waiting time. 

##### 2.5. **Shortest Job First (SJF)** Algorithm

- Features

  - Take into account some process metrics (*e.g., the burst time of the process.* ) by choosing the job that will be completed soonest as the next one to execute.
  - This algorithm can be shown to **minimise average waiting time**.
  - For estimating the burst time requires 
    - Data to be kept on the previous burst times of a process (if available), and 
    - The computation of some sort of average burst time. 

    > We didn’t have this problem with FCFS because that algorithm doesn’t care about burst times.

- Types

  - Non-preemptive SJF
    - Allow each process to complete or enter a wait state before another process is allowed to run
  - Preemptive SJF
    -  When a new process arrives in the ready queue
      1. the scheduler will compare the *(estimated) remaining burst time of the currently-running process* and *the (estimated) burst time of the newly-arrived process*, 
      2. and if the latter is lower the existing process will be preempted.

- Starvation and Priority Scheduling

  - Starvation
    - With SJF it could be the case that so many short jobs arrive that a longer job does not get the chance to execute. 
    - This is called **starvation** and <u>should be avoided</u>.
  - Techniques to avoid starvation
    - Bring an **age** factor 
      - The longer a process stays in the queue the higher its age gets. This can then be factored into the choice to ensure that, eventually, even long jobs will get executed.
      - We are affording a **priority value** to each process that is somehow a combination of its burst time and its age. Thus we can create a **priority scheduler**. 
  - Priority Scheduling
    - Preemptive
      - Scheduling the existing process would be interrupted.
    - Non-preemptive
      - If a new, higher-priority process arrives it must wait until the currently-running process has terminated or entered a wait state.

- Example

  >e.g.
  >
  >Consider the same four processes P1, P2, P3, P4 being scheduled using SJF. In this example, the processes are all assumed to arrive at the same time.
  >
  >| Process | Brust Time (ms) | Wait Time (ms) |
  >| :-----: | :-------------: | :------------: |
  >| **P2**  |        4        |       0        |
  >| **P4**  |        5        |       4        |
  >| **P3**  |        6        |       9        |
  >| **P1**  |       27        |       15       |
  >| **AWT** |                 |       7        |
  >
  >We have seen that this is potentially a lower average waiting time than for FCFS *(FCFS can only achieve this AWT if the processes arrive in this exact order)*.

##### 2.6. Round-Robin Scheduling

- Features

  - The **round-robin algorithm** operates in a similar way to FCFS <u>but has **preemption**</u>.
    - Use a FIFO queue <u>but treated as circular</u> to hold the processes. 
    - While a process is not terminated or waiting, it will remain in the queue. Once it has been to the front of the queue it will be returned to the back.
  - A **time quantum** is specified. 
    - The time quantum is <u>the maximum burst time allowed for a process in any one run</u>. 
    - The time quantum does not typically vary per process.

- Possibilities

  1. If a running process signals it is finished, or has entered a wait state, before its time quantum has expired
     - It is preempted for the next process in the queue.
  2. Otherwise, if the process reaches its maximum time quantum, 
     - It is preempted (unless no other process is in the queue).

>e.g.
>
>Consider a round robin algorithm in which the quantum is set to 2ms, and the following set of processes in the ready queue in the following order.
>
>| Process | Burst Time (ms) |
>| :-----: | :-------------: |
>| **P1**  |        5        |
>| **P2**  |        9        |
>| **P3**  |        2        |
>| **P4**  |        1        |
>| **P5**  |        6        |
>| **P6**  |       11        |
>
>The execution times will be as follows.
>
>>P1 - 2 (ms)
>>
>>P2 - 2
>>
>>P3 - 2
>>
>>P4 - 1
>>
>>P5 - 2
>>
>>P6 - 2
>>
>>P1 - 2
>>
>>P2 - 2
>>
>>P5 - 2
>>
>>P6 - 2
>>
>>P1 - 1
>>
>>P2 - 2
>>
>>P5 - 2
>>
>>P6 - 2
>>
>>...
>
>The waiting time for a given process is the time that it spends not executing. This is essentially the sum of the execution times of the processes between its runs.
>
>We can see that, by definition, the round-robin algorithm is *preemptive*.

##### 2.7. Multilevel Queue Scheduling

- Features

  - Multilevel queue scheduling involves multiple queues, each containing a particular class of process, arranged in a priority order. So, the highest priority processes are placed in the top level queue, and the lowest in the bottom queue.
  - These individual queues can also be organised by priority and each can use a different scheduling algorithm.
  - Processes **do not move** between queues.

- Multilevel Feedback Queue Scheduling

  - An even more flexible model that **allow processes to move** between queues.
  - With this model the processes are allocated to queues according to their burst time characteristics, and moved between queues on the same basis.

    > e.g. 
    >
    > Trocesses that take up a lot of CPU time might be moved to lower priority queues.

---

### 3. Threads

##### 3.1. Introduction

- Terminology

  - **thread**
    
    - Is introduced for <u>allowing each process to appear to do more than one thing at a time</u>.
    - A thread is a <u>strand of execution within a process</u>. A process may have many threads executing *at the same time*.
    - Each process can start threads. Threads can also start threads. 
    
      > e.g., In the Java language, there is a *Thread* class and a *Runnable* interface that can be used to create new threads.

- Threads

  >![img](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter4/4_01_ThreadDiagram.jpg)

  - Examples

    > A process creates a thread to handle some subtask of its overall task: 
    >
    > - a web server might start a new thread to handle each new connection from a browser. 
    > - User interfaces are also a classic example of the threaded approach. 
    > - An application can put its UI implementation into one or more threads so that it remains responsive even when the application is performing some heavy duty task.
    >
    > From the word processor example: 
    >
    > - The application might start a thread to do the spellchecking, saving the file, repagination, etc.

  - Features
    - When a process creates a thread, the thread shares the address space (exists in the same memory block) of the process.

      > This applies to all of the threads created under that process. In practice this means *they all access the same heap - the heap allocated to the process - and the globals, etc.*

    - However, each thread gets its own stack. Because each thread will do different things, call different methods, etc. So it’s easier to just give each thread a stack.

##### 3.2. CPUs

>Note. 
>
>Consider the single CPU system. Such a design can perform only one task at a time. The idea of processes was introduced to enable the system to have multiple tasks in the RAM and then, using a scheduler, switch between them to give the appearance that the tasks were being performed simultaneously. Only one task was ever being performed at any one instant, however.
>
>If threads are introduced to this model, the fact remains that only one thing at a time can actually happen. Within the running process, the threads would have to take turns to perform whatever task it is they are implementing.

- Multicore processors
  - **Multicore processors** is designed for getting performance increases on CPUs. 
  - Essentially have 2, 4. 8, etc. cores per processor. 
    - A core is a CPU. 
    - All cores can execute in parallel - literally at the same time.
  - Affects
    - The OS and the CPU are now able to schedule multiple processes to literally execute at the same time by allocating them to different cores. 
    - A single core CPU can handle multiple processes ‘simultaneously’ by scheduling them, each core in a multicore processor is able to do that.
    - This potentially <u>increases the throughput</u> of processes considerably.

##### 3.3. Types

- **User threads** - When a user (programmer) creates threads (including in Java, and when using the pthreads library)
- **Kernel threads** - Threads that are owned by the kernel *(i.e. the core operating system)*

    >The operating system will (usually) map user threads to *kernel threads* for execution in a one-to-one fashion. Threads can also be mapped many-to-one and many-to-many but these mappings are less common.

##### 3.4. Scheduling

- In the kernel, the **OS** actually schedules threads not processes. 

  - Since the kernel maps *user threads* to *kernel threads*, and 
  - *kernal threads* are the basic unit of execution for the OS.
  
  > e.g.
  >
  > This also applies to Java programs that create threads. Java threads are actually managed as kernel threads by the OS.

