## WEEK III - Probabilisitc Models 1

>[🏠 MENU - 6CCS3ML1](year3/6ccs3ml1.md)
>
>[⬅️ WEEK II - Inductive Learning](year3/6ccs3ml1/w2.md)
>
>[➡️ WEEK IV - Probabilisitc Models 2](year3/6ccs3ml1/w4.md)
>
>Outlines:
>
>1. Bayes Theorem and Naive Bayes Classifier
>1. Gaussian Mixture Models
>1. K-Means Clustering

> Ref: [7CCSMPNN: Week2](year3/7ccsmpnn/w2.md)

### 3.1. Bayes Theorem and Naive Bayes Classifier

##### 3.1.1. Probabilistic methods

- Motivation

  - Key idea is that we don't have a hard boundary between yes and no. Thus, we can make probabilistic predictions.
  - Pros
    - Can make probabilistic predictions.
    - Each training example has an incremental effect on the estimated probability (increase or decrease estimated probability).
    - Prior knowledge is combined with observed data to determine final probability.
  - Cons
    - Need initial knowledge of many probabilities.
    - Computational cost can be high.
- Scheme

  - Assume there are some set of **hypotheses**, given some data. 
  - Write $H$ for all of the possible ones, $h$ for a specific one. We want the **most probable** hypothesis.
    i.e., Need to calculate $p(h | \mathcal{D})$ for the various $h \in H$.

##### 3.1.2. Bayes Theorem

- Definition
  $$
  p(h|\mathcal{D}) = \frac{p(\mathcal{D} | h) p(h)}{p(\mathcal{D})}
  $$

  - $p(h)$ denote the probability of hypothesis $h$ before we have any data. 
    - Known as **prior probability**.
    - May reflect background knowledge, or some assumptions.
  - $p(\mathcal{D})$ denotes the prior probability that the data will be observed.
    - i.e., how probable we think the data is without any knowledge about which hypothesis holds.
  - $p(\mathcal{D}|h)$ denotes the probability of observing the data, given that the hypothesis is true.
    - Known as **likelihood**. 

    > e.g.
    >
    > When I am told that I’m looking at the coursework of an A student, how probable are these marks?

  - $p(h | \mathcal{D})$ denotes the probability of observing the hypothesis, given the data.
    - Known as **posterior probability**.
    - What we are looking for in ML. 
    - Usually, we want to find a optimal hyperparameter set $\theta$ from the dataset $\mathcal{D}$, thus we want to know $p(\theta|\mathcal{D})$ by applying Bayes theorem. 

    > e.g.
    >
    > Now that I have seen their coursework grades, how probable is it that this student will get an A?

- Relationship between four elements

  - $p(h | \mathcal{D})$ increases with $P(h), p(\mathcal{D}|h)$.
  - $p(h | \mathcal{D})$ decreases with $p(\mathcal{D})$. 

- Example: The wrong underwear

  > e.g. 
  >
  > You live with your partner. You come home from a business trip to discover a strange pair of underwear in your drawer. What should you think?
  >
  > > data = underwear 
  > >
  > > hypothesis = partner cheating
  > >
  > > ...

##### 3.1.3. Bayesian Learning

- Definition

  - Bayes theorem: principled way to calculate the posterior of $h$ given some data $\mathcal{D}$.
  - Basis for a learning algorithm that calculates the posterior of different $h$ (not necessarily binary) and outputs the one with the highest probability.
- **Maximum a posteriori** (MAP) hypothesis.

  - In some cases we assume that every hypothesis $h \in H$ is equally likely a priori.
  - Then we only need to consider the **likelihood** $p(\mathcal{D}|h)$. 
  - In this case, we want the maximum likelihood hypothesis $h_{ML}= \arg\max_{h \in H} p(\mathcal{D}|h)$. 

##### 3.1.4. Naive Bayes Classifier

- Definition

  - Naive Bayes Classifier is a practical approach to Bayesian learning. 
    - It is simple but effective, and particularly resistant to overfitting.
  - As a classifier, it fits the mould of $p(y_i | \textbf{x}_i, \mathcal{D})$
    where we learn an approximation of the mapping from $\textbf{x}$ to $y$ such that $y = f(\textbf{x})$
    - $f(\textbf{x})$ takes on values from a finite set $V$, represents the target value. 
    - $\textbf{x}$ is a vector of feature values where $\textbf{x} = \langle a_1, a_2, ..., a_n\rangle$
  - In this setting, a new instance is classified by looking for 
    $$
    v_{MAP} = \arg \max_{v_i \in V} p(v_i |a_1, a_2, ..., a_n) = \arg\max_{v_i \in V} p(a_1, a_2, ..., a_n)p(v_i)
    $$
  
- Process

  - In order to achieve our goal to calculate $v_{MAP}$, we estimate them based on training data.
    - $p(v_i)$ based on the number of times $v_i$ appears in the training data. 
    - Similair for $p(a_1, a_2, ..., a_n | v_i)$: Proportion of the cases that have $v_i$ which also have $a_1, a_2,..., a_n$.
    - Number of possible combinations is large, thus lots of data is required.
  - So we simplify
    1. Assume position does not matter (if applicable).
    2. Assume that features are conditionally independent given the target $v$ (strong independence assumption).
    3. Thus, $p(a_1, a_2, ..., a_n | v_i) = \prod_j p(a_j|v_i)$ 
       - $p(a_j|v_i)$​ look for all the cases in which $v_i$ occurs and then compute the proportion which have $a_j$.
       - Need much less data to be accurate.
  - After calculating $p(v_i), p(a_j | v_i)$, we can make predictions about the $v_i$ for a given $\textbf{x}_k = \langle a_1, a_2, ..., a_n \rangle$  (a feature vector with $n$ features).
    $$
    v_{NB} = \arg\max_{v_i \in V} p(v_i) \prod_j p(a_j|v_i)
    $$
  
    - When the assumption about conditional independence is satisfied, where $v_{MAP} = v_{NB}$ is provable. 
    - When the assumption about conditional independence is not satisfied, $v_{NB}$ is often a good solution. This is established by experiment.
  
- Note

  -  When we introduced Bayesian learning, it was in terms of a hypothesis.
  - We looked for the MAP hypothesis $p(h | \mathcal{D})$.  In Naive Bayes, the hypothesis is the particular values of: 
    $$
    p(v_i) \\
    p(a_j | v_i)
    $$
  - The data, $\mathcal{D}$, is, as ever, all the examples from which we computed those values.
  
- Example

  > e.g.
  >
  > ...

##### 3.1.5. Naive Bayes output

- Small Numbers

  - Note the small probabilities that we compute. If there are more features it can get easily **underflow** errors.

    > e.g. $\langle 0.0053, 0.0206 \rangle$

  - Avoid this by taking **logs**:
    $$
    v_{NB} = \arg\max_{v_i \in V} p(v_i) \prod_j p(a_j | v_i) \implies \\
    v_{NB} = \arg\max_{v_i \in V} \log P(v_i) + \sum_j \log P(a_j | v_i)
    $$

- Estimating Probabilities

  - Motivation
    - So far we computed probabilities by just counting. we estimated $p(A = a|B = b)$ by 
      $$
      \frac{n_c}{n}
      $$
      where
      - $n$ is total number of cases where $B = b$, and
      - $n_c$ is the number of those cases where $A = a$.

    - It can be poor when $n_c$ is close to zero. Which can happen we have only a few samples and can lead to **overfitting**. 
    - Solution is to use a better estimator.

  - $m$-estimate

    - An example for an improved estimator. Can also use other, more sophisticated priors when appropriate.
    - We estimate as follows
      $$
      \frac{n_c + m \cdot p}{n + m}
      $$
      where 
      - $n_c ,n$ are defined as previous, 
      - $p$ is a prior estimate (before taking any data into account). Typically $p = \frac{1}{k}$ if the feature has $k$ values.
      - $m$ is a constant, equivalent sample size.

- Notes

  - When we compute $v_{NB}$, we are actually computing 
    $$
    p(v_i | a_1, a_2, ..., a_n, \mathcal{D}) = \arg\max_{v_i \in V} p(v_i | \mathcal{D}) \prod_i p(a_j | v_i, \mathcal{D})
    $$

    - For the key step of NB classifier, we computed these conditionals by counting instances. 
    - This is fine for *discrete-valued* features,  but the model is more general.

      > e.g.
      >
      > If the features are real-valued, we might model each as a Gaussian and compute:
      > $$
      > p(a_1, a_2, ..., a_n | v_i) = \prod_j^n \mathcal{N} (a_j | \mu_{jc}, \sigma^2_{jc})
      > $$
      > where $\mu_{jc}, \sigma^2_{jc}$​ are the mean and variance of feature $j$ in instances of class $c = v_i$.


---

### 3.2. Guassian Mixture Models

##### 3.2.1. Mixture Models

- Motivation

  - Might model each feature $x$ as a Gaussian distribution with mean and variance, notated as $\mu, \sigma^2$.
    - (Probability of $x$ taking different values).
  - Often, the data we are trying to model is much more complex. 
  - Model the data in terms of a mixture of several components.

- Definition

  - A mixture model is a combination of probability models. In the general case, we mix $K$ base distributions (can be of any type) so that:
    $$
    p(\textbf{x}_i) = \sum^K_{k=1} \pi_k p_k(\textbf{x}_i)
    $$
    where the $\pi_k$ are the mixing weights.

  - i.e., the probability of an element $\textbf{x}_i$ is the sum of the probabilities assigned to $\textbf{x}_i$ by each of the probability models in the mixture.

  - The most widely used mixture model is probably **the mixture of Gaussians**.

    - Each model is a *multivariate Gaussian* with mean $\mu_k$ and covariance $\Sigma_k$ (matrix $n \times n$).Mixture is:
      $$
      p(\textbf{x}_i) = \sum^K_{k=1} \pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)
      $$
      which again just assigns $\textbf{x}_i$ the sum of the probabilities given to it by each of the components of the mixture.

    - When all Gaussians are *univariate* we have:
      $$
      p(\textbf{x}_i) = \sum^K_{k=1} \pi_k \mathcal{N} (x_i | \mu_k, \sigma_k^2)
      $$
      where $\sigma^2$ is the variance. 

    - Probabilistic model for representing a number of normally distributed sub-populations within an overall population.

      > e.g. modelling human height (fitting a single Gaussian doesn't work)

- Features

  - Mixture models are helpful, since lots of processes are Gaussian. i.e., lots of stuff in the world generates values that are normally distributed.
  - Combinations of these things will be mixture models.

##### 3.2.2. Estimating the Means of k Gaussians

- Motivation

  - Assume that $\mathcal{D}$ is generated by a mixture model. Then, 

    1. First, pick the distribution with equal probability $\frac{1}{k}$. 
    2. Then pick a value according to the distribution. 
    3. Repeat $k$ times. 

  - Assume that each Gaussian has the same variance. We want to learn a hypothesis that describes the means of each distribution
    $$
    h = \langle \mu_1, ..., \mu_k \rangle
    $$
    such that $p(\mathcal{D}|h)$ is maximum.

  - i.e., Maximum likelihood hypothesis, $h_{ML}$. 

- Solution

  - We can solve this problem using the EM algorithm. 

##### 3.2.3. Expectation Maximisation (EM) algorithm.

- Definition

  - Given a current hypothesis $h = \langle \mu_1, ..., \mu_k \rangle$, Estimate the expected values of the hidden variables $z_{ij}$. Then re-calculate the maximum likelihood hypothesis given those values.

  - Process (for a 2 Gaussian case)

    1. We initialise EM by picking $h = \langle \mu_1, \mu_2 \rangle$, for arbitrary initial values.

    2. Repeat

       - Calculate the expected value $E[z_{ij}]$ of each hidden variable $z_{ij}$ assuming $h = \langle \mu_1, \mu_2 \rangle$ is true.

         - $E[z_{ij}]$ is the probability that $xi$ was generated by the $j$th normal distribution.
           $$
           E[z_{ij}] = \frac{p(x = x_i | \mu = \mu_j)}{\sum_{n=1}^2 p(x=x_i|\mu=\mu_n)} = \frac{e^{-\frac{1}{2\sigma^2}(x_i - \mu_j)^2}}{\sum_{n=1}^2 e^{-\frac{1}{2\sigma^2}(x_i - \mu_n)^2}}
           $$

         - Relative ratio of $j$th Gaussian in $i$th example.

         - Then the first step is just plugging the current values of $μ_1, μ_2$ and $x_i$ into the above equation.

       - Calculate a new maximum likelihood hypothesis $h' = \langle \mu_1', \mu_2' \rangle$, assuming each $z_{ij}$ takes its expected value $E[z_{ij}]$. Replace $h$ by $h'$. 

         - Adjust the values of the means to maximise the likelihood. 
           $$
           \mu_j \leftarrow \frac{\sum^m_{i=1} E[z_{ij}] x_i}{\sum^m_{i=1} E[z_{ij}]}
           $$

         - The sample mean for $\mu_j$, weighted by the expectation $E[z_{ij}]$. If $E[z_{ij}]$ is small, then $x_i$ contributes less to $\mu_j$ (the mean of $j$th).

       Until convergence (change in parameters becomes very small).

  - Evaluation

    - Can show that each iteration adjusts $h$ to increase the likelihood $p(\mathcal{D}|h)$ unless likelihood is at a local maximum. 
    - Thus EM converges to a local maximum of the likelihood. 
    - Global vs. local maximum/minimum.

- EM algorithm More Generally

  - EM can be applied to any situation in which we want to estimate parameters $\theta$, that describe a distribution, given only a portion of the data generated by the distribution.

    - In general, we have 
      - $\mathcal{D} = \{\textbf{x}_1, ..., \textbf{x}_m\}$ as $m$ independent drawn observations, and
      - $Z = \{\textbf{z}_1, ..., \textbf{z}_m\}$ as the corresponding unknown data which identifies which distribution the $\textbf{x}_1$ come from.
    - Then, the full data can be defined as $Y = \mathcal{D} \cup Z$.

  - Process

    1. Expectation (E) step

       - Since we do not know $\theta$, EM uses the current hypothesis $h$ (about $θ$) to estimate the distribution over $Y$. We define $Q(h'|h)$ as:
         $$
         Q(h'|h) = E[\ln p(Y|h') | h, \mathcal{D}]
         $$
         giving $E[\ln p(Y|h')]$ under the assumption that $\mathcal{D}$ and $\theta = h$. 

       - We can calculate $Q(h'|h)$ with the given hypothesis and observed data to estimate the probabilities, i.e.,
         $$
         Q(h'|h) \leftarrow E[\ln p(Y|h') | h, \mathcal{D}]
         $$

    2. Maximisation (M) step

       - Repeat the following step by replacing $h$ by $h'$ which maximizes $Q$:
         $$
         h \leftarrow \arg \max_{h'} Q(h'|h)
         $$
         Until convergence. 

  - Note

    - If $Q$ is continuous, then EM converges to a stationary point of the likelihood function $p(Y|h')$.
    - Global maximum <u>if the likelihood function has a single maximum</u>, otherwise a local maximum.
    - So, random restart, or just careful selection of starting points.

---

### 3.3. K-Means Clustering

##### 3.3.1. K-Means Clustering Algorithm

- Motivation

  - Our example of establishing the means of $K$ is common enough that it's a thing in ML. We can use K-means algorithm.
  - Each $\mu_j$ is a cluster centre, and each $x_i$ is assigned to the cluster closest to it.
  - Leads to another statement of the algorithm (clustering algorithm).

- Process

  1. Pick random values for $\mu_k$ .

  2. Repeat the following steps

     - Assign each $\textbf{x}_i$ to its closest cluster centre:
       $$
       z_i = \arg\min_z |\textbf{x}_i - \mu_k|
       $$

     - Update each cluster centre as the mean of the points assigned to that cluster:
       $$
       \mu_k = \frac{1}{|\{j: z_j = k\}|} \sum_{i \in \{j: z_j = k\}} \textbf{x}_i
       $$

     Until converge.

  > Note.
  >
  > The notation allows for multi-dimensional $\textbf{x}_i, \mu_k$ .

- Two K-means

  - **Hard clustering**: the K-means we just looked at just reports which cluster $\textbf{x}_i$ is part of.
  - **Soft clustering**: probability that $\textbf{x}_i$ is in the cluster.
  - So for every point $\textbf{x}_i$ we have $K$ probabilities, defined as $p_k(\textbf{x}_i)$

